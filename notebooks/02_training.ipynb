{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "## Model Training Pipeline: ESM2 with LoRA Fine-tuning\n",
    "\n",
    "This notebook handles the complete model training pipeline:\n",
    "\n",
    "1. **Environment Setup** - Check GPU availability and load configurations\n",
    "2. **Data Loading** - Load tokenized datasets from disk\n",
    "3. **Model Initialization** - Load ESM2 model with LoRA adapters\n",
    "4. **Training Setup** - Configure training arguments and initialize trainer\n",
    "5. **Model Training** - Train the model with validation monitoring\n",
    "6. **Save Model** - Save the trained model checkpoint for later evaluation\n",
    "\n",
    "After running this notebook, you should have:\n",
    "- `models/checkpoints/` - Model checkpoints saved during training\n",
    "- `models/final_model/` - Final trained model\n",
    "- Training metrics logged to Weights & Biases\n",
    "\n",
    "**Note:** For detailed evaluation and analysis, run `03_evaluation.ipynb` after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_section",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_from_disk\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "\n",
    "# Enable cuDNN benchmark for potential performance gains on fixed-size inputs\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment_section",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(\"=\" * 70)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    cuda_version = torch.version.cuda\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"✓ GPU detected: {device_name}\")\n",
    "    print(f\"  CUDA version: {cuda_version}\")\n",
    "    print(f\"  Available memory: {total_memory:.2f} GB\")\n",
    "    print(f\"  Number of GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: No GPU detected!\")\n",
    "    print(\"  Training will be extremely slow on CPU.\")\n",
    "    print(\"  Consider using a GPU-enabled environment.\")\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "print(\"\\n=== LOADING CONFIGURATION ===\")\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"  Base model: {cfg['model']['name']}\")\n",
    "print(f\"  Number of labels: {cfg['model']['num_labels']}\")\n",
    "\n",
    "print(\"\\nLoRA Configuration:\")\n",
    "for key, value in cfg['model']['lora'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {cfg['training']['epochs']}\")\n",
    "print(f\"  Batch size: {cfg['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {cfg['training']['learning_rate']}\")\n",
    "print(f\"  Gradient accumulation steps: {cfg['training']['grad_accum_steps']}\")\n",
    "print(f\"  FP16: {cfg['training']['fp16']}\")\n",
    "\n",
    "print(\"\\n✓ Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_section",
   "metadata": {},
   "source": [
    "## 2. Load Tokenized Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== LOADING TOKENIZED DATASETS ===\")\n",
    "\n",
    "# Load datasets from disk\n",
    "train_path = \"../data/tokenized/train_dataset\"\n",
    "val_path = \"../data/tokenized/val_dataset\"\n",
    "\n",
    "print(f\"Loading train dataset from: {train_path}\")\n",
    "train_ds = load_from_disk(train_path).with_format(\"torch\")\n",
    "\n",
    "print(f\"Loading validation dataset from: {val_path}\")\n",
    "val_ds = load_from_disk(val_path).with_format(\"torch\")\n",
    "\n",
    "print(\"\\n=== DATASET SUMMARY ===\")\n",
    "print(f\"Train samples: {len(train_ds):,}\")\n",
    "print(f\"Validation samples: {len(val_ds):,}\")\n",
    "print(f\"Total samples: {len(train_ds) + len(val_ds):,}\")\n",
    "\n",
    "print(\"\\nDataset features:\")\n",
    "print(f\"  {train_ds.features}\")\n",
    "\n",
    "print(\"\\n✓ Datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify_class_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify class distribution in loaded datasets\n",
    "print(\"\\n=== VERIFYING CLASS DISTRIBUTION ===\")\n",
    "\n",
    "# Get label distributions\n",
    "train_labels = [int(label) for label in train_ds['labels']]\n",
    "val_labels = [int(label) for label in val_ds['labels']]\n",
    "\n",
    "train_dist = pd.Series(train_labels).value_counts().sort_index()\n",
    "val_dist = pd.Series(val_labels).value_counts().sort_index()\n",
    "\n",
    "print(f\"\\n{'Class':<7} {'Train':>10} {'Val':>10} {'Total':>10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "ec_names = [\n",
    "    \"Oxidoreductases\",\n",
    "    \"Transferases\",\n",
    "    \"Hydrolases\",\n",
    "    \"Lyases\",\n",
    "    \"Isomerases\",\n",
    "    \"Ligases\",\n",
    "    \"Translocases\"\n",
    "]\n",
    "\n",
    "for i in range(7):\n",
    "    train_c = train_dist.get(i, 0)\n",
    "    val_c = val_dist.get(i, 0)\n",
    "    total_c = train_c + val_c\n",
    "    print(f\"{i} ({ec_names[i][:3]}): {train_c:>10,} {val_c:>10,} {total_c:>10,}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Total':<7} {len(train_ds):>10,} {len(val_ds):>10,} {len(train_ds)+len(val_ds):>10,}\")\n",
    "\n",
    "print(\"\\n✓ Class distribution verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_section",
   "metadata": {},
   "source": [
    "## 3. Model Initialization with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== LOADING BASE MODEL ===\")\n",
    "\n",
    "model_name = cfg['model']['name']\n",
    "num_labels = cfg['model']['num_labels']\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Task: Sequence Classification with {num_labels} labels\")\n",
    "\n",
    "# Determine device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Load base model\n",
    "print(\"\\nLoading base model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✓ Base model loaded\")\n",
    "\n",
    "# Count parameters before LoRA\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters in base model: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply_lora",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== APPLYING LoRA ADAPTERS ===\")\n",
    "\n",
    "lora_cfg = cfg['model']['lora']\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_cfg['r'],\n",
    "    lora_alpha=lora_cfg['alpha'],\n",
    "    target_modules=lora_cfg['target_modules'],\n",
    "    lora_dropout=lora_cfg['dropout'],\n",
    "    bias=lora_cfg['bias'],\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Bias: {lora_config.bias}\")\n",
    "\n",
    "# Apply LoRA to model\n",
    "print(\"\\nApplying LoRA adapters...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\n=== TRAINABLE PARAMETERS ===\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"\\n✓ Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_setup_section",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "print(\"\\n=== SETTING UP EVALUATION METRICS ===\")\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy for evaluation\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "print(\"✓ Evaluation metrics configured\")\n",
    "print(\"  Primary metric: accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_wandb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "print(\"\\n=== INITIALIZING WEIGHTS & BIASES ===\")\n",
    "\n",
    "if cfg['wandb']['enabled']:\n",
    "    wandb.init(\n",
    "        project=cfg['wandb']['project'],\n",
    "        name=cfg['wandb']['run_name'],\n",
    "        config=cfg\n",
    "    )\n",
    "    print(f\"✓ W&B initialized\")\n",
    "    print(f\"  Project: {cfg['wandb']['project']}\")\n",
    "    print(f\"  Run name: {cfg['wandb']['run_name']}\")\n",
    "else:\n",
    "    print(\"⚠ W&B logging disabled in config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_arguments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "print(\"\\n=== CONFIGURING TRAINING ARGUMENTS ===\")\n",
    "\n",
    "output_dir = (Path('..') / 'models' / 'checkpoints').resolve()\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir = str(output_dir)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=cfg['training']['epochs'],\n",
    "    per_device_train_batch_size=cfg['training']['batch_size'],\n",
    "    per_device_eval_batch_size=cfg['training']['batch_size'],\n",
    "    gradient_accumulation_steps=cfg['training']['grad_accum_steps'],\n",
    "    learning_rate=float(cfg['training']['learning_rate']),\n",
    "    fp16=cfg['training']['fp16'] and torch.cuda.is_available(),\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=cfg['training']['eval_strategy'],\n",
    "    save_strategy=cfg['training']['save_strategy'],\n",
    "    save_total_limit=cfg['training']['save_total_limit'],\n",
    "    load_best_model_at_end=cfg['training']['load_best_model_at_end'],\n",
    "    metric_for_best_model=cfg['training']['metric_for_best_model'],\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=[\"wandb\"] if cfg['wandb']['enabled'] else [],\n",
    "    run_name=cfg['wandb']['run_name'],\n",
    "    \n",
    "    # Data loading\n",
    "    dataloader_num_workers=cfg['training']['num_workers'],\n",
    "    dataloader_pin_memory=cfg['training']['pin_memory'],\n",
    "    dataloader_persistent_workers=cfg['training']['persistent_workers'],\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Output directory: {output_dir}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")\n",
    "print(f\"  Evaluation strategy: {training_args.eval_strategy}\")\n",
    "print(f\"  Save strategy: {training_args.save_strategy}\")\n",
    "\n",
    "# Calculate effective batch size\n",
    "effective_batch_size = (\n",
    "    training_args.per_device_train_batch_size * \n",
    "    training_args.gradient_accumulation_steps\n",
    ")\n",
    "print(f\"\\nEffective batch size: {effective_batch_size}\")\n",
    "\n",
    "# Estimate training steps\n",
    "steps_per_epoch = len(train_ds) // effective_batch_size\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "print(f\"Estimated steps per epoch: {steps_per_epoch:,}\")\n",
    "print(f\"Estimated total training steps: {total_steps:,}\")\n",
    "\n",
    "print(\"\\n✓ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initialize_trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "print(\"\\n=== INITIALIZING TRAINER ===\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")\n",
    "print(f\"  Training samples: {len(trainer.train_dataset):,}\")\n",
    "print(f\"  Evaluation samples: {len(trainer.eval_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_section",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Build checkpoint directory relative to notebook (not hard-coded '../')\n",
    "checkpoint_dir = (Path('..') / cfg['data']['paths']['checkpoints']).resolve()\n",
    "\n",
    "def _checkpoint_number(path: Path):\n",
    "    try:\n",
    "        return int(path.name.split('-')[-1])\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    \"\"\"Return the latest checkpoint Path or None\"\"\"\n",
    "    if not isinstance(output_dir, Path):\n",
    "        output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        return None\n",
    "\n",
    "    checkpoints = [p for p in output_dir.iterdir() if p.is_dir() and p.name.startswith('checkpoint-')]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    latest = max(checkpoints, key=_checkpoint_number)\n",
    "    return latest\n",
    "\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nThis may take a while depending on your hardware...\\n\")\n",
    "\n",
    "checkpoint_path = get_latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    adapter_model = checkpoint_path / 'adapter_model.safetensors'\n",
    "    trainer_state = checkpoint_path / 'trainer_state.json'\n",
    "\n",
    "    if adapter_model.exists() and trainer_state.exists():\n",
    "        print(f\"Resuming from checkpoint: {checkpoint_path}\\n\")\n",
    "        scaler_path = checkpoint_path / 'scaler.pt'\n",
    "        if scaler_path.exists():\n",
    "            scaler_path.unlink()\n",
    "        train_result = trainer.train(resume_from_checkpoint=str(checkpoint_path))\n",
    "    else:\n",
    "        print(\"Invalid checkpoint. Starting fresh training...\\n\")\n",
    "        train_result = trainer.train()\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh training...\\n\")\n",
    "    train_result = trainer.train()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if train_result:\n",
    "    print(\"\\nFinal metrics:\")\n",
    "    for key, value in train_result.metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = (Path('..') / cfg['data']['paths']['checkpoints'] / 'final_model').resolve()\n",
    "print(f\"\\nSaving final model to: {final_model_path}\")\n",
    "trainer.save_model(str(final_model_path))\n",
    "print(\"✓ Model saved successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_section",
   "metadata": {},
   "source": [
    "## 6. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "print(\"\\n=== SAVING FINAL MODEL ===\")\n",
    "\n",
    "final_model_dir = (Path('..') / 'models' / 'final_model').resolve()\n",
    "final_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to: {final_model_dir}\")\n",
    "trainer.save_model(str(final_model_dir))\n",
    "\n",
    "print(\"\\nModel saved successfully\")\n",
    "print(f\"\\nSaved files:\")\n",
    "for file in final_model_dir.iterdir():\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation evaluation\n",
    "print(\"\\n=== QUICK VALIDATION CHECK ===\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Validation Metrics:\")\n",
    "for key, value in eval_results.items():\n",
    "    if key.startswith('eval_'):\n",
    "        metric_name = key.replace('eval_', '').replace('_', ' ').title()\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nValidation check complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finish_wandb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish W&B run\n",
    "if cfg['wandb']['enabled']:\n",
    "    print(\"\\n=== FINALIZING WEIGHTS & BIASES ===\")\n",
    "    wandb.finish()\n",
    "    print(\"✓ W&B run finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
