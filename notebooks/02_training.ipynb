{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "## Model Training Pipeline: ESM2 with LoRA Fine-tuning\n",
    "\n",
    "This notebook handles the complete model training pipeline with flexible experimentation support:\n",
    "\n",
    "1. **Environment Setup** - Check GPU and load base configuration\n",
    "2. **Training Configuration** - Override hyperparameters for experimentation\n",
    "3. **Data Loading** - Load tokenized datasets\n",
    "4. **Model Initialization** - Load ESM2 model with LoRA adapters\n",
    "5. **Training** - Train with automatic checkpointing and W&B logging\n",
    "\n",
    "**Quick Start:**\n",
    "- First time: Run all cells to train for configured epochs\n",
    "- Resume training: Set `RESUME_TRAINING = True` to continue from last checkpoint\n",
    "- New experiment: Set `RESUME_TRAINING = False` to start fresh training\n",
    "- Change hyperparameters: Modify the \"Training Configuration\" cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_section",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_from_disk\n",
    "from evaluate import load\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Enable cuDNN benchmark for performance\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment_section",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "check_gpu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENVIRONMENT CHECK\n",
      "======================================================================\n",
      "GPU detected: NVIDIA GeForce RTX 5060\n",
      "  CUDA version: 12.9\n",
      "  Available memory: 8.55 GB\n",
      "  Number of GPUs: 1\n",
      "\n",
      "PyTorch version: 2.8.0+cu129\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "print(\"=\" * 70)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    cuda_version = torch.version.cuda\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"GPU detected: {device_name}\")\n",
    "    print(f\"  CUDA version: {cuda_version}\")\n",
    "    print(f\"  Available memory: {total_memory:.2f} GB\")\n",
    "    print(f\"  Number of GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "    print(\"  Training will be extremely slow on CPU.\")\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING BASE CONFIGURATION ===\n",
      "\n",
      "Base configuration loaded from config.yaml\n",
      "  Model: facebook/esm2_t12_35M_UR50D\n",
      "\n",
      "Tip: Modify the next cell to override hyperparameters for experimentation\n"
     ]
    }
   ],
   "source": [
    "# Load base configuration from config.yaml\n",
    "print(\"\\n=== LOADING BASE CONFIGURATION ===\")\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\nBase configuration loaded from config.yaml\")\n",
    "print(f\"  Model: {cfg['model']['name']}\")\n",
    "print(\"\\nTip: Modify the next cell to override hyperparameters for experimentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_config_section",
   "metadata": {},
   "source": [
    "## 2. Training Configuration\n",
    "\n",
    "**Modify this cell for experimentation**\n",
    "\n",
    "### Resume vs New Training\n",
    "- `RESUME_TRAINING = True` - Continue from last checkpoint with same hyperparameters\n",
    "- `RESUME_TRAINING = False` - Start fresh training (creates new W&B run)\n",
    "\n",
    "### Hyperparameter Overrides\n",
    "Set any to `None` to use value from `config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "experiment_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPERIMENT CONFIGURATION ===\n",
      "\n",
      "Mode: NEW TRAINING\n",
      "  Epochs: 3 (from config)\n",
      "  Learning rate: 2e-4 (from config)\n",
      "  Batch size: 8 (from config)\n",
      "  Gradient accumulation: 1 (from config)\n",
      "  FP16: True (from config)\n",
      "\n",
      "  LoRA rank: 8 (from config)\n",
      "  LoRA alpha: 16 (from config)\n",
      "  LoRA dropout: 0.1 (from config)\n",
      "\n",
      "Checkpoint directory: ..\\models\\checkpoints\n",
      "W&B run name: esm2-lora-baseline\n",
      "\n",
      "Configuration ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT CONFIGURATION - MODIFY THIS FOR YOUR EXPERIMENTS\n",
    "# ============================================================================\n",
    "\n",
    "# Resume from checkpoint or start fresh?\n",
    "RESUME_TRAINING = False  # Set to True to continue training from last checkpoint\n",
    "\n",
    "# Experiment name (used for W&B run name and checkpoint directory)\n",
    "EXPERIMENT_NAME = None  # e.g., \"high_lr_experiment\" or None to use config\n",
    "\n",
    "# Training hyperparameters (None = use config.yaml value)\n",
    "EPOCHS = None           # e.g., 5 for 5 epochs total\n",
    "LEARNING_RATE = None    # e.g., 5e-4 for higher learning rate\n",
    "BATCH_SIZE = None       # e.g., 8 for smaller batch\n",
    "GRAD_ACCUM_STEPS = None # e.g., 4 for gradient accumulation\n",
    "\n",
    "# LoRA hyperparameters (None = use config.yaml value)\n",
    "LORA_R = None          # e.g., 16 for rank 16\n",
    "LORA_ALPHA = None      # e.g., 32 for alpha 32\n",
    "LORA_DROPOUT = None    # e.g., 0.1 for 10% dropout\n",
    "\n",
    "# Advanced options\n",
    "WARMUP_RATIO = 0.1     # Learning rate warmup (10% of training)\n",
    "FP16 = None            # Mixed precision (None = use config.yaml)\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Apply overrides\n",
    "print(\"\\n=== EXPERIMENT CONFIGURATION ===\")\n",
    "print(f\"\\nMode: {'RESUME TRAINING' if RESUME_TRAINING else 'NEW TRAINING'}\")\n",
    "\n",
    "# Create experiment-specific checkpoint directory\n",
    "if EXPERIMENT_NAME:\n",
    "    base_checkpoint_dir = Path('..') / 'models' / 'experiments' / EXPERIMENT_NAME\n",
    "    run_name = EXPERIMENT_NAME\n",
    "else:\n",
    "    base_checkpoint_dir = Path('..') / cfg['paths']['checkpoints']\n",
    "    run_name = cfg['wandb']['run_name']\n",
    "\n",
    "base_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Override training config\n",
    "if EPOCHS is not None:\n",
    "    cfg['training']['epochs'] = EPOCHS\n",
    "    print(f\"  Epochs: {EPOCHS} (overridden)\")\n",
    "else:\n",
    "    print(f\"  Epochs: {cfg['training']['epochs']} (from config)\")\n",
    "\n",
    "if LEARNING_RATE is not None:\n",
    "    cfg['training']['learning_rate'] = LEARNING_RATE\n",
    "    print(f\"  Learning rate: {LEARNING_RATE} (overridden)\")\n",
    "else:\n",
    "    print(f\"  Learning rate: {cfg['training']['learning_rate']} (from config)\")\n",
    "\n",
    "if BATCH_SIZE is not None:\n",
    "    cfg['training']['batch_size'] = BATCH_SIZE\n",
    "    print(f\"  Batch size: {BATCH_SIZE} (overridden)\")\n",
    "else:\n",
    "    print(f\"  Batch size: {cfg['training']['batch_size']} (from config)\")\n",
    "\n",
    "if GRAD_ACCUM_STEPS is not None:\n",
    "    cfg['training']['grad_accum_steps'] = GRAD_ACCUM_STEPS\n",
    "    print(f\"  Gradient accumulation: {GRAD_ACCUM_STEPS} (overridden)\")\n",
    "else:\n",
    "    print(f\"  Gradient accumulation: {cfg['training']['grad_accum_steps']} (from config)\")\n",
    "\n",
    "if FP16 is not None:\n",
    "    cfg['training']['fp16'] = FP16\n",
    "    print(f\"  FP16: {FP16} (overridden)\")\n",
    "else:\n",
    "    print(f\"  FP16: {cfg['training']['fp16']} (from config)\")\n",
    "\n",
    "# Override LoRA config\n",
    "if LORA_R is not None:\n",
    "    cfg['model']['lora']['r'] = LORA_R\n",
    "    print(f\"\\n  LoRA rank: {LORA_R} (overridden)\")\n",
    "else:\n",
    "    print(f\"\\n  LoRA rank: {cfg['model']['lora']['r']} (from config)\")\n",
    "\n",
    "if LORA_ALPHA is not None:\n",
    "    cfg['model']['lora']['alpha'] = LORA_ALPHA\n",
    "    print(f\"  LoRA alpha: {LORA_ALPHA} (overridden)\")\n",
    "else:\n",
    "    print(f\"  LoRA alpha: {cfg['model']['lora']['alpha']} (from config)\")\n",
    "\n",
    "if LORA_DROPOUT is not None:\n",
    "    cfg['model']['lora']['dropout'] = LORA_DROPOUT\n",
    "    print(f\"  LoRA dropout: {LORA_DROPOUT} (overridden)\")\n",
    "else:\n",
    "    print(f\"  LoRA dropout: {cfg['model']['lora']['dropout']} (from config)\")\n",
    "\n",
    "print(f\"\\nCheckpoint directory: {base_checkpoint_dir}\")\n",
    "print(f\"W&B run name: {run_name}\")\n",
    "print(\"\\nConfiguration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_section",
   "metadata": {},
   "source": [
    "## 3. Load Tokenized Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load_datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING TOKENIZED DATASETS ===\n",
      "Train samples: 111,434\n",
      "Validation samples: 23,879\n",
      "\n",
      "Class distribution:\n",
      "0    21000\n",
      "1    21000\n",
      "2    21000\n",
      "3    14141\n",
      "4     9020\n",
      "5    16797\n",
      "6     8476\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Datasets loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== LOADING TOKENIZED DATASETS ===\")\n",
    "\n",
    "train_path = \"../data/tokenized/train_dataset\"\n",
    "val_path = \"../data/tokenized/val_dataset\"\n",
    "\n",
    "train_ds = load_from_disk(train_path).with_format(\"torch\")\n",
    "val_ds = load_from_disk(val_path).with_format(\"torch\")\n",
    "\n",
    "print(f\"Train samples: {len(train_ds):,}\")\n",
    "print(f\"Validation samples: {len(val_ds):,}\")\n",
    "\n",
    "# Quick class distribution check\n",
    "train_labels = pd.Series([int(label) for label in train_ds['labels']])\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_labels.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nDatasets loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_section",
   "metadata": {},
   "source": [
    "## 4. Model Initialization\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** adds small trainable adapter layers to the frozen base model:\n",
    "- Trains <1% of parameters\n",
    "- Reduces memory and training time\n",
    "- Preserves pre-trained knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "load_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING BASE MODEL ===\n",
      "Model: facebook/esm2_t12_35M_UR50D\n",
      "Device: cuda\n",
      "\n",
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded (33,503,768 parameters)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== LOADING BASE MODEL ===\")\n",
    "\n",
    "model_name = cfg['model']['name']\n",
    "num_labels = cfg['model']['num_labels']\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load base model\n",
    "print(\"\\nLoading base model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Base model loaded ({total_params:,} parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "apply_lora",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== APPLYING LoRA ADAPTERS ===\n",
      "LoRA config: r=8, alpha=16, dropout=0.1\n",
      "trainable params: 418,567 || all params: 33,922,335 || trainable%: 1.2339\n",
      "\n",
      "Model ready on cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== APPLYING LoRA ADAPTERS ===\")\n",
    "\n",
    "lora_cfg = cfg['model']['lora']\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_cfg['r'],\n",
    "    lora_alpha=lora_cfg['alpha'],\n",
    "    target_modules=lora_cfg['target_modules'],\n",
    "    lora_dropout=lora_cfg['dropout'],\n",
    "    bias=lora_cfg['bias'],\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "print(f\"LoRA config: r={lora_config.r}, alpha={lora_config.lora_alpha}, dropout={lora_config.lora_dropout}\")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"\\nModel ready on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wandb_section",
   "metadata": {},
   "source": [
    "## 5. W&B Setup and Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wandb_utils",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B utilities loaded\n"
     ]
    }
   ],
   "source": [
    "def get_or_create_run_id(checkpoint_dir: Path, resume: bool) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Get existing run ID or create new one for W&B.\n",
    "    \n",
    "    Returns:\n",
    "        (run_id, resume_mode) tuple\n",
    "    \"\"\"\n",
    "    run_id_file = checkpoint_dir / 'wandb_run_id.txt'\n",
    "    \n",
    "    if resume and run_id_file.exists():\n",
    "        with open(run_id_file, 'r') as f:\n",
    "            run_id = f.read().strip()\n",
    "        print(f\"Resuming W&B run: {run_id}\")\n",
    "        return run_id, \"must\"\n",
    "    else:\n",
    "        run_id = wandb.util.generate_id()\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with open(run_id_file, 'w') as f:\n",
    "            f.write(run_id)\n",
    "        print(f\"Starting new W&B run: {run_id}\")\n",
    "        return run_id, \"never\"\n",
    "\n",
    "def flatten_config(config: Dict, parent_key: str = '') -> Dict:\n",
    "    \"\"\"Flatten nested config for W&B.\"\"\"\n",
    "    items = []\n",
    "    for k, v in config.items():\n",
    "        new_key = f\"{parent_key}/{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_config(v, new_key).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir: Path) -> Optional[Path]:\n",
    "    \"\"\"Get the latest checkpoint directory.\"\"\"\n",
    "    if not checkpoint_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    checkpoints = [p for p in checkpoint_dir.iterdir() \n",
    "                   if p.is_dir() and p.name.startswith('checkpoint-')]\n",
    "    \n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    \n",
    "    # Sort by checkpoint number\n",
    "    def get_checkpoint_num(path):\n",
    "        try:\n",
    "            return int(path.name.split('-')[-1])\n",
    "        except:\n",
    "            return -1\n",
    "    \n",
    "    return max(checkpoints, key=get_checkpoint_num)\n",
    "\n",
    "class WandBCallbackEnhanced(TrainerCallback):\n",
    "    \"\"\"Enhanced W&B callback for richer logging.\"\"\"\n",
    "    \n",
    "    def __init__(self, class_names: list):\n",
    "        self.class_names = class_names\n",
    "        self.best_f1_macro = 0\n",
    "        self.best_accuracy = 0\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and wandb.run:\n",
    "            if 'learning_rate' in logs:\n",
    "                wandb.log({\"train/learning_rate\": logs['learning_rate']}, step=state.global_step)\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics and wandb.run:\n",
    "            # Track best metrics\n",
    "            if 'eval_f1_macro' in metrics:\n",
    "                if metrics['eval_f1_macro'] > self.best_f1_macro:\n",
    "                    self.best_f1_macro = metrics['eval_f1_macro']\n",
    "                    wandb.run.summary['best_f1_macro'] = self.best_f1_macro\n",
    "            \n",
    "            if 'eval_accuracy' in metrics:\n",
    "                if metrics['eval_accuracy'] > self.best_accuracy:\n",
    "                    self.best_accuracy = metrics['eval_accuracy']\n",
    "                    wandb.run.summary['best_accuracy'] = self.best_accuracy\n",
    "\n",
    "print(\"W&B utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics_section",
   "metadata": {},
   "source": [
    "## 6. Detailed Metrics Setup\n",
    "\n",
    "- **Accuracy**: Overall correctness (Number of correct predictions) / (Total number of predictions)\n",
    "- **Macro F1**: Average F1 across all classes (treats each class equally)\n",
    "- **Micro F1**: Global F1 (weighted by class frequency, similar to accuracy)\n",
    "- **Per-class F1**: Shows performance on individual enzyme classes\n",
    "\n",
    "**Macro F1 is used as the key metric** - Treats each enzyme class equally, best for balanced evaluation across all 7 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metrics_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SETTING UP DETAILED METRICS ===\n",
      "\n",
      "Metrics configured:\n",
      "  ✓ Accuracy (overall)\n",
      "  ✓ Macro F1 (treats each class equally)\n",
      "  ✓ Micro F1 (weighted by class frequency)\n",
      "  ✓ Precision & Recall (both macro and micro)\n",
      "  ✓ Per-class F1 scores for all 7 enzyme classes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SETTING UP DETAILED METRICS ===\")\n",
    "\n",
    "# EC class names for reference\n",
    "EC_NAMES = [\n",
    "    \"Oxidoreductases\",  # EC 1\n",
    "    \"Transferases\",     # EC 2\n",
    "    \"Hydrolases\",       # EC 3\n",
    "    \"Lyases\",           # EC 4\n",
    "    \"Isomerases\",       # EC 5\n",
    "    \"Ligases\",          # EC 6\n",
    "    \"Translocases\"      # EC 7\n",
    "]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute comprehensive metrics for multi-class classification.\n",
    "    \n",
    "    Returns:\n",
    "        - accuracy: Overall accuracy\n",
    "        - f1_macro: Macro-averaged F1 (treats each class equally)\n",
    "        - f1_micro: Micro-averaged F1 (weighted by frequency)\n",
    "        - precision_macro, recall_macro: Macro averages\n",
    "        - precision_micro, recall_micro: Micro averages\n",
    "        - f1_class_N: Per-class F1 scores for each enzyme class\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(-1)\n",
    "    \n",
    "    # Basic accuracy\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    \n",
    "    # Macro metrics (average across classes - treats each class equally)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Micro metrics (global average - weighted by frequency)\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='micro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Build comprehensive metrics dictionary\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy),\n",
    "        \n",
    "        # Macro metrics\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'precision_macro': float(precision_macro),\n",
    "        'recall_macro': float(recall_macro),\n",
    "        \n",
    "        # Micro metrics\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'precision_micro': float(precision_micro),\n",
    "        'recall_micro': float(recall_micro),\n",
    "    }\n",
    "    \n",
    "    # Add per-class F1 scores\n",
    "    for i, (f1, name) in enumerate(zip(f1_per_class, EC_NAMES)):\n",
    "        metrics[f'f1_class_{i}_{name[:8]}'] = float(f1)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"\\nMetrics configured:\")\n",
    "print(\"  ✓ Accuracy (overall)\")\n",
    "print(\"  ✓ Macro F1 (treats each class equally)\")\n",
    "print(\"  ✓ Micro F1 (weighted by class frequency)\")\n",
    "print(\"  ✓ Precision & Recall (both macro and micro)\")\n",
    "print(\"  ✓ Per-class F1 scores for all 7 enzyme classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_section",
   "metadata": {},
   "source": [
    "## 7. Training Configuration\n",
    "\n",
    "**If `RESUME_TRAINING = False`:**\n",
    "- Starts fresh training\n",
    "- Creates new W&B run\n",
    "- Trains for configured epochs\n",
    "- Saves checkpoints along the way\n",
    "\n",
    "**If `RESUME_TRAINING = True`:**\n",
    "- Continues from last checkpoint\n",
    "- Resumes existing W&B run\n",
    "- Trains for remaining epochs\n",
    "\n",
    "**Checkpoints:**\n",
    "- Saved at end of each epoch (if `save_strategy=\"epoch\"`)\n",
    "- Only keeps last N checkpoints (set by `save_total_limit`)\n",
    "- Best model automatically loaded at end (based on f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "training_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONFIGURING TRAINING ===\n",
      "Effective batch size: 8\n",
      "Steps per epoch: 13,929\n",
      "Total training steps: 41,787\n",
      "\n",
      "Best model selection: Based on MACRO F1 (not accuracy)\n",
      "This ensures the model performs well on ALL enzyme classes.\n",
      "\n",
      "Training configured\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CONFIGURING TRAINING ===\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(base_checkpoint_dir),\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=cfg['training']['epochs'],\n",
    "    per_device_train_batch_size=cfg['training']['batch_size'],\n",
    "    per_device_eval_batch_size=cfg['training']['batch_size'],\n",
    "    gradient_accumulation_steps=cfg['training']['grad_accum_steps'],\n",
    "    learning_rate=float(cfg['training']['learning_rate']),\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=cfg['training']['fp16'] and torch.cuda.is_available(),\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=cfg['training']['save_strategy'],\n",
    "    save_total_limit=cfg['training']['save_total_limit'],\n",
    "    load_best_model_at_end=cfg['training']['load_best_model_at_end'],\n",
    "    \n",
    "    # Evaluation - USE F1_MACRO as primary metric\n",
    "    eval_strategy=cfg['training']['eval_strategy'],\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=str(base_checkpoint_dir / \"logs\"),\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    report_to=[\"wandb\"] if cfg['wandb']['enabled'] else [],\n",
    "    \n",
    "    # Data loading\n",
    "    dataloader_num_workers=cfg['training']['num_workers'],\n",
    "    dataloader_pin_memory=cfg['training']['pin_memory'],\n",
    "    dataloader_persistent_workers=cfg['training']['persistent_workers'],\n",
    ")\n",
    "\n",
    "effective_batch = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "steps_per_epoch = len(train_ds) // effective_batch\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "print(f\"Effective batch size: {effective_batch}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch:,}\")\n",
    "print(f\"Total training steps: {total_steps:,}\")\n",
    "print(\"\\nBest model selection: Based on MACRO F1 (not accuracy)\")\n",
    "print(\"This ensures the model performs well on ALL enzyme classes.\")\n",
    "print(\"\\nTraining configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_section",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "\n",
      "✓ Starting fresh training (RESUME_TRAINING=False)\n",
      "\n",
      "Starting new W&B run: gd3mymsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinalee0723\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\calam\\repos\\enzyme_classifier\\notebooks\\wandb\\run-20251212_090324-gd3mymsv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinalee0723/enzyme-classification-esm2/runs/gd3mymsv' target=\"_blank\">esm2-lora-baseline</a></strong> to <a href='https://wandb.ai/cristinalee0723/enzyme-classification-esm2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinalee0723/enzyme-classification-esm2' target=\"_blank\">https://wandb.ai/cristinalee0723/enzyme-classification-esm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinalee0723/enzyme-classification-esm2/runs/gd3mymsv' target=\"_blank\">https://wandb.ai/cristinalee0723/enzyme-classification-esm2/runs/gd3mymsv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41790' max='41790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [41790/41790 2:37:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Class 0 Oxidored</th>\n",
       "      <th>F1 Class 1 Transfer</th>\n",
       "      <th>F1 Class 2 Hydrolas</th>\n",
       "      <th>F1 Class 3 Lyases</th>\n",
       "      <th>F1 Class 4 Isomeras</th>\n",
       "      <th>F1 Class 5 Ligases</th>\n",
       "      <th>F1 Class 6 Transloc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>0.234101</td>\n",
       "      <td>0.941790</td>\n",
       "      <td>0.941408</td>\n",
       "      <td>0.946499</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0.941790</td>\n",
       "      <td>0.941790</td>\n",
       "      <td>0.941790</td>\n",
       "      <td>0.949890</td>\n",
       "      <td>0.940486</td>\n",
       "      <td>0.924637</td>\n",
       "      <td>0.913275</td>\n",
       "      <td>0.922407</td>\n",
       "      <td>0.978664</td>\n",
       "      <td>0.960494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>0.150899</td>\n",
       "      <td>0.964655</td>\n",
       "      <td>0.964143</td>\n",
       "      <td>0.963366</td>\n",
       "      <td>0.965047</td>\n",
       "      <td>0.964655</td>\n",
       "      <td>0.964655</td>\n",
       "      <td>0.964655</td>\n",
       "      <td>0.970198</td>\n",
       "      <td>0.959920</td>\n",
       "      <td>0.958030</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.940488</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>0.980511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.139935</td>\n",
       "      <td>0.970476</td>\n",
       "      <td>0.970897</td>\n",
       "      <td>0.970919</td>\n",
       "      <td>0.970891</td>\n",
       "      <td>0.970476</td>\n",
       "      <td>0.970476</td>\n",
       "      <td>0.970476</td>\n",
       "      <td>0.972985</td>\n",
       "      <td>0.965379</td>\n",
       "      <td>0.963743</td>\n",
       "      <td>0.958607</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.990391</td>\n",
       "      <td>0.985173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETED\n",
      "======================================================================\n",
      "\n",
      "Final Metrics:\n",
      "  Runtime: 9452.8848\n",
      "  Samples Per Second: 35.3650\n",
      "  Steps Per Second: 4.4210\n",
      "  Total Flos: 68146074658037880.0000\n",
      "  Loss: 0.2739\n",
      "  Epoch: 3.0000\n",
      "\n",
      "Saving final model to: ..\\models\\checkpoints\\final_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (..\\models\\checkpoints\\final_model)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging model to W&B artifacts...\n",
      "✓ Model artifact logged\n",
      "\n",
      "✓ Training complete!\n",
      "\n",
      "   Model: ..\\models\\checkpoints\\final_model\n",
      "   W&B: https://wandb.ai/cristinalee0723/enzyme-classification-esm2/runs/gd3mymsv\n",
      "\n",
      "======================================================================\n",
      "NEXT STEPS\n",
      "======================================================================\n",
      "\n",
      "1. To continue training for more epochs:\n",
      "   - Set RESUME_TRAINING = True\n",
      "   - Adjust EPOCHS to total desired epochs\n",
      "   - Re-run this cell\n",
      "\n",
      "2. To start a new experiment:\n",
      "   - Set RESUME_TRAINING = False\n",
      "   - Set EXPERIMENT_NAME to a new name\n",
      "   - Adjust hyperparameters as needed\n",
      "   - Re-run from 'Training Configuration' cell\n",
      "\n",
      "3. For evaluation:\n",
      "   - Run 03_evaluation.ipynb\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▇█</td></tr><tr><td>eval/f1_class_0_Oxidored</td><td>▁▇█</td></tr><tr><td>eval/f1_class_1_Transfer</td><td>▁▆█</td></tr><tr><td>eval/f1_class_2_Hydrolas</td><td>▁▇█</td></tr><tr><td>eval/f1_class_3_Lyases</td><td>▁▇█</td></tr><tr><td>eval/f1_class_4_Isomeras</td><td>▁▄█</td></tr><tr><td>eval/f1_class_5_Ligases</td><td>▁▆█</td></tr><tr><td>eval/f1_class_6_Transloc</td><td>▁▇█</td></tr><tr><td>eval/f1_macro</td><td>▁▆█</td></tr><tr><td>eval/f1_micro</td><td>▁▇█</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>0.97048</td></tr><tr><td>best_f1_macro</td><td>0.9709</td></tr><tr><td>data/train_samples</td><td>111434</td></tr><tr><td>data/val_samples</td><td>23879</td></tr><tr><td>eval/accuracy</td><td>0.97048</td></tr><tr><td>eval/f1_class_0_Oxidored</td><td>0.97298</td></tr><tr><td>eval/f1_class_1_Transfer</td><td>0.96538</td></tr><tr><td>eval/f1_class_2_Hydrolas</td><td>0.96374</td></tr><tr><td>eval/f1_class_3_Lyases</td><td>0.95861</td></tr><tr><td>eval/f1_class_4_Isomeras</td><td>0.96</td></tr><tr><td>+25</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">esm2-lora-baseline</strong> at: <a href='https://wandb.ai/cristinalee0723/enzyme-classification-esm2/runs/gd3mymsv' target=\"_blank\">https://wandb.ai/cristinalee0723/enzyme-classification-esm2/runs/gd3mymsv</a><br> View project at: <a href='https://wandb.ai/cristinalee0723/enzyme-classification-esm2' target=\"_blank\">https://wandb.ai/cristinalee0723/enzyme-classification-esm2</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_090324-gd3mymsv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# EC class names\n",
    "ec_names = [\n",
    "    \"Oxidoreductases\", \"Transferases\", \"Hydrolases\",\n",
    "    \"Lyases\", \"Isomerases\", \"Ligases\", \"Translocases\"\n",
    "]\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint_to_resume = None\n",
    "if RESUME_TRAINING:\n",
    "    checkpoint_to_resume = get_latest_checkpoint(base_checkpoint_dir)\n",
    "    if checkpoint_to_resume:\n",
    "        print(f\"\\n✓ Found checkpoint: {checkpoint_to_resume.name}\")\n",
    "        print(\"  Will resume training from this checkpoint\\n\")\n",
    "    else:\n",
    "        print(\"\\n⚠ No checkpoint found. Starting fresh training\\n\")\n",
    "else:\n",
    "    print(\"\\n✓ Starting fresh training (RESUME_TRAINING=False)\\n\")\n",
    "\n",
    "# Initialize W&B\n",
    "if cfg['wandb']['enabled']:\n",
    "    run_id, resume_mode = get_or_create_run_id(base_checkpoint_dir, RESUME_TRAINING)\n",
    "    \n",
    "    wandb_context = wandb.init(\n",
    "        project=cfg['wandb']['project'],\n",
    "        name=run_name,\n",
    "        id=run_id,\n",
    "        resume=resume_mode,\n",
    "        config=flatten_config(cfg),\n",
    "        tags=['esm2', 'lora', 'protein-classification'],\n",
    "        notes=f\"Training {cfg['model']['name']} with LoRA (r={cfg['model']['lora']['r']})\"\n",
    "    )\n",
    "else:\n",
    "    from contextlib import nullcontext\n",
    "    wandb_context = nullcontext()\n",
    "\n",
    "with wandb_context:\n",
    "    # Log model info to W&B\n",
    "    if cfg['wandb']['enabled']:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        wandb.run.summary.update({\n",
    "            'model/total_parameters': total_params,\n",
    "            'model/trainable_parameters': trainable_params,\n",
    "            'model/trainable_percentage': 100 * trainable_params / total_params,\n",
    "            'data/train_samples': len(train_ds),\n",
    "            'data/val_samples': len(val_ds),\n",
    "        })\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[WandBCallbackEnhanced(ec_names)] if cfg['wandb']['enabled'] else []\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    if checkpoint_to_resume:\n",
    "        print(f\"Resuming from: {checkpoint_to_resume}\\n\")\n",
    "        train_result = trainer.train(resume_from_checkpoint=str(checkpoint_to_resume))\n",
    "    else:\n",
    "        train_result = trainer.train()\n",
    "    \n",
    "    # Training completed\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    for key, value in train_result.metrics.items():\n",
    "        metric_name = key.replace('train_', '').replace('_', ' ').title()\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric_name}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric_name}: {value}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = base_checkpoint_dir / \"final_model\"\n",
    "    print(f\"\\nSaving final model to: {final_model_path}\")\n",
    "    trainer.save_model(str(final_model_path))\n",
    "    \n",
    "    # Log model as W&B artifact\n",
    "    if cfg['wandb']['enabled']:\n",
    "        print(\"\\nLogging model to W&B artifacts...\")\n",
    "        artifact = wandb.Artifact(\n",
    "            name=f\"esm2-lora-ec-classifier\",\n",
    "            type=\"model\",\n",
    "            description=f\"ESM2 with LoRA (r={cfg['model']['lora']['r']}) for EC classification\",\n",
    "            metadata={\n",
    "                'epochs': cfg['training']['epochs'],\n",
    "                'learning_rate': cfg['training']['learning_rate'],\n",
    "                'lora_r': cfg['model']['lora']['r'],\n",
    "            }\n",
    "        )\n",
    "        artifact.add_dir(str(final_model_path))\n",
    "        wandb.log_artifact(artifact)\n",
    "        print(\"✓ Model artifact logged\")\n",
    "    \n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    print(f\"\\n   Model: {final_model_path}\")\n",
    "    if cfg['wandb']['enabled']:\n",
    "        print(f\"   W&B: {wandb.run.url}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"NEXT STEPS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n1. To continue training for more epochs:\")\n",
    "    print(\"   - Set RESUME_TRAINING = True\")\n",
    "    print(\"   - Adjust EPOCHS to total desired epochs\")\n",
    "    print(\"   - Re-run this cell\")\n",
    "    print(\"\\n2. To start a new experiment:\")\n",
    "    print(\"   - Set RESUME_TRAINING = False\")\n",
    "    print(\"   - Set EXPERIMENT_NAME to a new name\")\n",
    "    print(\"   - Adjust hyperparameters as needed\")\n",
    "    print(\"   - Re-run from 'Training Configuration' cell\")\n",
    "    print(\"\\n3. For evaluation:\")\n",
    "    print(\"   - Run 03_evaluation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
