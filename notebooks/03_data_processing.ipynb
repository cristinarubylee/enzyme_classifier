{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878b25bf",
   "metadata": {},
   "source": [
    "## Data Processing and Train/Val/Test Split\n",
    "\n",
    "After running this notebook, you should have:\n",
    "\n",
    "1. Raw processed CSV datasets\n",
    "\n",
    "    - `data/processed/train.csv` — **Training set** (70%)  \n",
    "    - `data/processed/val.csv` — **Validation set** (15%)  \n",
    "    - `data/processed/test.csv` — **Test set** (15%)  \n",
    "\n",
    "    - Columns: `Entry`, `Sequence`, `EC Number`\n",
    "\n",
    "\n",
    "2. Tokenized datasets (for model training)\n",
    "\n",
    "    - `data/tokenized/train_tokenized/`  \n",
    "    - `data/tokenized/val_tokenized/`\n",
    "    - `data/tokenized/test_tokenized/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec4313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load cleaned data from exploration step\n",
    "df = pd.read_csv('../data/raw/uniprot_cleaned.csv')\n",
    "print(f\"Loaded {len(df):,} entries from cleaned dataset\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert main EC class numbers (1-7) into ML labels (0-6)\n",
    "df['Label'] = df['EC_Main_Class'] - 1\n",
    "\n",
    "# Drop rows with invalid labels\n",
    "initial_count = len(df)\n",
    "df = df.dropna(subset=['Label'])\n",
    "df['Label'] = df['Label'].astype(int)\n",
    "\n",
    "print(f\"\\nExtracted labels for {len(df):,} entries\")\n",
    "print(f\"Dropped {initial_count - len(df):,} entries with invalid EC numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check original class distribution and imbalance\n",
    "print(\"\\n=== ORIGINAL CLASS DISTRIBUTION ===\")\n",
    "class_counts = df['Label'].value_counts().sort_index()\n",
    "for label, count in class_counts.items():\n",
    "    print(f\"Class {label}: {count:>8,} samples\")\n",
    "\n",
    "original_max = class_counts.max()\n",
    "original_min = class_counts.min()\n",
    "imbalance_ratio = original_max / original_min\n",
    "print(f\"\\nOriginal imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"(Max class: {original_max:,} / Min class: {original_min:,})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load balancing targets from config\n",
    "with open(\"../configs/data.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "targets = cfg[\"targets\"]\n",
    "random_seed = cfg[\"random_seed\"]\n",
    "\n",
    "print(\"\\n=== BALANCING TARGETS FROM CONFIG ===\")\n",
    "print(f\"Random seed: {random_seed}\")\n",
    "print(\"\\nTarget samples per class:\")\n",
    "for label, target in targets.items():\n",
    "    print(f\"  Class {label}: {target:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply class balancing\n",
    "print(\"\\n=== APPLYING CLASS BALANCING ===\")\n",
    "balanced_dfs = []\n",
    "\n",
    "for label, target_count in targets.items():\n",
    "    class_df = df[df['Label'] == label]\n",
    "    current_count = len(class_df)\n",
    "    \n",
    "    if current_count == 0:\n",
    "        print(f\"Warning: No samples for class {label}\")\n",
    "        continue\n",
    "    \n",
    "    # Take minimum of available samples and target\n",
    "    sample_count = min(current_count, target_count)\n",
    "    \n",
    "    # Random sampling with fixed seed for reproducibility\n",
    "    sampled_df = class_df.sample(n=sample_count, random_state=42)\n",
    "    balanced_dfs.append(sampled_df)\n",
    "    \n",
    "    print(f\"Class {label}: {current_count:>7,} → {sample_count:>7,}\")\n",
    "\n",
    "# Combine all balanced classes\n",
    "df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n{'Total':<8} {len(df):>7,} → {len(df_balanced):>7,}\")\n",
    "\n",
    "# Check final imbalance\n",
    "balanced_class_counts = df_balanced['Label'].value_counts().sort_index()\n",
    "balanced_max = balanced_class_counts.max()\n",
    "balanced_min = balanced_class_counts.min()\n",
    "balanced_imbalance = balanced_max / balanced_min\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1 → {balanced_imbalance:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for splitting (only Sequence and Label columns)\n",
    "df_final = df_balanced[['Sequence', 'Label']].copy()\n",
    "\n",
    "print(\"\\n=== DATASET READY FOR SPLITTING ===\")\n",
    "print(f\"Total samples: {len(df_final):,}\")\n",
    "print(f\"Features: Sequence\")\n",
    "print(f\"Target: Label (0-6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f7g8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== PERFORMING STRATIFIED TRAIN/VAL/TEST SPLIT ===\")\n",
    "\n",
    "# First split: 70% train, 30% temp (which will become 15% val, 15% test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_final,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=df_final['Label']\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 50% val, 50% test (15% each of original)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    random_state=42,\n",
    "    stratify=temp_df['Label']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df):>7,} ({len(train_df)/len(df_final)*100:.1f}%)\")\n",
    "print(f\"Val:   {len(val_df):>7,} ({len(val_df)/len(df_final)*100:.1f}%)\")\n",
    "print(f\"Test:  {len(test_df):>7,} ({len(test_df)/len(df_final)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4370066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify stratification - check class distribution in each split\n",
    "print(\"\\n=== CLASS DISTRIBUTION IN SPLITS ===\")\n",
    "print(f\"{'Class':<7} {'Train':>8} {'Val':>8} {'Test':>8}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "train_counts = train_df['Label'].value_counts().sort_index()\n",
    "val_counts = val_df['Label'].value_counts().sort_index()\n",
    "test_counts = test_df['Label'].value_counts().sort_index()\n",
    "\n",
    "for label in range(7):\n",
    "    train_c = train_counts.get(label, 0)\n",
    "    val_c = val_counts.get(label, 0)\n",
    "    test_c = test_counts.get(label, 0)\n",
    "    print(f\"{label:<7} {train_c:>8,} {val_c:>8,} {test_c:>8,}\")\n",
    "\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'Total':<7} {len(train_df):>8,} {len(val_df):>8,} {len(test_df):>8,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "train_path = \"../data/processed/train.csv\"\n",
    "val_path = \"../data/processed/val.csv\"\n",
    "test_path = \"../data/processed/test.csv\"\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"\\n=== SAVED PROCESSED DATASETS ===\")\n",
    "print(f\"Train: {train_path}\")\n",
    "print(f\"Val:   {val_path}\")\n",
    "print(f\"Test:  {test_path}\")\n",
    "\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"Total sequences: {len(train_df) + len(val_df) + len(test_df):,}\")\n",
    "print(f\"Number of classes: {train_df['Label'].nunique()}\")\n",
    "print(f\"Class balance ratio: {balanced_imbalance:.2f}:1\")\n",
    "\n",
    "print(\"\\nData processing complete! Ready for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Tokenize and save datasets\n",
    "print(\"\\n=== STARTING TOKENIZATION ===\")\n",
    "\n",
    "# Load the processed CSVs\n",
    "train_df = pd.read_csv(\"../data/processed/train.csv\")\n",
    "val_df   = pd.read_csv(\"../data/processed/val.csv\")\n",
    "test_df  = pd.read_csv(\"../data/processed/test.csv\")\n",
    "\n",
    "train_sequences = train_df[\"Sequence\"].tolist()\n",
    "val_sequences   = val_df[\"Sequence\"].tolist()\n",
    "test_sequences  = test_df[\"Sequence\"].tolist()\n",
    "\n",
    "train_labels = train_df[\"Label\"].tolist()\n",
    "val_labels   = val_df[\"Label\"].tolist()\n",
    "test_labels  = test_df[\"Label\"].tolist()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "\n",
    "print(\"Tokenizing train set...\")\n",
    "train_tokenized = tokenizer(train_sequences, padding=True, truncation=True)\n",
    "\n",
    "print(\"Tokenizing validation set...\")\n",
    "val_tokenized = tokenizer(val_sequences, padding=True, truncation=True)\n",
    "\n",
    "print(\"Tokenizing test set...\")\n",
    "test_tokenized = tokenizer(test_sequences, padding=True, truncation=True)\n",
    "\n",
    "# Create Hugging Face Datasets\n",
    "train_dataset = Dataset.from_dict(train_tokenized).add_column(\"labels\", train_labels)\n",
    "val_dataset   = Dataset.from_dict(val_tokenized).add_column(\"labels\", val_labels)\n",
    "test_dataset  = Dataset.from_dict(test_tokenized).add_column(\"labels\", test_labels)\n",
    "\n",
    "# Save to disk\n",
    "save_dir = \"../data/tokenized\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "train_dataset.save_to_disk(f\"{save_dir}/train_dataset\")\n",
    "val_dataset.save_to_disk(f\"{save_dir}/val_dataset\")\n",
    "test_dataset.save_to_disk(f\"{save_dir}/test_dataset\")\n",
    "\n",
    "print(\"\\n=== SAVED TOKENIZED DATASETS ===\")\n",
    "print(f\"Train: {save_dir}/train_dataset\")\n",
    "print(f\"Val:   {save_dir}/val_dataset\")\n",
    "print(f\"Test:  {save_dir}/test_dataset\")\n",
    "\n",
    "print(\"\\n=== TOKENIZATION SUMMARY ===\")\n",
    "print(f\"Train samples: {len(train_dataset):,}\")\n",
    "print(f\"Val samples:   {len(val_dataset):,}\")\n",
    "print(f\"Test samples:  {len(test_dataset):,}\")\n",
    "\n",
    "print(\"\\nTokenization complete! Ready for model setup.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
