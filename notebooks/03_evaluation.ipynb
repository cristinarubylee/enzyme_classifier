{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "## Model Evaluation Pipeline: Performance Analysis & Visualization\n",
    "\n",
    "This notebook handles comprehensive model evaluation and analysis:\n",
    "\n",
    "1. **Setup & Model Loading** - Load the trained model and datasets\n",
    "2. **Prediction Generation** - Generate predictions on validation and test sets\n",
    "3. **Performance Metrics** - Calculate accuracy, precision, recall, F1-score\n",
    "4. **Confusion Matrix Analysis** - Visualize classification patterns and errors\n",
    "5. **Per-Class Analysis** - Detailed metrics for each enzyme class\n",
    "6. **Error Analysis** - Examine misclassified examples\n",
    "7. **Results Visualization** - Interactive plots and comprehensive reports\n",
    "\n",
    "After running this notebook, you should have:\n",
    "- `results/predictions/` - Saved predictions for both validation and test sets\n",
    "- `results/figures/` - Confusion matrices and performance visualizations\n",
    "- `results/metrics/` - Detailed classification reports and per-class statistics\n",
    "- Comprehensive understanding of model strengths and weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_section",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_section",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\nConfiguration loaded:\")\n",
    "print(f\"  Project: {cfg['project']['name']}\")\n",
    "print(f\"  Model: {cfg['model']['name']}\")\n",
    "print(f\"  Number of classes: {cfg['model']['num_labels']}\")\n",
    "\n",
    "# EC class names\n",
    "ec_names = [\n",
    "    \"Oxidoreductases\",\n",
    "    \"Transferases\",\n",
    "    \"Hydrolases\",\n",
    "    \"Lyases\",\n",
    "    \"Isomerases\",\n",
    "    \"Ligases\",\n",
    "    \"Translocases\"\n",
    "]\n",
    "\n",
    "ec_names_short = [\"Oxido\", \"Trans\", \"Hydro\", \"Lyase\", \"Isom\", \"Ligase\", \"Transl\"]\n",
    "\n",
    "print(\"\\n✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_dirs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "print(\"\\n=== CREATING OUTPUT DIRECTORIES ===\")\n",
    "\n",
    "output_dirs = [\n",
    "    \"../results/predictions\",\n",
    "    \"../results/figures\",\n",
    "    \"../results/metrics\"\n",
    "]\n",
    "\n",
    "for dir_path in output_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"  ✓ {dir_path}\")\n",
    "\n",
    "print(\"\\n✓ Output directories ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "print(\"\\n=== DEVICE CHECK ===\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_section",
   "metadata": {},
   "source": [
    "## 2. Load Model & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== LOADING TRAINED MODEL ===\")\n",
    "\n",
    "model_path = \"../models/final_model\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"❌ Error: Model not found at {model_path}\")\n",
    "    print(\"Please run 02_training.ipynb first to train the model.\")\n",
    "else:\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    # Load the base model first\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg['model']['name'],\n",
    "        num_labels=cfg['model']['num_labels'],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load LoRA weights\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"✓ Model loaded successfully\")\n",
    "    print(f\"  Model on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== LOADING DATASETS ===\")\n",
    "\n",
    "# Load tokenized datasets\n",
    "val_path = \"../data/tokenized/val_dataset\"\n",
    "test_path = \"../data/tokenized/test_dataset\"\n",
    "\n",
    "print(f\"Loading validation dataset from: {val_path}\")\n",
    "val_ds = load_from_disk(val_path).with_format(\"torch\")\n",
    "\n",
    "print(f\"Loading test dataset from: {test_path}\")\n",
    "test_ds = load_from_disk(test_path).with_format(\"torch\")\n",
    "\n",
    "print(\"\\n=== DATASET SUMMARY ===\")\n",
    "print(f\"Validation samples: {len(val_ds):,}\")\n",
    "print(f\"Test samples: {len(test_ds):,}\")\n",
    "\n",
    "print(\"\\n✓ Datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction_section",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer for prediction\n",
    "print(\"\\n=== SETTING UP TRAINER FOR EVALUATION ===\")\n",
    "\n",
    "# Minimal training arguments for evaluation\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"../models/eval_temp\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on validation set\n",
    "print(\"\\n=== GENERATING VALIDATION PREDICTIONS ===\")\n",
    "\n",
    "val_predictions = trainer.predict(val_ds)\n",
    "val_logits = val_predictions.predictions\n",
    "val_pred_labels = val_logits.argmax(-1)\n",
    "val_true_labels = val_predictions.label_ids\n",
    "\n",
    "# Get prediction probabilities\n",
    "val_probs = torch.softmax(torch.tensor(val_logits), dim=-1).numpy()\n",
    "val_confidence = val_probs.max(-1)\n",
    "\n",
    "print(f\"✓ Generated predictions for {len(val_pred_labels):,} validation samples\")\n",
    "print(f\"  Prediction shape: {val_logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "print(\"\\n=== GENERATING TEST PREDICTIONS ===\")\n",
    "\n",
    "test_predictions = trainer.predict(test_ds)\n",
    "test_logits = test_predictions.predictions\n",
    "test_pred_labels = test_logits.argmax(-1)\n",
    "test_true_labels = test_predictions.label_ids\n",
    "\n",
    "# Get prediction probabilities\n",
    "test_probs = torch.softmax(torch.tensor(test_logits), dim=-1).numpy()\n",
    "test_confidence = test_probs.max(-1)\n",
    "\n",
    "print(f\"✓ Generated predictions for {len(test_pred_labels):,} test samples\")\n",
    "print(f\"  Prediction shape: {test_logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics_section",
   "metadata": {},
   "source": [
    "## 4. Overall Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "print(\"\\n=== OVERALL PERFORMANCE METRICS ===\")\n",
    "\n",
    "# Validation metrics\n",
    "val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
    "val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "    val_true_labels, val_pred_labels, average='weighted'\n",
    ")\n",
    "\n",
    "# Test metrics\n",
    "test_accuracy = accuracy_score(test_true_labels, test_pred_labels)\n",
    "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n",
    "    test_true_labels, test_pred_labels, average='weighted'\n",
    ")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  Accuracy:  {val_accuracy:.4f}\")\n",
    "print(f\"  Precision: {val_precision:.4f}\")\n",
    "print(f\"  Recall:    {val_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Split': ['Validation', 'Test'],\n",
    "    'Accuracy': [val_accuracy, test_accuracy],\n",
    "    'Precision': [val_precision, test_precision],\n",
    "    'Recall': [val_recall, test_recall],\n",
    "    'F1-Score': [val_f1, test_f1]\n",
    "})\n",
    "\n",
    "metrics_df.to_csv('../results/metrics/overall_metrics.csv', index=False)\n",
    "print(\"\\n✓ Overall metrics saved to results/metrics/overall_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overall metrics\n",
    "print(\"\\n=== VISUALIZING OVERALL METRICS ===\")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "val_values = [val_accuracy, val_precision, val_recall, val_f1]\n",
    "test_values = [test_accuracy, test_precision, test_recall, test_f1]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Validation',\n",
    "    x=metrics_to_plot,\n",
    "    y=val_values,\n",
    "    text=[f'{v:.3f}' for v in val_values],\n",
    "    textposition='auto',\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Test',\n",
    "    x=metrics_to_plot,\n",
    "    y=test_values,\n",
    "    text=[f'{v:.3f}' for v in test_values],\n",
    "    textposition='auto',\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Overall Model Performance: Validation vs Test',\n",
    "    xaxis_title='Metric',\n",
    "    yaxis_title='Score',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    barmode='group',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(\"✓ Overall metrics visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion_section",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrices\n",
    "print(\"\\n=== COMPUTING CONFUSION MATRICES ===\")\n",
    "\n",
    "val_cm = confusion_matrix(val_true_labels, val_pred_labels)\n",
    "test_cm = confusion_matrix(test_true_labels, test_pred_labels)\n",
    "\n",
    "# Normalize confusion matrices\n",
    "val_cm_norm = val_cm.astype('float') / val_cm.sum(axis=1)[:, np.newaxis]\n",
    "test_cm_norm = test_cm.astype('float') / test_cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"✓ Confusion matrices computed\")\n",
    "print(f\"  Validation CM shape: {val_cm.shape}\")\n",
    "print(f\"  Test CM shape: {test_cm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_confusion_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test confusion matrix (main result)\n",
    "print(\"\\n=== TEST SET CONFUSION MATRIX ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Absolute counts\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=ec_names_short, yticklabels=ec_names_short,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Absolute Counts', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Normalized (percentages)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(test_cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=ec_names_short, yticklabels=ec_names_short,\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "plt.title('Confusion Matrix - Normalized', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/confusion_matrix_test.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Test confusion matrix saved to results/figures/confusion_matrix_test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_confusion_val",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation confusion matrix\n",
    "print(\"\\n=== VALIDATION SET CONFUSION MATRIX ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Absolute counts\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(val_cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=ec_names_short, yticklabels=ec_names_short,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Absolute Counts', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Normalized (percentages)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(val_cm_norm, annot=True, fmt='.2%', cmap='Greens',\n",
    "            xticklabels=ec_names_short, yticklabels=ec_names_short,\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "plt.title('Confusion Matrix - Normalized', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/confusion_matrix_val.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Validation confusion matrix saved to results/figures/confusion_matrix_val.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive_confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive confusion matrix\n",
    "print(\"\\n=== INTERACTIVE CONFUSION MATRIX (TEST SET) ===\")\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=test_cm_norm,\n",
    "    x=ec_names,\n",
    "    y=ec_names,\n",
    "    colorscale='Blues',\n",
    "    text=test_cm,\n",
    "    texttemplate='%{text}<br>(%{z:.1%})',\n",
    "    textfont={\"size\": 10},\n",
    "    colorbar=dict(title=\"Accuracy\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Interactive Confusion Matrix - Test Set',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='True Label',\n",
    "    width=800,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(\"✓ Interactive confusion matrix displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perclass_section",
   "metadata": {},
   "source": [
    "## 6. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification reports\n",
    "print(\"\\n=== CLASSIFICATION REPORT (TEST SET) ===\")\n",
    "\n",
    "test_report = classification_report(\n",
    "    test_true_labels,\n",
    "    test_pred_labels,\n",
    "    target_names=ec_names,\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "print(test_report)\n",
    "\n",
    "# Save report\n",
    "with open('../results/metrics/classification_report_test.txt', 'w') as f:\n",
    "    f.write(test_report)\n",
    "\n",
    "print(\"\\n✓ Classification report saved to results/metrics/classification_report_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perclass_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "print(\"\\n=== PER-CLASS METRICS (TEST SET) ===\")\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    test_true_labels, test_pred_labels\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "perclass_df = pd.DataFrame({\n",
    "    'Class': range(7),\n",
    "    'Class_Name': ec_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "# Calculate accuracy per class from confusion matrix\n",
    "class_accuracy = test_cm_norm.diagonal()\n",
    "perclass_df['Accuracy'] = class_accuracy\n",
    "\n",
    "print(\"\\n\" + perclass_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "perclass_df.to_csv('../results/metrics/perclass_metrics_test.csv', index=False)\n",
    "print(\"\\n✓ Per-class metrics saved to results/metrics/perclass_metrics_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_perclass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class performance\n",
    "print(\"\\n=== VISUALIZING PER-CLASS PERFORMANCE ===\")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Precision',\n",
    "    x=ec_names_short,\n",
    "    y=perclass_df['Precision'],\n",
    "    text=[f'{v:.3f}' for v in perclass_df['Precision']],\n",
    "    textposition='auto',\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Recall',\n",
    "    x=ec_names_short,\n",
    "    y=perclass_df['Recall'],\n",
    "    text=[f'{v:.3f}' for v in perclass_df['Recall']],\n",
    "    textposition='auto',\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='F1-Score',\n",
    "    x=ec_names_short,\n",
    "    y=perclass_df['F1-Score'],\n",
    "    text=[f'{v:.3f}' for v in perclass_df['F1-Score']],\n",
    "    textposition='auto',\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Per-Class Performance Metrics (Test Set)',\n",
    "    xaxis_title='Enzyme Class',\n",
    "    yaxis_title='Score',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    barmode='group',\n",
    "    width=1000,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(\"✓ Per-class performance visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_worst_classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best and worst performing classes\n",
    "print(\"\\n=== BEST AND WORST PERFORMING CLASSES ===\")\n",
    "\n",
    "sorted_by_f1 = perclass_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 3 Classes (by F1-Score):\")\n",
    "for idx, row in sorted_by_f1.head(3).iterrows():\n",
    "    print(f\"  {row['Class']}. {row['Class_Name']}: F1={row['F1-Score']:.4f}, Acc={row['Accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nBottom 3 Classes (by F1-Score):\")\n",
    "for idx, row in sorted_by_f1.tail(3).iterrows():\n",
    "    print(f\"  {row['Class']}. {row['Class_Name']}: F1={row['F1-Score']:.4f}, Acc={row['Accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Performance analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error_section",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "misclassification_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications\n",
    "print(\"\\n=== MISCLASSIFICATION ANALYSIS (TEST SET) ===\")\n",
    "\n",
    "# Find misclassified samples\n",
    "misclassified_mask = test_pred_labels != test_true_labels\n",
    "num_misclassified = misclassified_mask.sum()\n",
    "total_samples = len(test_pred_labels)\n",
    "\n",
    "print(f\"\\nTotal misclassifications: {num_misclassified:,} / {total_samples:,} ({num_misclassified/total_samples*100:.2f}%)\")\n",
    "\n",
    "# Get misclassified samples with details\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'Index': np.where(misclassified_mask)[0],\n",
    "    'True_Label': test_true_labels[misclassified_mask],\n",
    "    'Pred_Label': test_pred_labels[misclassified_mask],\n",
    "    'True_Class': [ec_names[label] for label in test_true_labels[misclassified_mask]],\n",
    "    'Pred_Class': [ec_names[label] for label in test_pred_labels[misclassified_mask]],\n",
    "    'Confidence': test_confidence[misclassified_mask]\n",
    "})\n",
    "\n",
    "print(f\"\\nMisclassified samples DataFrame shape: {misclassified_df.shape}\")\n",
    "print(\"\\nFirst 10 misclassifications:\")\n",
    "print(misclassified_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common_errors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common error patterns\n",
    "print(\"\\n=== MOST COMMON ERROR PATTERNS ===\")\n",
    "\n",
    "error_pairs = misclassified_df.groupby(['True_Class', 'Pred_Class']).size().reset_index(name='Count')\n",
    "error_pairs = error_pairs.sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 confusion pairs:\")\n",
    "print(f\"\\n{'True Class':<20} {'Predicted As':<20} {'Count':>8} {'% of Errors':>12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for idx, row in error_pairs.head(10).iterrows():\n",
    "    pct = (row['Count'] / num_misclassified) * 100\n",
    "    print(f\"{row['True_Class']:<20} {row['Pred_Class']:<20} {row['Count']:>8} {pct:>11.2f}%\")\n",
    "\n",
    "# Save error analysis\n",
    "error_pairs.to_csv('../results/metrics/error_patterns.csv', index=False)\n",
    "misclassified_df.to_csv('../results/predictions/misclassified_samples.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Error patterns saved to results/metrics/error_patterns.csv\")\n",
    "print(\"✓ Misclassified samples saved to results/predictions/misclassified_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidence_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "print(\"\\n=== PREDICTION CONFIDENCE ANALYSIS ===\")\n",
    "\n",
    "# Compare confidence for correct vs incorrect predictions\n",
    "correct_mask = ~misclassified_mask\n",
    "correct_confidence = test_confidence[correct_mask]\n",
    "incorrect_confidence = test_confidence[misclassified_mask]\n",
    "\n",
    "print(f\"\\nCorrect predictions:\")\n",
    "print(f\"  Mean confidence: {correct_confidence.mean():.4f}\")\n",
    "print(f\"  Median confidence: {np.median(correct_confidence):.4f}\")\n",
    "print(f\"  Std confidence: {correct_confidence.std():.4f}\")\n",
    "\n",
    "print(f\"\\nIncorrect predictions:\")\n",
    "print(f\"  Mean confidence: {incorrect_confidence.mean():.4f}\")\n",
    "print(f\"  Median confidence: {np.median(incorrect_confidence):.4f}\")\n",
    "print(f\"  Std confidence: {incorrect_confidence.std():.4f}\")\n",
    "\n",
    "# Plot confidence distributions\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=correct_confidence,\n",
    "    name='Correct',\n",
    "    opacity=0.7,\n",
    "    nbinsx=50\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=incorrect_confidence,\n",
    "    name='Incorrect',\n",
    "    opacity=0.7,\n",
    "    nbinsx=50\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Prediction Confidence Distribution: Correct vs Incorrect',\n",
    "    xaxis_title='Confidence Score',\n",
    "    yaxis_title='Count',\n",
    "    barmode='overlay',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(\"\\n✓ Confidence analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "low_confidence_errors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine low-confidence errors\n",
    "print(\"\\n=== LOW CONFIDENCE MISCLASSIFICATIONS ===\")\n",
    "\n",
    "# Sort misclassified by confidence\n",
    "low_conf_errors = misclassified_df.sort_values('Confidence').head(20)\n",
    "\n",
    "print(\"\\nTop 20 lowest confidence errors:\")\n",
    "print(low_conf_errors[['True_Class', 'Pred_Class', 'Confidence']].to_string(index=False))\n",
    "\n",
    "# High confidence errors (model was wrong but very confident)\n",
    "high_conf_errors = misclassified_df.sort_values('Confidence', ascending=False).head(20)\n",
    "\n",
    "print(\"\\n=== HIGH CONFIDENCE MISCLASSIFICATIONS (Most Concerning) ===\")\n",
    "print(\"\\nTop 20 highest confidence errors:\")\n",
    "print(high_conf_errors[['True_Class', 'Pred_Class', 'Confidence']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nNumber of high-confidence errors (>0.9): {(misclassified_df['Confidence'] > 0.9).sum()}\")\n",
    "print(f\"Number of high-confidence errors (>0.8): {(misclassified_df['Confidence'] > 0.8).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_section",
   "metadata": {},
   "source": [
    "## 8. Save Predictions & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all predictions\n",
    "print(\"\\n=== SAVING PREDICTIONS ===\")\n",
    "\n",
    "# Validation predictions\n",
    "val_pred_df = pd.DataFrame({\n",
    "    'True_Label': val_true_labels,\n",
    "    'Pred_Label': val_pred_labels,\n",
    "    'True_Class': [ec_names[label] for label in val_true_labels],\n",
    "    'Pred_Class': [ec_names[label] for label in val_pred_labels],\n",
    "    'Confidence': val_confidence,\n",
    "    'Correct': val_true_labels == val_pred_labels\n",
    "})\n",
    "\n",
    "# Add probability for each class\n",
    "for i, class_name in enumerate(ec_names):\n",
    "    val_pred_df[f'Prob_{class_name}'] = val_probs[:, i]\n",
    "\n",
    "val_pred_df.to_csv('../results/predictions/validation_predictions.csv', index=False)\n",
    "print(f\"✓ Validation predictions saved ({len(val_pred_df):,} samples)\")\n",
    "\n",
    "# Test predictions\n",
    "test_pred_df = pd.DataFrame({\n",
    "    'True_Label': test_true_labels,\n",
    "    'Pred_Label': test_pred_labels,\n",
    "    'True_Class': [ec_names[label] for label in test_true_labels],\n",
    "    'Pred_Class': [ec_names[label] for label in test_pred_labels],\n",
    "    'Confidence': test_confidence,\n",
    "    'Correct': test_true_labels == test_pred_labels\n",
    "})\n",
    "\n",
    "# Add probability for each class\n",
    "for i, class_name in enumerate(ec_names):\n",
    "    test_pred_df[f'Prob_{class_name}'] = test_probs[:, i]\n",
    "\n",
    "test_pred_df.to_csv('../results/predictions/test_predictions.csv', index=False)\n",
    "print(f\"✓ Test predictions saved ({len(test_pred_df):,} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"\\n=== GENERATING SUMMARY REPORT ===\")\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "{'='*80}\n",
    "ENZYME CLASSIFICATION MODEL - EVALUATION SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "Model: {cfg['model']['name']}\n",
    "Training: LoRA Fine-tuning\n",
    "Number of Classes: {cfg['model']['num_labels']}\n",
    "\n",
    "{'='*80}\n",
    "OVERALL PERFORMANCE\n",
    "{'='*80}\n",
    "\n",
    "Validation Set ({len(val_ds):,} samples):\n",
    "  Accuracy:  {val_accuracy:.4f}\n",
    "  Precision: {val_precision:.4f}\n",
    "  Recall:    {val_recall:.4f}\n",
    "  F1-Score:  {val_f1:.4f}\n",
    "\n",
    "Test Set ({len(test_ds):,} samples):\n",
    "  Accuracy:  {test_accuracy:.4f}\n",
    "  Precision: {test_precision:.4f}\n",
    "  Recall:    {test_recall:.4f}\n",
    "  F1-Score:  {test_f1:.4f}\n",
    "\n",
    "{'='*80}\n",
    "PER-CLASS PERFORMANCE (Test Set)\n",
    "{'='*80}\n",
    "\n",
    "{perclass_df.to_string(index=False)}\n",
    "\n",
    "{'='*80}\n",
    "BEST PERFORMING CLASSES (by F1-Score)\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "for idx, row in sorted_by_f1.head(3).iterrows():\n",
    "    summary_report += f\"\\n{row['Class']}. {row['Class_Name']}: F1={row['F1-Score']:.4f}, Acc={row['Accuracy']:.4f}\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "{'='*80}\n",
    "WORST PERFORMING CLASSES (by F1-Score)\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "for idx, row in sorted_by_f1.tail(3).iterrows():\n",
    "    summary_report += f\"\\n{row['Class']}. {row['Class_Name']}: F1={row['F1-Score']:.4f}, Acc={row['Accuracy']:.4f}\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "{'='*80}\n",
    "ERROR ANALYSIS\n",
    "{'='*80}\n",
    "\n",
    "Total Misclassifications: {num_misclassified:,} / {total_samples:,} ({num_misclassified/total_samples*100:.2f}%)\n",
    "\n",
    "Confidence Statistics:\n",
    "  Correct Predictions:   Mean={correct_confidence.mean():.4f}, Std={correct_confidence.std():.4f}\n",
    "  Incorrect Predictions: Mean={incorrect_confidence.mean():.4f}, Std={incorrect_confidence.std():.4f}\n",
    "\n",
    "High Confidence Errors (>0.9): {(misclassified_df['Confidence'] > 0.9).sum()}\n",
    "High Confidence Errors (>0.8): {(misclassified_df['Confidence'] > 0.8).sum()}\n",
    "\n",
    "Most Common Error Patterns:\n",
    "\"\"\"\n",
    "\n",
    "for idx, row in error_pairs.head(5).iterrows():\n",
    "    pct = (row['Count'] / num_misclassified) * 100\n",
    "    summary_report += f\"\\n  {row['True_Class']} → {row['Pred_Class']}: {row['Count']} ({pct:.2f}%)\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "{'='*80}\n",
    "OUTPUT FILES\n",
    "{'='*80}\n",
    "\n",
    "Predictions:\n",
    "  - results/predictions/validation_predictions.csv\n",
    "  - results/predictions/test_predictions.csv\n",
    "  - results/predictions/misclassified_samples.csv\n",
    "\n",
    "Metrics:\n",
    "  - results/metrics/overall_metrics.csv\n",
    "  - results/metrics/perclass_metrics_test.csv\n",
    "  - results/metrics/classification_report_test.txt\n",
    "  - results/metrics/error_patterns.csv\n",
    "\n",
    "Figures:\n",
    "  - results/figures/confusion_matrix_test.png\n",
    "  - results/figures/confusion_matrix_val.png\n",
    "\n",
    "{'='*80}\n",
    "END OF REPORT\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "with open('../results/evaluation_summary.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(summary_report)\n",
    "print(\"\\n✓ Summary report saved to results/evaluation_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
