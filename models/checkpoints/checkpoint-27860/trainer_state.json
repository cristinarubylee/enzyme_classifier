{
  "best_global_step": 27860,
  "best_metric": 0.9641429070553534,
  "best_model_checkpoint": "..\\models\\checkpoints\\checkpoint-27860",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 27860,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 7.178750897343863e-05,
      "grad_norm": 2.2443807125091553,
      "learning_rate": 0.0,
      "loss": 1.9568,
      "step": 1
    },
    {
      "epoch": 0.0007178750897343862,
      "grad_norm": 2.565307855606079,
      "learning_rate": 4.3072505384063175e-07,
      "loss": 1.9357,
      "step": 10
    },
    {
      "epoch": 0.0014357501794687725,
      "grad_norm": 1.2856775522232056,
      "learning_rate": 9.093084469968892e-07,
      "loss": 1.9221,
      "step": 20
    },
    {
      "epoch": 0.0021536252692031586,
      "grad_norm": 1.0540913343429565,
      "learning_rate": 1.3878918401531467e-06,
      "loss": 1.9348,
      "step": 30
    },
    {
      "epoch": 0.002871500358937545,
      "grad_norm": 2.216089963912964,
      "learning_rate": 1.8664752333094043e-06,
      "loss": 1.9577,
      "step": 40
    },
    {
      "epoch": 0.003589375448671931,
      "grad_norm": 1.276774525642395,
      "learning_rate": 2.3450586264656617e-06,
      "loss": 1.9291,
      "step": 50
    },
    {
      "epoch": 0.004307250538406317,
      "grad_norm": 1.6183334589004517,
      "learning_rate": 2.823642019621919e-06,
      "loss": 1.9286,
      "step": 60
    },
    {
      "epoch": 0.005025125628140704,
      "grad_norm": 2.501128911972046,
      "learning_rate": 3.3022254127781766e-06,
      "loss": 1.9422,
      "step": 70
    },
    {
      "epoch": 0.00574300071787509,
      "grad_norm": 0.9974015355110168,
      "learning_rate": 3.780808805934434e-06,
      "loss": 1.9344,
      "step": 80
    },
    {
      "epoch": 0.006460875807609476,
      "grad_norm": 1.145450234413147,
      "learning_rate": 4.259392199090692e-06,
      "loss": 1.9481,
      "step": 90
    },
    {
      "epoch": 0.007178750897343862,
      "grad_norm": 1.1056952476501465,
      "learning_rate": 4.73797559224695e-06,
      "loss": 1.9415,
      "step": 100
    },
    {
      "epoch": 0.007896625987078248,
      "grad_norm": 1.0103105306625366,
      "learning_rate": 5.216558985403207e-06,
      "loss": 1.9318,
      "step": 110
    },
    {
      "epoch": 0.008614501076812634,
      "grad_norm": 2.1496083736419678,
      "learning_rate": 5.6951423785594645e-06,
      "loss": 1.9239,
      "step": 120
    },
    {
      "epoch": 0.00933237616654702,
      "grad_norm": 1.3574650287628174,
      "learning_rate": 6.173725771715722e-06,
      "loss": 1.9339,
      "step": 130
    },
    {
      "epoch": 0.010050251256281407,
      "grad_norm": 1.2681936025619507,
      "learning_rate": 6.652309164871979e-06,
      "loss": 1.9339,
      "step": 140
    },
    {
      "epoch": 0.010768126346015794,
      "grad_norm": 1.3008111715316772,
      "learning_rate": 7.130892558028236e-06,
      "loss": 1.891,
      "step": 150
    },
    {
      "epoch": 0.01148600143575018,
      "grad_norm": 1.5573484897613525,
      "learning_rate": 7.609475951184494e-06,
      "loss": 1.9174,
      "step": 160
    },
    {
      "epoch": 0.012203876525484566,
      "grad_norm": 1.5936486721038818,
      "learning_rate": 8.088059344340752e-06,
      "loss": 1.9233,
      "step": 170
    },
    {
      "epoch": 0.012921751615218953,
      "grad_norm": 1.6593544483184814,
      "learning_rate": 8.566642737497008e-06,
      "loss": 1.9271,
      "step": 180
    },
    {
      "epoch": 0.013639626704953339,
      "grad_norm": 1.5412921905517578,
      "learning_rate": 9.045226130653267e-06,
      "loss": 1.9111,
      "step": 190
    },
    {
      "epoch": 0.014357501794687724,
      "grad_norm": 1.3005762100219727,
      "learning_rate": 9.523809523809523e-06,
      "loss": 1.905,
      "step": 200
    },
    {
      "epoch": 0.01507537688442211,
      "grad_norm": 2.003216028213501,
      "learning_rate": 1.0002392916965782e-05,
      "loss": 1.9019,
      "step": 210
    },
    {
      "epoch": 0.015793251974156496,
      "grad_norm": 1.508143424987793,
      "learning_rate": 1.048097631012204e-05,
      "loss": 1.9105,
      "step": 220
    },
    {
      "epoch": 0.016511127063890883,
      "grad_norm": 1.9425115585327148,
      "learning_rate": 1.0959559703278297e-05,
      "loss": 1.9011,
      "step": 230
    },
    {
      "epoch": 0.01722900215362527,
      "grad_norm": 0.7284009456634521,
      "learning_rate": 1.1438143096434554e-05,
      "loss": 1.9182,
      "step": 240
    },
    {
      "epoch": 0.017946877243359655,
      "grad_norm": 1.8167644739151,
      "learning_rate": 1.1916726489590812e-05,
      "loss": 1.9072,
      "step": 250
    },
    {
      "epoch": 0.01866475233309404,
      "grad_norm": 1.2865263223648071,
      "learning_rate": 1.239530988274707e-05,
      "loss": 1.9043,
      "step": 260
    },
    {
      "epoch": 0.019382627422828428,
      "grad_norm": 1.484291434288025,
      "learning_rate": 1.2873893275903327e-05,
      "loss": 1.9121,
      "step": 270
    },
    {
      "epoch": 0.020100502512562814,
      "grad_norm": 2.0690457820892334,
      "learning_rate": 1.3352476669059586e-05,
      "loss": 1.9012,
      "step": 280
    },
    {
      "epoch": 0.0208183776022972,
      "grad_norm": 1.9599432945251465,
      "learning_rate": 1.3831060062215842e-05,
      "loss": 1.8854,
      "step": 290
    },
    {
      "epoch": 0.021536252692031587,
      "grad_norm": 1.151863694190979,
      "learning_rate": 1.4309643455372099e-05,
      "loss": 1.8832,
      "step": 300
    },
    {
      "epoch": 0.022254127781765973,
      "grad_norm": 2.4154529571533203,
      "learning_rate": 1.4788226848528358e-05,
      "loss": 1.8812,
      "step": 310
    },
    {
      "epoch": 0.02297200287150036,
      "grad_norm": 1.7338829040527344,
      "learning_rate": 1.5266810241684614e-05,
      "loss": 1.9107,
      "step": 320
    },
    {
      "epoch": 0.023689877961234746,
      "grad_norm": 1.2140504121780396,
      "learning_rate": 1.574539363484087e-05,
      "loss": 1.9016,
      "step": 330
    },
    {
      "epoch": 0.024407753050969132,
      "grad_norm": 1.81125807762146,
      "learning_rate": 1.622397702799713e-05,
      "loss": 1.9119,
      "step": 340
    },
    {
      "epoch": 0.02512562814070352,
      "grad_norm": 1.7674694061279297,
      "learning_rate": 1.6702560421153386e-05,
      "loss": 1.905,
      "step": 350
    },
    {
      "epoch": 0.025843503230437905,
      "grad_norm": 1.6668033599853516,
      "learning_rate": 1.7181143814309644e-05,
      "loss": 1.8963,
      "step": 360
    },
    {
      "epoch": 0.02656137832017229,
      "grad_norm": 1.1810059547424316,
      "learning_rate": 1.76597272074659e-05,
      "loss": 1.8769,
      "step": 370
    },
    {
      "epoch": 0.027279253409906678,
      "grad_norm": 1.7451016902923584,
      "learning_rate": 1.8138310600622162e-05,
      "loss": 1.9161,
      "step": 380
    },
    {
      "epoch": 0.02799712849964106,
      "grad_norm": 1.1852329969406128,
      "learning_rate": 1.8616893993778416e-05,
      "loss": 1.8557,
      "step": 390
    },
    {
      "epoch": 0.028715003589375447,
      "grad_norm": 2.6491968631744385,
      "learning_rate": 1.9095477386934673e-05,
      "loss": 1.9002,
      "step": 400
    },
    {
      "epoch": 0.029432878679109833,
      "grad_norm": 1.6303825378417969,
      "learning_rate": 1.9574060780090934e-05,
      "loss": 1.8836,
      "step": 410
    },
    {
      "epoch": 0.03015075376884422,
      "grad_norm": 0.9596300721168518,
      "learning_rate": 2.005264417324719e-05,
      "loss": 1.8815,
      "step": 420
    },
    {
      "epoch": 0.030868628858578606,
      "grad_norm": 2.0150625705718994,
      "learning_rate": 2.0531227566403445e-05,
      "loss": 1.8421,
      "step": 430
    },
    {
      "epoch": 0.03158650394831299,
      "grad_norm": 1.2371537685394287,
      "learning_rate": 2.1009810959559703e-05,
      "loss": 1.8913,
      "step": 440
    },
    {
      "epoch": 0.03230437903804738,
      "grad_norm": 1.2437065839767456,
      "learning_rate": 2.1488394352715964e-05,
      "loss": 1.9136,
      "step": 450
    },
    {
      "epoch": 0.033022254127781765,
      "grad_norm": 2.2579267024993896,
      "learning_rate": 2.1966977745872218e-05,
      "loss": 1.9229,
      "step": 460
    },
    {
      "epoch": 0.03374012921751615,
      "grad_norm": 1.4224483966827393,
      "learning_rate": 2.2445561139028475e-05,
      "loss": 1.8696,
      "step": 470
    },
    {
      "epoch": 0.03445800430725054,
      "grad_norm": 1.451357364654541,
      "learning_rate": 2.2924144532184736e-05,
      "loss": 1.8942,
      "step": 480
    },
    {
      "epoch": 0.035175879396984924,
      "grad_norm": 0.9637187123298645,
      "learning_rate": 2.3402727925340993e-05,
      "loss": 1.8321,
      "step": 490
    },
    {
      "epoch": 0.03589375448671931,
      "grad_norm": 2.0366294384002686,
      "learning_rate": 2.3881311318497247e-05,
      "loss": 1.8452,
      "step": 500
    },
    {
      "epoch": 0.0366116295764537,
      "grad_norm": 1.0826271772384644,
      "learning_rate": 2.4359894711653508e-05,
      "loss": 1.8962,
      "step": 510
    },
    {
      "epoch": 0.03732950466618808,
      "grad_norm": 2.024017333984375,
      "learning_rate": 2.4838478104809766e-05,
      "loss": 1.873,
      "step": 520
    },
    {
      "epoch": 0.03804737975592247,
      "grad_norm": 2.615903854370117,
      "learning_rate": 2.5317061497966023e-05,
      "loss": 1.9043,
      "step": 530
    },
    {
      "epoch": 0.038765254845656856,
      "grad_norm": 1.0314217805862427,
      "learning_rate": 2.5795644891122277e-05,
      "loss": 1.8677,
      "step": 540
    },
    {
      "epoch": 0.03948312993539124,
      "grad_norm": 2.5536932945251465,
      "learning_rate": 2.6274228284278535e-05,
      "loss": 1.8159,
      "step": 550
    },
    {
      "epoch": 0.04020100502512563,
      "grad_norm": 1.8632893562316895,
      "learning_rate": 2.6752811677434795e-05,
      "loss": 1.9295,
      "step": 560
    },
    {
      "epoch": 0.040918880114860015,
      "grad_norm": 1.3956190347671509,
      "learning_rate": 2.7231395070591053e-05,
      "loss": 1.8864,
      "step": 570
    },
    {
      "epoch": 0.0416367552045944,
      "grad_norm": 2.197035074234009,
      "learning_rate": 2.770997846374731e-05,
      "loss": 1.8535,
      "step": 580
    },
    {
      "epoch": 0.04235463029432879,
      "grad_norm": 1.1177005767822266,
      "learning_rate": 2.8188561856903568e-05,
      "loss": 1.8338,
      "step": 590
    },
    {
      "epoch": 0.043072505384063174,
      "grad_norm": 1.2030924558639526,
      "learning_rate": 2.8667145250059825e-05,
      "loss": 1.8391,
      "step": 600
    },
    {
      "epoch": 0.04379038047379756,
      "grad_norm": 2.2328665256500244,
      "learning_rate": 2.914572864321608e-05,
      "loss": 1.8409,
      "step": 610
    },
    {
      "epoch": 0.04450825556353195,
      "grad_norm": 1.3226879835128784,
      "learning_rate": 2.9624312036372337e-05,
      "loss": 1.8205,
      "step": 620
    },
    {
      "epoch": 0.04522613065326633,
      "grad_norm": 1.7998862266540527,
      "learning_rate": 3.01028954295286e-05,
      "loss": 1.8197,
      "step": 630
    },
    {
      "epoch": 0.04594400574300072,
      "grad_norm": 2.222020387649536,
      "learning_rate": 3.0581478822684855e-05,
      "loss": 1.9478,
      "step": 640
    },
    {
      "epoch": 0.046661880832735106,
      "grad_norm": 1.8798229694366455,
      "learning_rate": 3.106006221584111e-05,
      "loss": 1.8183,
      "step": 650
    },
    {
      "epoch": 0.04737975592246949,
      "grad_norm": 1.698262095451355,
      "learning_rate": 3.153864560899737e-05,
      "loss": 1.8574,
      "step": 660
    },
    {
      "epoch": 0.04809763101220388,
      "grad_norm": 1.603096604347229,
      "learning_rate": 3.201722900215363e-05,
      "loss": 1.881,
      "step": 670
    },
    {
      "epoch": 0.048815506101938265,
      "grad_norm": 1.2774364948272705,
      "learning_rate": 3.2495812395309884e-05,
      "loss": 1.8413,
      "step": 680
    },
    {
      "epoch": 0.04953338119167265,
      "grad_norm": 1.5902067422866821,
      "learning_rate": 3.297439578846614e-05,
      "loss": 1.8358,
      "step": 690
    },
    {
      "epoch": 0.05025125628140704,
      "grad_norm": 1.68280029296875,
      "learning_rate": 3.34529791816224e-05,
      "loss": 1.7922,
      "step": 700
    },
    {
      "epoch": 0.050969131371141424,
      "grad_norm": 1.599617600440979,
      "learning_rate": 3.393156257477866e-05,
      "loss": 1.8275,
      "step": 710
    },
    {
      "epoch": 0.05168700646087581,
      "grad_norm": 1.6982134580612183,
      "learning_rate": 3.4410145967934914e-05,
      "loss": 1.8144,
      "step": 720
    },
    {
      "epoch": 0.0524048815506102,
      "grad_norm": 1.9980179071426392,
      "learning_rate": 3.488872936109117e-05,
      "loss": 1.7624,
      "step": 730
    },
    {
      "epoch": 0.05312275664034458,
      "grad_norm": 1.4489531517028809,
      "learning_rate": 3.536731275424743e-05,
      "loss": 1.8356,
      "step": 740
    },
    {
      "epoch": 0.05384063173007897,
      "grad_norm": 1.7002869844436646,
      "learning_rate": 3.5845896147403686e-05,
      "loss": 1.8743,
      "step": 750
    },
    {
      "epoch": 0.054558506819813356,
      "grad_norm": 2.1740975379943848,
      "learning_rate": 3.6324479540559944e-05,
      "loss": 1.7607,
      "step": 760
    },
    {
      "epoch": 0.05527638190954774,
      "grad_norm": 1.541532278060913,
      "learning_rate": 3.68030629337162e-05,
      "loss": 1.7592,
      "step": 770
    },
    {
      "epoch": 0.05599425699928212,
      "grad_norm": 1.8421859741210938,
      "learning_rate": 3.728164632687246e-05,
      "loss": 1.7964,
      "step": 780
    },
    {
      "epoch": 0.05671213208901651,
      "grad_norm": 1.821946382522583,
      "learning_rate": 3.7760229720028716e-05,
      "loss": 1.7281,
      "step": 790
    },
    {
      "epoch": 0.057430007178750894,
      "grad_norm": 2.726100206375122,
      "learning_rate": 3.8238813113184974e-05,
      "loss": 1.6456,
      "step": 800
    },
    {
      "epoch": 0.05814788226848528,
      "grad_norm": 2.074702262878418,
      "learning_rate": 3.871739650634123e-05,
      "loss": 1.701,
      "step": 810
    },
    {
      "epoch": 0.05886575735821967,
      "grad_norm": 1.9369333982467651,
      "learning_rate": 3.919597989949749e-05,
      "loss": 1.7419,
      "step": 820
    },
    {
      "epoch": 0.05958363244795405,
      "grad_norm": 1.8708157539367676,
      "learning_rate": 3.9674563292653746e-05,
      "loss": 1.7963,
      "step": 830
    },
    {
      "epoch": 0.06030150753768844,
      "grad_norm": 2.3492534160614014,
      "learning_rate": 4.015314668581e-05,
      "loss": 1.7238,
      "step": 840
    },
    {
      "epoch": 0.061019382627422826,
      "grad_norm": 1.5069295167922974,
      "learning_rate": 4.063173007896626e-05,
      "loss": 1.7361,
      "step": 850
    },
    {
      "epoch": 0.06173725771715721,
      "grad_norm": 2.4299814701080322,
      "learning_rate": 4.111031347212252e-05,
      "loss": 1.6967,
      "step": 860
    },
    {
      "epoch": 0.0624551328068916,
      "grad_norm": 2.202486276626587,
      "learning_rate": 4.1588896865278775e-05,
      "loss": 1.7011,
      "step": 870
    },
    {
      "epoch": 0.06317300789662599,
      "grad_norm": 2.4245569705963135,
      "learning_rate": 4.206748025843503e-05,
      "loss": 1.694,
      "step": 880
    },
    {
      "epoch": 0.06389088298636038,
      "grad_norm": 2.711670160293579,
      "learning_rate": 4.254606365159129e-05,
      "loss": 1.6484,
      "step": 890
    },
    {
      "epoch": 0.06460875807609476,
      "grad_norm": 2.273820161819458,
      "learning_rate": 4.3024647044747555e-05,
      "loss": 1.6244,
      "step": 900
    },
    {
      "epoch": 0.06532663316582915,
      "grad_norm": 3.159074544906616,
      "learning_rate": 4.3503230437903805e-05,
      "loss": 1.6236,
      "step": 910
    },
    {
      "epoch": 0.06604450825556353,
      "grad_norm": 2.4758360385894775,
      "learning_rate": 4.398181383106006e-05,
      "loss": 1.6669,
      "step": 920
    },
    {
      "epoch": 0.06676238334529792,
      "grad_norm": 2.3365976810455322,
      "learning_rate": 4.446039722421632e-05,
      "loss": 1.5381,
      "step": 930
    },
    {
      "epoch": 0.0674802584350323,
      "grad_norm": 2.6437838077545166,
      "learning_rate": 4.493898061737258e-05,
      "loss": 1.5091,
      "step": 940
    },
    {
      "epoch": 0.0681981335247667,
      "grad_norm": 2.309617757797241,
      "learning_rate": 4.5417564010528835e-05,
      "loss": 1.5474,
      "step": 950
    },
    {
      "epoch": 0.06891600861450108,
      "grad_norm": 2.955366611480713,
      "learning_rate": 4.58961474036851e-05,
      "loss": 1.5733,
      "step": 960
    },
    {
      "epoch": 0.06963388370423547,
      "grad_norm": 2.0069541931152344,
      "learning_rate": 4.6374730796841356e-05,
      "loss": 1.4538,
      "step": 970
    },
    {
      "epoch": 0.07035175879396985,
      "grad_norm": 2.2787604331970215,
      "learning_rate": 4.685331418999761e-05,
      "loss": 1.6158,
      "step": 980
    },
    {
      "epoch": 0.07106963388370424,
      "grad_norm": 3.990144729614258,
      "learning_rate": 4.7331897583153865e-05,
      "loss": 1.4793,
      "step": 990
    },
    {
      "epoch": 0.07178750897343862,
      "grad_norm": 3.4244203567504883,
      "learning_rate": 4.781048097631012e-05,
      "loss": 1.6441,
      "step": 1000
    },
    {
      "epoch": 0.07250538406317301,
      "grad_norm": 5.179141044616699,
      "learning_rate": 4.828906436946638e-05,
      "loss": 1.4999,
      "step": 1010
    },
    {
      "epoch": 0.0732232591529074,
      "grad_norm": 3.4903416633605957,
      "learning_rate": 4.876764776262264e-05,
      "loss": 1.505,
      "step": 1020
    },
    {
      "epoch": 0.07394113424264179,
      "grad_norm": 2.5894253253936768,
      "learning_rate": 4.92462311557789e-05,
      "loss": 1.4935,
      "step": 1030
    },
    {
      "epoch": 0.07465900933237617,
      "grad_norm": 4.7708420753479,
      "learning_rate": 4.972481454893516e-05,
      "loss": 1.4477,
      "step": 1040
    },
    {
      "epoch": 0.07537688442211055,
      "grad_norm": 2.4397072792053223,
      "learning_rate": 5.020339794209141e-05,
      "loss": 1.4105,
      "step": 1050
    },
    {
      "epoch": 0.07609475951184494,
      "grad_norm": 2.709057569503784,
      "learning_rate": 5.068198133524767e-05,
      "loss": 1.4431,
      "step": 1060
    },
    {
      "epoch": 0.07681263460157932,
      "grad_norm": 2.3625783920288086,
      "learning_rate": 5.1160564728403924e-05,
      "loss": 1.5343,
      "step": 1070
    },
    {
      "epoch": 0.07753050969131371,
      "grad_norm": 4.42650842666626,
      "learning_rate": 5.163914812156019e-05,
      "loss": 1.3878,
      "step": 1080
    },
    {
      "epoch": 0.07824838478104809,
      "grad_norm": 2.8134679794311523,
      "learning_rate": 5.211773151471644e-05,
      "loss": 1.4475,
      "step": 1090
    },
    {
      "epoch": 0.07896625987078248,
      "grad_norm": 3.06288743019104,
      "learning_rate": 5.25963149078727e-05,
      "loss": 1.413,
      "step": 1100
    },
    {
      "epoch": 0.07968413496051686,
      "grad_norm": 3.0025134086608887,
      "learning_rate": 5.3074898301028954e-05,
      "loss": 1.4636,
      "step": 1110
    },
    {
      "epoch": 0.08040201005025126,
      "grad_norm": 2.1776576042175293,
      "learning_rate": 5.355348169418522e-05,
      "loss": 1.4215,
      "step": 1120
    },
    {
      "epoch": 0.08111988513998564,
      "grad_norm": 4.126188278198242,
      "learning_rate": 5.4032065087341475e-05,
      "loss": 1.3194,
      "step": 1130
    },
    {
      "epoch": 0.08183776022972003,
      "grad_norm": 3.1780917644500732,
      "learning_rate": 5.4510648480497726e-05,
      "loss": 1.3628,
      "step": 1140
    },
    {
      "epoch": 0.08255563531945441,
      "grad_norm": 2.6346211433410645,
      "learning_rate": 5.498923187365399e-05,
      "loss": 1.4062,
      "step": 1150
    },
    {
      "epoch": 0.0832735104091888,
      "grad_norm": 3.326159715652466,
      "learning_rate": 5.546781526681024e-05,
      "loss": 1.3053,
      "step": 1160
    },
    {
      "epoch": 0.08399138549892318,
      "grad_norm": 3.6236445903778076,
      "learning_rate": 5.5946398659966505e-05,
      "loss": 1.435,
      "step": 1170
    },
    {
      "epoch": 0.08470926058865758,
      "grad_norm": 3.560957431793213,
      "learning_rate": 5.6424982053122756e-05,
      "loss": 1.3781,
      "step": 1180
    },
    {
      "epoch": 0.08542713567839195,
      "grad_norm": 3.8284475803375244,
      "learning_rate": 5.690356544627902e-05,
      "loss": 1.2626,
      "step": 1190
    },
    {
      "epoch": 0.08614501076812635,
      "grad_norm": 2.075970411300659,
      "learning_rate": 5.738214883943528e-05,
      "loss": 1.3245,
      "step": 1200
    },
    {
      "epoch": 0.08686288585786073,
      "grad_norm": 3.9108822345733643,
      "learning_rate": 5.786073223259153e-05,
      "loss": 1.4092,
      "step": 1210
    },
    {
      "epoch": 0.08758076094759512,
      "grad_norm": 3.8019468784332275,
      "learning_rate": 5.833931562574779e-05,
      "loss": 1.2928,
      "step": 1220
    },
    {
      "epoch": 0.0882986360373295,
      "grad_norm": 3.748494863510132,
      "learning_rate": 5.881789901890404e-05,
      "loss": 1.3432,
      "step": 1230
    },
    {
      "epoch": 0.0890165111270639,
      "grad_norm": 3.99947452545166,
      "learning_rate": 5.929648241206031e-05,
      "loss": 1.3838,
      "step": 1240
    },
    {
      "epoch": 0.08973438621679827,
      "grad_norm": 3.401789665222168,
      "learning_rate": 5.977506580521656e-05,
      "loss": 1.2676,
      "step": 1250
    },
    {
      "epoch": 0.09045226130653267,
      "grad_norm": 5.539731025695801,
      "learning_rate": 6.025364919837282e-05,
      "loss": 1.2266,
      "step": 1260
    },
    {
      "epoch": 0.09117013639626705,
      "grad_norm": 3.4242360591888428,
      "learning_rate": 6.073223259152908e-05,
      "loss": 1.2603,
      "step": 1270
    },
    {
      "epoch": 0.09188801148600144,
      "grad_norm": 3.390345335006714,
      "learning_rate": 6.121081598468533e-05,
      "loss": 1.1189,
      "step": 1280
    },
    {
      "epoch": 0.09260588657573582,
      "grad_norm": 4.255853652954102,
      "learning_rate": 6.16893993778416e-05,
      "loss": 1.3492,
      "step": 1290
    },
    {
      "epoch": 0.09332376166547021,
      "grad_norm": 5.684360980987549,
      "learning_rate": 6.216798277099784e-05,
      "loss": 1.2454,
      "step": 1300
    },
    {
      "epoch": 0.09404163675520459,
      "grad_norm": 3.348914384841919,
      "learning_rate": 6.26465661641541e-05,
      "loss": 1.2322,
      "step": 1310
    },
    {
      "epoch": 0.09475951184493898,
      "grad_norm": 2.8491337299346924,
      "learning_rate": 6.312514955731036e-05,
      "loss": 1.3105,
      "step": 1320
    },
    {
      "epoch": 0.09547738693467336,
      "grad_norm": 4.290123462677002,
      "learning_rate": 6.360373295046662e-05,
      "loss": 1.0627,
      "step": 1330
    },
    {
      "epoch": 0.09619526202440776,
      "grad_norm": 3.3092825412750244,
      "learning_rate": 6.408231634362289e-05,
      "loss": 1.2538,
      "step": 1340
    },
    {
      "epoch": 0.09691313711414214,
      "grad_norm": 2.836055278778076,
      "learning_rate": 6.456089973677913e-05,
      "loss": 1.1857,
      "step": 1350
    },
    {
      "epoch": 0.09763101220387653,
      "grad_norm": 5.234561443328857,
      "learning_rate": 6.50394831299354e-05,
      "loss": 1.237,
      "step": 1360
    },
    {
      "epoch": 0.09834888729361091,
      "grad_norm": 4.0797834396362305,
      "learning_rate": 6.551806652309165e-05,
      "loss": 1.3178,
      "step": 1370
    },
    {
      "epoch": 0.0990667623833453,
      "grad_norm": 4.727780818939209,
      "learning_rate": 6.59966499162479e-05,
      "loss": 1.0666,
      "step": 1380
    },
    {
      "epoch": 0.09978463747307968,
      "grad_norm": 8.29603385925293,
      "learning_rate": 6.647523330940418e-05,
      "loss": 1.0612,
      "step": 1390
    },
    {
      "epoch": 0.10050251256281408,
      "grad_norm": 8.412070274353027,
      "learning_rate": 6.695381670256042e-05,
      "loss": 1.0741,
      "step": 1400
    },
    {
      "epoch": 0.10122038765254845,
      "grad_norm": 5.2341532707214355,
      "learning_rate": 6.743240009571669e-05,
      "loss": 1.2502,
      "step": 1410
    },
    {
      "epoch": 0.10193826274228285,
      "grad_norm": 3.5218753814697266,
      "learning_rate": 6.791098348887293e-05,
      "loss": 1.1084,
      "step": 1420
    },
    {
      "epoch": 0.10265613783201723,
      "grad_norm": 4.137821197509766,
      "learning_rate": 6.83895668820292e-05,
      "loss": 1.067,
      "step": 1430
    },
    {
      "epoch": 0.10337401292175162,
      "grad_norm": 8.240632057189941,
      "learning_rate": 6.886815027518545e-05,
      "loss": 1.299,
      "step": 1440
    },
    {
      "epoch": 0.104091888011486,
      "grad_norm": 6.401393413543701,
      "learning_rate": 6.93467336683417e-05,
      "loss": 1.2005,
      "step": 1450
    },
    {
      "epoch": 0.1048097631012204,
      "grad_norm": 5.582906723022461,
      "learning_rate": 6.982531706149798e-05,
      "loss": 1.1599,
      "step": 1460
    },
    {
      "epoch": 0.10552763819095477,
      "grad_norm": 5.363931655883789,
      "learning_rate": 7.030390045465422e-05,
      "loss": 1.083,
      "step": 1470
    },
    {
      "epoch": 0.10624551328068917,
      "grad_norm": 6.151541233062744,
      "learning_rate": 7.078248384781049e-05,
      "loss": 1.0527,
      "step": 1480
    },
    {
      "epoch": 0.10696338837042355,
      "grad_norm": 4.001503944396973,
      "learning_rate": 7.126106724096674e-05,
      "loss": 1.0438,
      "step": 1490
    },
    {
      "epoch": 0.10768126346015794,
      "grad_norm": 6.217864513397217,
      "learning_rate": 7.1739650634123e-05,
      "loss": 1.1803,
      "step": 1500
    },
    {
      "epoch": 0.10839913854989232,
      "grad_norm": 2.8016159534454346,
      "learning_rate": 7.221823402727925e-05,
      "loss": 1.1704,
      "step": 1510
    },
    {
      "epoch": 0.10911701363962671,
      "grad_norm": 4.4364142417907715,
      "learning_rate": 7.269681742043551e-05,
      "loss": 1.1072,
      "step": 1520
    },
    {
      "epoch": 0.10983488872936109,
      "grad_norm": 2.4713282585144043,
      "learning_rate": 7.317540081359178e-05,
      "loss": 0.9767,
      "step": 1530
    },
    {
      "epoch": 0.11055276381909548,
      "grad_norm": 6.3961005210876465,
      "learning_rate": 7.365398420674802e-05,
      "loss": 1.1699,
      "step": 1540
    },
    {
      "epoch": 0.11127063890882986,
      "grad_norm": 2.6396937370300293,
      "learning_rate": 7.41325675999043e-05,
      "loss": 1.1372,
      "step": 1550
    },
    {
      "epoch": 0.11198851399856424,
      "grad_norm": 7.047487258911133,
      "learning_rate": 7.461115099306054e-05,
      "loss": 1.1002,
      "step": 1560
    },
    {
      "epoch": 0.11270638908829864,
      "grad_norm": 2.6243550777435303,
      "learning_rate": 7.508973438621681e-05,
      "loss": 1.0593,
      "step": 1570
    },
    {
      "epoch": 0.11342426417803302,
      "grad_norm": 4.237468719482422,
      "learning_rate": 7.556831777937305e-05,
      "loss": 1.1633,
      "step": 1580
    },
    {
      "epoch": 0.11414213926776741,
      "grad_norm": 4.69016695022583,
      "learning_rate": 7.604690117252932e-05,
      "loss": 1.1564,
      "step": 1590
    },
    {
      "epoch": 0.11486001435750179,
      "grad_norm": 3.33019757270813,
      "learning_rate": 7.652548456568558e-05,
      "loss": 0.9475,
      "step": 1600
    },
    {
      "epoch": 0.11557788944723618,
      "grad_norm": 7.647523403167725,
      "learning_rate": 7.700406795884182e-05,
      "loss": 0.963,
      "step": 1610
    },
    {
      "epoch": 0.11629576453697056,
      "grad_norm": 6.932960510253906,
      "learning_rate": 7.74826513519981e-05,
      "loss": 1.1205,
      "step": 1620
    },
    {
      "epoch": 0.11701363962670495,
      "grad_norm": 6.048612594604492,
      "learning_rate": 7.796123474515434e-05,
      "loss": 1.0081,
      "step": 1630
    },
    {
      "epoch": 0.11773151471643933,
      "grad_norm": 9.383432388305664,
      "learning_rate": 7.843981813831061e-05,
      "loss": 1.0969,
      "step": 1640
    },
    {
      "epoch": 0.11844938980617373,
      "grad_norm": 6.47802734375,
      "learning_rate": 7.891840153146685e-05,
      "loss": 1.1282,
      "step": 1650
    },
    {
      "epoch": 0.1191672648959081,
      "grad_norm": 6.443363666534424,
      "learning_rate": 7.939698492462313e-05,
      "loss": 1.0008,
      "step": 1660
    },
    {
      "epoch": 0.1198851399856425,
      "grad_norm": 5.676765441894531,
      "learning_rate": 7.987556831777938e-05,
      "loss": 0.8855,
      "step": 1670
    },
    {
      "epoch": 0.12060301507537688,
      "grad_norm": 11.1361722946167,
      "learning_rate": 8.035415171093563e-05,
      "loss": 1.1525,
      "step": 1680
    },
    {
      "epoch": 0.12132089016511127,
      "grad_norm": 5.4564924240112305,
      "learning_rate": 8.08327351040919e-05,
      "loss": 1.0345,
      "step": 1690
    },
    {
      "epoch": 0.12203876525484565,
      "grad_norm": 5.289679527282715,
      "learning_rate": 8.131131849724814e-05,
      "loss": 0.7943,
      "step": 1700
    },
    {
      "epoch": 0.12275664034458005,
      "grad_norm": 5.74692964553833,
      "learning_rate": 8.178990189040441e-05,
      "loss": 1.0827,
      "step": 1710
    },
    {
      "epoch": 0.12347451543431442,
      "grad_norm": 5.383884906768799,
      "learning_rate": 8.226848528356066e-05,
      "loss": 1.0641,
      "step": 1720
    },
    {
      "epoch": 0.12419239052404882,
      "grad_norm": 7.843161106109619,
      "learning_rate": 8.274706867671693e-05,
      "loss": 0.8659,
      "step": 1730
    },
    {
      "epoch": 0.1249102656137832,
      "grad_norm": 8.115278244018555,
      "learning_rate": 8.322565206987318e-05,
      "loss": 0.9888,
      "step": 1740
    },
    {
      "epoch": 0.12562814070351758,
      "grad_norm": 5.084176540374756,
      "learning_rate": 8.370423546302943e-05,
      "loss": 0.9691,
      "step": 1750
    },
    {
      "epoch": 0.12634601579325197,
      "grad_norm": 3.238266706466675,
      "learning_rate": 8.41828188561857e-05,
      "loss": 0.9925,
      "step": 1760
    },
    {
      "epoch": 0.12706389088298636,
      "grad_norm": 5.760537624359131,
      "learning_rate": 8.466140224934194e-05,
      "loss": 0.8247,
      "step": 1770
    },
    {
      "epoch": 0.12778176597272076,
      "grad_norm": 5.118504047393799,
      "learning_rate": 8.513998564249821e-05,
      "loss": 0.9862,
      "step": 1780
    },
    {
      "epoch": 0.12849964106245512,
      "grad_norm": 3.3820927143096924,
      "learning_rate": 8.561856903565447e-05,
      "loss": 0.8784,
      "step": 1790
    },
    {
      "epoch": 0.12921751615218952,
      "grad_norm": 15.020299911499023,
      "learning_rate": 8.609715242881073e-05,
      "loss": 0.99,
      "step": 1800
    },
    {
      "epoch": 0.1299353912419239,
      "grad_norm": 10.144827842712402,
      "learning_rate": 8.657573582196699e-05,
      "loss": 1.0072,
      "step": 1810
    },
    {
      "epoch": 0.1306532663316583,
      "grad_norm": 18.707775115966797,
      "learning_rate": 8.705431921512323e-05,
      "loss": 1.0446,
      "step": 1820
    },
    {
      "epoch": 0.13137114142139267,
      "grad_norm": 2.030277967453003,
      "learning_rate": 8.75329026082795e-05,
      "loss": 0.8966,
      "step": 1830
    },
    {
      "epoch": 0.13208901651112706,
      "grad_norm": 5.105950832366943,
      "learning_rate": 8.801148600143575e-05,
      "loss": 0.8731,
      "step": 1840
    },
    {
      "epoch": 0.13280689160086145,
      "grad_norm": 5.05544900894165,
      "learning_rate": 8.849006939459202e-05,
      "loss": 1.029,
      "step": 1850
    },
    {
      "epoch": 0.13352476669059585,
      "grad_norm": 6.325115203857422,
      "learning_rate": 8.896865278774827e-05,
      "loss": 0.8869,
      "step": 1860
    },
    {
      "epoch": 0.1342426417803302,
      "grad_norm": 7.615192890167236,
      "learning_rate": 8.944723618090453e-05,
      "loss": 0.998,
      "step": 1870
    },
    {
      "epoch": 0.1349605168700646,
      "grad_norm": 4.161067485809326,
      "learning_rate": 8.992581957406079e-05,
      "loss": 0.8176,
      "step": 1880
    },
    {
      "epoch": 0.135678391959799,
      "grad_norm": 10.043288230895996,
      "learning_rate": 9.040440296721703e-05,
      "loss": 0.9349,
      "step": 1890
    },
    {
      "epoch": 0.1363962670495334,
      "grad_norm": 11.2156343460083,
      "learning_rate": 9.08829863603733e-05,
      "loss": 0.7531,
      "step": 1900
    },
    {
      "epoch": 0.13711414213926776,
      "grad_norm": 5.6340227127075195,
      "learning_rate": 9.136156975352955e-05,
      "loss": 0.9626,
      "step": 1910
    },
    {
      "epoch": 0.13783201722900215,
      "grad_norm": 9.184479713439941,
      "learning_rate": 9.184015314668582e-05,
      "loss": 0.9832,
      "step": 1920
    },
    {
      "epoch": 0.13854989231873654,
      "grad_norm": 7.968409538269043,
      "learning_rate": 9.231873653984208e-05,
      "loss": 0.8454,
      "step": 1930
    },
    {
      "epoch": 0.13926776740847094,
      "grad_norm": 6.879753112792969,
      "learning_rate": 9.279731993299833e-05,
      "loss": 0.7955,
      "step": 1940
    },
    {
      "epoch": 0.1399856424982053,
      "grad_norm": 8.788783073425293,
      "learning_rate": 9.327590332615459e-05,
      "loss": 0.7551,
      "step": 1950
    },
    {
      "epoch": 0.1407035175879397,
      "grad_norm": 6.267629623413086,
      "learning_rate": 9.375448671931083e-05,
      "loss": 0.792,
      "step": 1960
    },
    {
      "epoch": 0.1414213926776741,
      "grad_norm": 6.02031946182251,
      "learning_rate": 9.42330701124671e-05,
      "loss": 0.7463,
      "step": 1970
    },
    {
      "epoch": 0.14213926776740848,
      "grad_norm": 13.969943046569824,
      "learning_rate": 9.471165350562335e-05,
      "loss": 1.2166,
      "step": 1980
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 9.194106101989746,
      "learning_rate": 9.519023689877962e-05,
      "loss": 0.9409,
      "step": 1990
    },
    {
      "epoch": 0.14357501794687724,
      "grad_norm": 14.815942764282227,
      "learning_rate": 9.566882029193588e-05,
      "loss": 1.0299,
      "step": 2000
    },
    {
      "epoch": 0.14429289303661164,
      "grad_norm": 5.014829635620117,
      "learning_rate": 9.614740368509214e-05,
      "loss": 0.8435,
      "step": 2010
    },
    {
      "epoch": 0.14501076812634603,
      "grad_norm": 12.062332153320312,
      "learning_rate": 9.662598707824839e-05,
      "loss": 0.7693,
      "step": 2020
    },
    {
      "epoch": 0.1457286432160804,
      "grad_norm": 5.35463809967041,
      "learning_rate": 9.710457047140465e-05,
      "loss": 0.8387,
      "step": 2030
    },
    {
      "epoch": 0.1464465183058148,
      "grad_norm": 2.4742238521575928,
      "learning_rate": 9.758315386456091e-05,
      "loss": 0.8814,
      "step": 2040
    },
    {
      "epoch": 0.14716439339554918,
      "grad_norm": 6.616336345672607,
      "learning_rate": 9.806173725771715e-05,
      "loss": 0.9781,
      "step": 2050
    },
    {
      "epoch": 0.14788226848528357,
      "grad_norm": 10.939091682434082,
      "learning_rate": 9.854032065087342e-05,
      "loss": 0.7903,
      "step": 2060
    },
    {
      "epoch": 0.14860014357501794,
      "grad_norm": 7.1362624168396,
      "learning_rate": 9.901890404402968e-05,
      "loss": 0.8672,
      "step": 2070
    },
    {
      "epoch": 0.14931801866475233,
      "grad_norm": 6.353552341461182,
      "learning_rate": 9.949748743718594e-05,
      "loss": 0.694,
      "step": 2080
    },
    {
      "epoch": 0.15003589375448673,
      "grad_norm": 9.171842575073242,
      "learning_rate": 9.99760708303422e-05,
      "loss": 0.9326,
      "step": 2090
    },
    {
      "epoch": 0.1507537688442211,
      "grad_norm": 3.3821277618408203,
      "learning_rate": 0.00010045465422349845,
      "loss": 0.9364,
      "step": 2100
    },
    {
      "epoch": 0.15147164393395549,
      "grad_norm": 5.643245697021484,
      "learning_rate": 0.0001009332376166547,
      "loss": 0.9211,
      "step": 2110
    },
    {
      "epoch": 0.15218951902368988,
      "grad_norm": 13.525012969970703,
      "learning_rate": 0.00010141182100981097,
      "loss": 0.9425,
      "step": 2120
    },
    {
      "epoch": 0.15290739411342427,
      "grad_norm": 6.765262603759766,
      "learning_rate": 0.00010189040440296722,
      "loss": 0.8873,
      "step": 2130
    },
    {
      "epoch": 0.15362526920315864,
      "grad_norm": 17.292112350463867,
      "learning_rate": 0.00010236898779612347,
      "loss": 0.7105,
      "step": 2140
    },
    {
      "epoch": 0.15434314429289303,
      "grad_norm": 9.959692001342773,
      "learning_rate": 0.00010284757118927974,
      "loss": 1.0505,
      "step": 2150
    },
    {
      "epoch": 0.15506101938262742,
      "grad_norm": 6.958469390869141,
      "learning_rate": 0.000103326154582436,
      "loss": 0.8959,
      "step": 2160
    },
    {
      "epoch": 0.15577889447236182,
      "grad_norm": 3.8736472129821777,
      "learning_rate": 0.00010380473797559225,
      "loss": 0.7775,
      "step": 2170
    },
    {
      "epoch": 0.15649676956209618,
      "grad_norm": 5.373655796051025,
      "learning_rate": 0.0001042833213687485,
      "loss": 0.7484,
      "step": 2180
    },
    {
      "epoch": 0.15721464465183058,
      "grad_norm": 7.68709659576416,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.9461,
      "step": 2190
    },
    {
      "epoch": 0.15793251974156497,
      "grad_norm": 6.573485851287842,
      "learning_rate": 0.00010524048815506103,
      "loss": 0.8246,
      "step": 2200
    },
    {
      "epoch": 0.15865039483129936,
      "grad_norm": 24.33491325378418,
      "learning_rate": 0.00010571907154821727,
      "loss": 0.6506,
      "step": 2210
    },
    {
      "epoch": 0.15936826992103373,
      "grad_norm": 6.94605016708374,
      "learning_rate": 0.00010619765494137354,
      "loss": 0.8385,
      "step": 2220
    },
    {
      "epoch": 0.16008614501076812,
      "grad_norm": 9.930744171142578,
      "learning_rate": 0.0001066762383345298,
      "loss": 0.8252,
      "step": 2230
    },
    {
      "epoch": 0.16080402010050251,
      "grad_norm": 7.9328155517578125,
      "learning_rate": 0.00010715482172768606,
      "loss": 0.5707,
      "step": 2240
    },
    {
      "epoch": 0.1615218951902369,
      "grad_norm": 6.82647705078125,
      "learning_rate": 0.0001076334051208423,
      "loss": 0.7431,
      "step": 2250
    },
    {
      "epoch": 0.16223977027997127,
      "grad_norm": 6.889920234680176,
      "learning_rate": 0.00010811198851399857,
      "loss": 0.9678,
      "step": 2260
    },
    {
      "epoch": 0.16295764536970567,
      "grad_norm": 11.620723724365234,
      "learning_rate": 0.00010859057190715483,
      "loss": 0.9276,
      "step": 2270
    },
    {
      "epoch": 0.16367552045944006,
      "grad_norm": 5.477067947387695,
      "learning_rate": 0.00010906915530031107,
      "loss": 0.8079,
      "step": 2280
    },
    {
      "epoch": 0.16439339554917445,
      "grad_norm": 6.905364513397217,
      "learning_rate": 0.00010954773869346736,
      "loss": 0.7724,
      "step": 2290
    },
    {
      "epoch": 0.16511127063890882,
      "grad_norm": 3.5112037658691406,
      "learning_rate": 0.0001100263220866236,
      "loss": 0.8345,
      "step": 2300
    },
    {
      "epoch": 0.1658291457286432,
      "grad_norm": 3.988696813583374,
      "learning_rate": 0.00011050490547977986,
      "loss": 0.4657,
      "step": 2310
    },
    {
      "epoch": 0.1665470208183776,
      "grad_norm": 5.467114448547363,
      "learning_rate": 0.00011098348887293613,
      "loss": 0.8694,
      "step": 2320
    },
    {
      "epoch": 0.167264895908112,
      "grad_norm": 7.147907733917236,
      "learning_rate": 0.00011146207226609237,
      "loss": 0.7485,
      "step": 2330
    },
    {
      "epoch": 0.16798277099784636,
      "grad_norm": NaN,
      "learning_rate": 0.00011194065565924863,
      "loss": 0.7853,
      "step": 2340
    },
    {
      "epoch": 0.16870064608758076,
      "grad_norm": 14.907742500305176,
      "learning_rate": 0.00011241923905240487,
      "loss": 0.7109,
      "step": 2350
    },
    {
      "epoch": 0.16941852117731515,
      "grad_norm": 3.100329875946045,
      "learning_rate": 0.00011289782244556116,
      "loss": 0.7739,
      "step": 2360
    },
    {
      "epoch": 0.17013639626704954,
      "grad_norm": 8.877232551574707,
      "learning_rate": 0.0001133764058387174,
      "loss": 0.7135,
      "step": 2370
    },
    {
      "epoch": 0.1708542713567839,
      "grad_norm": 4.095461368560791,
      "learning_rate": 0.00011385498923187366,
      "loss": 0.6622,
      "step": 2380
    },
    {
      "epoch": 0.1715721464465183,
      "grad_norm": 5.261692523956299,
      "learning_rate": 0.00011433357262502993,
      "loss": 0.7047,
      "step": 2390
    },
    {
      "epoch": 0.1722900215362527,
      "grad_norm": 5.572995662689209,
      "learning_rate": 0.00011481215601818617,
      "loss": 0.61,
      "step": 2400
    },
    {
      "epoch": 0.1730078966259871,
      "grad_norm": 11.10497760772705,
      "learning_rate": 0.00011529073941134243,
      "loss": 0.6879,
      "step": 2410
    },
    {
      "epoch": 0.17372577171572146,
      "grad_norm": 6.496926784515381,
      "learning_rate": 0.00011576932280449868,
      "loss": 0.7433,
      "step": 2420
    },
    {
      "epoch": 0.17444364680545585,
      "grad_norm": 5.608358383178711,
      "learning_rate": 0.00011624790619765496,
      "loss": 0.8487,
      "step": 2430
    },
    {
      "epoch": 0.17516152189519024,
      "grad_norm": 8.422104835510254,
      "learning_rate": 0.0001167264895908112,
      "loss": 0.6935,
      "step": 2440
    },
    {
      "epoch": 0.17587939698492464,
      "grad_norm": 2.0980517864227295,
      "learning_rate": 0.00011720507298396746,
      "loss": 0.6888,
      "step": 2450
    },
    {
      "epoch": 0.176597272074659,
      "grad_norm": 14.499276161193848,
      "learning_rate": 0.00011768365637712373,
      "loss": 0.6267,
      "step": 2460
    },
    {
      "epoch": 0.1773151471643934,
      "grad_norm": 8.760613441467285,
      "learning_rate": 0.00011816223977027998,
      "loss": 0.7062,
      "step": 2470
    },
    {
      "epoch": 0.1780330222541278,
      "grad_norm": 8.471842765808105,
      "learning_rate": 0.00011864082316343623,
      "loss": 0.6625,
      "step": 2480
    },
    {
      "epoch": 0.17875089734386218,
      "grad_norm": 10.356825828552246,
      "learning_rate": 0.00011911940655659248,
      "loss": 0.8655,
      "step": 2490
    },
    {
      "epoch": 0.17946877243359655,
      "grad_norm": 9.727413177490234,
      "learning_rate": 0.00011959798994974876,
      "loss": 0.7242,
      "step": 2500
    },
    {
      "epoch": 0.18018664752333094,
      "grad_norm": 11.176281929016113,
      "learning_rate": 0.000120076573342905,
      "loss": 0.5737,
      "step": 2510
    },
    {
      "epoch": 0.18090452261306533,
      "grad_norm": 7.602197170257568,
      "learning_rate": 0.00012055515673606126,
      "loss": 0.6335,
      "step": 2520
    },
    {
      "epoch": 0.18162239770279973,
      "grad_norm": 6.86177921295166,
      "learning_rate": 0.00012103374012921753,
      "loss": 0.8596,
      "step": 2530
    },
    {
      "epoch": 0.1823402727925341,
      "grad_norm": 5.123377799987793,
      "learning_rate": 0.00012151232352237378,
      "loss": 0.6754,
      "step": 2540
    },
    {
      "epoch": 0.18305814788226848,
      "grad_norm": 12.986041069030762,
      "learning_rate": 0.00012199090691553004,
      "loss": 0.7505,
      "step": 2550
    },
    {
      "epoch": 0.18377602297200288,
      "grad_norm": 11.9631986618042,
      "learning_rate": 0.00012246949030868628,
      "loss": 0.717,
      "step": 2560
    },
    {
      "epoch": 0.18449389806173727,
      "grad_norm": 4.9883599281311035,
      "learning_rate": 0.00012294807370184256,
      "loss": 0.6439,
      "step": 2570
    },
    {
      "epoch": 0.18521177315147164,
      "grad_norm": 9.047348976135254,
      "learning_rate": 0.00012342665709499882,
      "loss": 0.6022,
      "step": 2580
    },
    {
      "epoch": 0.18592964824120603,
      "grad_norm": 6.597160339355469,
      "learning_rate": 0.00012390524048815505,
      "loss": 0.6684,
      "step": 2590
    },
    {
      "epoch": 0.18664752333094042,
      "grad_norm": 8.051156997680664,
      "learning_rate": 0.00012438382388131134,
      "loss": 0.4393,
      "step": 2600
    },
    {
      "epoch": 0.1873653984206748,
      "grad_norm": 9.522806167602539,
      "learning_rate": 0.0001248624072744676,
      "loss": 0.6191,
      "step": 2610
    },
    {
      "epoch": 0.18808327351040918,
      "grad_norm": 4.117171287536621,
      "learning_rate": 0.00012534099066762382,
      "loss": 0.6132,
      "step": 2620
    },
    {
      "epoch": 0.18880114860014358,
      "grad_norm": 5.601903915405273,
      "learning_rate": 0.00012581957406078008,
      "loss": 0.6359,
      "step": 2630
    },
    {
      "epoch": 0.18951902368987797,
      "grad_norm": 7.992208003997803,
      "learning_rate": 0.00012629815745393637,
      "loss": 0.9735,
      "step": 2640
    },
    {
      "epoch": 0.19023689877961233,
      "grad_norm": 15.124784469604492,
      "learning_rate": 0.00012677674084709262,
      "loss": 0.735,
      "step": 2650
    },
    {
      "epoch": 0.19095477386934673,
      "grad_norm": 4.907828330993652,
      "learning_rate": 0.00012725532424024885,
      "loss": 0.7049,
      "step": 2660
    },
    {
      "epoch": 0.19167264895908112,
      "grad_norm": 9.75167179107666,
      "learning_rate": 0.00012773390763340514,
      "loss": 0.7065,
      "step": 2670
    },
    {
      "epoch": 0.19239052404881551,
      "grad_norm": 14.89174747467041,
      "learning_rate": 0.0001282124910265614,
      "loss": 0.7381,
      "step": 2680
    },
    {
      "epoch": 0.19310839913854988,
      "grad_norm": 9.204333305358887,
      "learning_rate": 0.00012869107441971763,
      "loss": 0.8075,
      "step": 2690
    },
    {
      "epoch": 0.19382627422828427,
      "grad_norm": 11.886959075927734,
      "learning_rate": 0.00012916965781287388,
      "loss": 0.6364,
      "step": 2700
    },
    {
      "epoch": 0.19454414931801867,
      "grad_norm": 9.528482437133789,
      "learning_rate": 0.00012964824120603017,
      "loss": 0.4559,
      "step": 2710
    },
    {
      "epoch": 0.19526202440775306,
      "grad_norm": 7.994287967681885,
      "learning_rate": 0.00013012682459918643,
      "loss": 0.6437,
      "step": 2720
    },
    {
      "epoch": 0.19597989949748743,
      "grad_norm": 10.91869068145752,
      "learning_rate": 0.00013060540799234266,
      "loss": 0.6766,
      "step": 2730
    },
    {
      "epoch": 0.19669777458722182,
      "grad_norm": 8.779091835021973,
      "learning_rate": 0.00013108399138549894,
      "loss": 0.5577,
      "step": 2740
    },
    {
      "epoch": 0.1974156496769562,
      "grad_norm": 7.145724296569824,
      "learning_rate": 0.0001315625747786552,
      "loss": 0.5535,
      "step": 2750
    },
    {
      "epoch": 0.1981335247666906,
      "grad_norm": 13.842679023742676,
      "learning_rate": 0.00013204115817181143,
      "loss": 0.7507,
      "step": 2760
    },
    {
      "epoch": 0.19885139985642497,
      "grad_norm": 5.403249740600586,
      "learning_rate": 0.00013251974156496769,
      "loss": 0.9176,
      "step": 2770
    },
    {
      "epoch": 0.19956927494615936,
      "grad_norm": 5.739748954772949,
      "learning_rate": 0.00013299832495812397,
      "loss": 0.4811,
      "step": 2780
    },
    {
      "epoch": 0.20028715003589376,
      "grad_norm": 5.3962626457214355,
      "learning_rate": 0.00013347690835128023,
      "loss": 0.7281,
      "step": 2790
    },
    {
      "epoch": 0.20100502512562815,
      "grad_norm": 9.795866966247559,
      "learning_rate": 0.00013395549174443646,
      "loss": 0.5901,
      "step": 2800
    },
    {
      "epoch": 0.20172290021536252,
      "grad_norm": 8.577753067016602,
      "learning_rate": 0.00013443407513759274,
      "loss": 0.7144,
      "step": 2810
    },
    {
      "epoch": 0.2024407753050969,
      "grad_norm": 10.673067092895508,
      "learning_rate": 0.000134912658530749,
      "loss": 0.8284,
      "step": 2820
    },
    {
      "epoch": 0.2031586503948313,
      "grad_norm": 6.139074802398682,
      "learning_rate": 0.00013539124192390523,
      "loss": 0.581,
      "step": 2830
    },
    {
      "epoch": 0.2038765254845657,
      "grad_norm": 6.311159610748291,
      "learning_rate": 0.0001358698253170615,
      "loss": 0.5736,
      "step": 2840
    },
    {
      "epoch": 0.20459440057430006,
      "grad_norm": 11.692688941955566,
      "learning_rate": 0.00013634840871021777,
      "loss": 0.6098,
      "step": 2850
    },
    {
      "epoch": 0.20531227566403445,
      "grad_norm": 5.0836181640625,
      "learning_rate": 0.00013682699210337403,
      "loss": 0.6399,
      "step": 2860
    },
    {
      "epoch": 0.20603015075376885,
      "grad_norm": 8.14512825012207,
      "learning_rate": 0.00013730557549653026,
      "loss": 0.707,
      "step": 2870
    },
    {
      "epoch": 0.20674802584350324,
      "grad_norm": 13.538914680480957,
      "learning_rate": 0.00013778415888968654,
      "loss": 0.5331,
      "step": 2880
    },
    {
      "epoch": 0.2074659009332376,
      "grad_norm": 18.874252319335938,
      "learning_rate": 0.0001382627422828428,
      "loss": 0.8376,
      "step": 2890
    },
    {
      "epoch": 0.208183776022972,
      "grad_norm": 13.792805671691895,
      "learning_rate": 0.00013874132567599903,
      "loss": 0.8831,
      "step": 2900
    },
    {
      "epoch": 0.2089016511127064,
      "grad_norm": 25.923690795898438,
      "learning_rate": 0.0001392199090691553,
      "loss": 0.8851,
      "step": 2910
    },
    {
      "epoch": 0.2096195262024408,
      "grad_norm": 11.843143463134766,
      "learning_rate": 0.00013969849246231157,
      "loss": 0.7247,
      "step": 2920
    },
    {
      "epoch": 0.21033740129217515,
      "grad_norm": 9.90977668762207,
      "learning_rate": 0.00014017707585546783,
      "loss": 0.7203,
      "step": 2930
    },
    {
      "epoch": 0.21105527638190955,
      "grad_norm": 9.637714385986328,
      "learning_rate": 0.00014065565924862406,
      "loss": 0.6989,
      "step": 2940
    },
    {
      "epoch": 0.21177315147164394,
      "grad_norm": 5.723646640777588,
      "learning_rate": 0.00014113424264178035,
      "loss": 0.7532,
      "step": 2950
    },
    {
      "epoch": 0.21249102656137833,
      "grad_norm": 17.064239501953125,
      "learning_rate": 0.0001416128260349366,
      "loss": 0.5989,
      "step": 2960
    },
    {
      "epoch": 0.2132089016511127,
      "grad_norm": 9.054686546325684,
      "learning_rate": 0.00014209140942809283,
      "loss": 0.5036,
      "step": 2970
    },
    {
      "epoch": 0.2139267767408471,
      "grad_norm": 14.086661338806152,
      "learning_rate": 0.0001425699928212491,
      "loss": 0.6187,
      "step": 2980
    },
    {
      "epoch": 0.21464465183058148,
      "grad_norm": 12.19704532623291,
      "learning_rate": 0.00014304857621440538,
      "loss": 0.7291,
      "step": 2990
    },
    {
      "epoch": 0.21536252692031588,
      "grad_norm": 22.42854881286621,
      "learning_rate": 0.00014352715960756163,
      "loss": 0.8748,
      "step": 3000
    },
    {
      "epoch": 0.21608040201005024,
      "grad_norm": 9.714693069458008,
      "learning_rate": 0.00014400574300071786,
      "loss": 0.5655,
      "step": 3010
    },
    {
      "epoch": 0.21679827709978464,
      "grad_norm": 14.807022094726562,
      "learning_rate": 0.00014448432639387415,
      "loss": 0.8005,
      "step": 3020
    },
    {
      "epoch": 0.21751615218951903,
      "grad_norm": 18.7128849029541,
      "learning_rate": 0.0001449629097870304,
      "loss": 0.673,
      "step": 3030
    },
    {
      "epoch": 0.21823402727925342,
      "grad_norm": 9.080941200256348,
      "learning_rate": 0.00014544149318018666,
      "loss": 0.5411,
      "step": 3040
    },
    {
      "epoch": 0.2189519023689878,
      "grad_norm": 10.273338317871094,
      "learning_rate": 0.0001459200765733429,
      "loss": 0.5779,
      "step": 3050
    },
    {
      "epoch": 0.21966977745872218,
      "grad_norm": 4.234987735748291,
      "learning_rate": 0.00014639865996649918,
      "loss": 0.563,
      "step": 3060
    },
    {
      "epoch": 0.22038765254845658,
      "grad_norm": 9.487408638000488,
      "learning_rate": 0.00014687724335965544,
      "loss": 0.7331,
      "step": 3070
    },
    {
      "epoch": 0.22110552763819097,
      "grad_norm": 26.212820053100586,
      "learning_rate": 0.00014735582675281167,
      "loss": 0.7097,
      "step": 3080
    },
    {
      "epoch": 0.22182340272792533,
      "grad_norm": 9.132706642150879,
      "learning_rate": 0.00014783441014596795,
      "loss": 0.5833,
      "step": 3090
    },
    {
      "epoch": 0.22254127781765973,
      "grad_norm": 1.9778395891189575,
      "learning_rate": 0.0001483129935391242,
      "loss": 0.7559,
      "step": 3100
    },
    {
      "epoch": 0.22325915290739412,
      "grad_norm": 9.06186294555664,
      "learning_rate": 0.00014879157693228047,
      "loss": 0.6076,
      "step": 3110
    },
    {
      "epoch": 0.22397702799712849,
      "grad_norm": 16.21387481689453,
      "learning_rate": 0.00014927016032543672,
      "loss": 0.7252,
      "step": 3120
    },
    {
      "epoch": 0.22469490308686288,
      "grad_norm": 6.9850873947143555,
      "learning_rate": 0.00014974874371859298,
      "loss": 0.4331,
      "step": 3130
    },
    {
      "epoch": 0.22541277817659727,
      "grad_norm": 3.291016101837158,
      "learning_rate": 0.00015022732711174924,
      "loss": 0.564,
      "step": 3140
    },
    {
      "epoch": 0.22613065326633167,
      "grad_norm": 4.393535614013672,
      "learning_rate": 0.00015070591050490547,
      "loss": 0.4705,
      "step": 3150
    },
    {
      "epoch": 0.22684852835606603,
      "grad_norm": 14.522414207458496,
      "learning_rate": 0.00015118449389806175,
      "loss": 0.5698,
      "step": 3160
    },
    {
      "epoch": 0.22756640344580042,
      "grad_norm": 9.782432556152344,
      "learning_rate": 0.000151663077291218,
      "loss": 0.7435,
      "step": 3170
    },
    {
      "epoch": 0.22828427853553482,
      "grad_norm": 7.542337894439697,
      "learning_rate": 0.00015214166068437427,
      "loss": 0.4843,
      "step": 3180
    },
    {
      "epoch": 0.2290021536252692,
      "grad_norm": 6.674903392791748,
      "learning_rate": 0.00015262024407753052,
      "loss": 0.5537,
      "step": 3190
    },
    {
      "epoch": 0.22972002871500358,
      "grad_norm": 4.597530364990234,
      "learning_rate": 0.00015309882747068678,
      "loss": 0.5325,
      "step": 3200
    },
    {
      "epoch": 0.23043790380473797,
      "grad_norm": 6.6177191734313965,
      "learning_rate": 0.00015357741086384304,
      "loss": 0.5083,
      "step": 3210
    },
    {
      "epoch": 0.23115577889447236,
      "grad_norm": 8.7581148147583,
      "learning_rate": 0.00015405599425699927,
      "loss": 0.6325,
      "step": 3220
    },
    {
      "epoch": 0.23187365398420676,
      "grad_norm": 2.469641923904419,
      "learning_rate": 0.00015453457765015555,
      "loss": 0.4229,
      "step": 3230
    },
    {
      "epoch": 0.23259152907394112,
      "grad_norm": 11.01044750213623,
      "learning_rate": 0.0001550131610433118,
      "loss": 0.539,
      "step": 3240
    },
    {
      "epoch": 0.23330940416367552,
      "grad_norm": 2.4956064224243164,
      "learning_rate": 0.00015549174443646807,
      "loss": 0.3817,
      "step": 3250
    },
    {
      "epoch": 0.2340272792534099,
      "grad_norm": 11.202369689941406,
      "learning_rate": 0.00015597032782962433,
      "loss": 0.5843,
      "step": 3260
    },
    {
      "epoch": 0.2347451543431443,
      "grad_norm": 7.298858165740967,
      "learning_rate": 0.00015644891122278058,
      "loss": 0.5296,
      "step": 3270
    },
    {
      "epoch": 0.23546302943287867,
      "grad_norm": 10.163223266601562,
      "learning_rate": 0.00015692749461593684,
      "loss": 0.6379,
      "step": 3280
    },
    {
      "epoch": 0.23618090452261306,
      "grad_norm": 9.196334838867188,
      "learning_rate": 0.00015740607800909307,
      "loss": 0.6336,
      "step": 3290
    },
    {
      "epoch": 0.23689877961234745,
      "grad_norm": 15.86329174041748,
      "learning_rate": 0.00015788466140224936,
      "loss": 0.6961,
      "step": 3300
    },
    {
      "epoch": 0.23761665470208185,
      "grad_norm": 9.804224967956543,
      "learning_rate": 0.00015836324479540561,
      "loss": 0.5858,
      "step": 3310
    },
    {
      "epoch": 0.2383345297918162,
      "grad_norm": 4.930755138397217,
      "learning_rate": 0.00015884182818856187,
      "loss": 0.5559,
      "step": 3320
    },
    {
      "epoch": 0.2390524048815506,
      "grad_norm": 2.841435670852661,
      "learning_rate": 0.00015932041158171813,
      "loss": 0.53,
      "step": 3330
    },
    {
      "epoch": 0.239770279971285,
      "grad_norm": 6.809842109680176,
      "learning_rate": 0.00015979899497487439,
      "loss": 0.5969,
      "step": 3340
    },
    {
      "epoch": 0.2404881550610194,
      "grad_norm": 7.742214202880859,
      "learning_rate": 0.00016027757836803064,
      "loss": 0.5026,
      "step": 3350
    },
    {
      "epoch": 0.24120603015075376,
      "grad_norm": 5.924073696136475,
      "learning_rate": 0.00016075616176118687,
      "loss": 0.5304,
      "step": 3360
    },
    {
      "epoch": 0.24192390524048815,
      "grad_norm": 9.11752700805664,
      "learning_rate": 0.00016123474515434316,
      "loss": 0.585,
      "step": 3370
    },
    {
      "epoch": 0.24264178033022255,
      "grad_norm": 1.3658801317214966,
      "learning_rate": 0.00016171332854749942,
      "loss": 0.6718,
      "step": 3380
    },
    {
      "epoch": 0.24335965541995694,
      "grad_norm": 7.507955551147461,
      "learning_rate": 0.00016219191194065567,
      "loss": 0.5923,
      "step": 3390
    },
    {
      "epoch": 0.2440775305096913,
      "grad_norm": 7.722149848937988,
      "learning_rate": 0.00016267049533381193,
      "loss": 0.4277,
      "step": 3400
    },
    {
      "epoch": 0.2447954055994257,
      "grad_norm": 13.858388900756836,
      "learning_rate": 0.0001631490787269682,
      "loss": 0.5101,
      "step": 3410
    },
    {
      "epoch": 0.2455132806891601,
      "grad_norm": 11.119125366210938,
      "learning_rate": 0.00016362766212012445,
      "loss": 0.5082,
      "step": 3420
    },
    {
      "epoch": 0.24623115577889448,
      "grad_norm": 6.957077503204346,
      "learning_rate": 0.00016410624551328068,
      "loss": 0.5271,
      "step": 3430
    },
    {
      "epoch": 0.24694903086862885,
      "grad_norm": 8.251750946044922,
      "learning_rate": 0.00016458482890643696,
      "loss": 0.3949,
      "step": 3440
    },
    {
      "epoch": 0.24766690595836324,
      "grad_norm": 11.67891788482666,
      "learning_rate": 0.00016506341229959322,
      "loss": 0.7389,
      "step": 3450
    },
    {
      "epoch": 0.24838478104809764,
      "grad_norm": 9.634100914001465,
      "learning_rate": 0.00016554199569274948,
      "loss": 0.6902,
      "step": 3460
    },
    {
      "epoch": 0.24910265613783203,
      "grad_norm": 5.616624355316162,
      "learning_rate": 0.00016602057908590573,
      "loss": 0.54,
      "step": 3470
    },
    {
      "epoch": 0.2498205312275664,
      "grad_norm": 6.906912326812744,
      "learning_rate": 0.000166499162479062,
      "loss": 0.5188,
      "step": 3480
    },
    {
      "epoch": 0.2505384063173008,
      "grad_norm": 19.347566604614258,
      "learning_rate": 0.00016697774587221825,
      "loss": 0.5423,
      "step": 3490
    },
    {
      "epoch": 0.25125628140703515,
      "grad_norm": 29.376249313354492,
      "learning_rate": 0.00016745632926537448,
      "loss": 0.5733,
      "step": 3500
    },
    {
      "epoch": 0.2519741564967696,
      "grad_norm": 19.529911041259766,
      "learning_rate": 0.00016793491265853076,
      "loss": 0.8297,
      "step": 3510
    },
    {
      "epoch": 0.25269203158650394,
      "grad_norm": 10.090792655944824,
      "learning_rate": 0.00016841349605168702,
      "loss": 0.6397,
      "step": 3520
    },
    {
      "epoch": 0.25340990667623836,
      "grad_norm": 5.996514797210693,
      "learning_rate": 0.00016889207944484328,
      "loss": 0.5551,
      "step": 3530
    },
    {
      "epoch": 0.2541277817659727,
      "grad_norm": 14.355762481689453,
      "learning_rate": 0.00016937066283799953,
      "loss": 0.4565,
      "step": 3540
    },
    {
      "epoch": 0.2548456568557071,
      "grad_norm": 5.475924968719482,
      "learning_rate": 0.0001698492462311558,
      "loss": 0.5119,
      "step": 3550
    },
    {
      "epoch": 0.2555635319454415,
      "grad_norm": 8.232370376586914,
      "learning_rate": 0.00017032782962431205,
      "loss": 0.5185,
      "step": 3560
    },
    {
      "epoch": 0.2562814070351759,
      "grad_norm": 22.18854331970215,
      "learning_rate": 0.00017080641301746828,
      "loss": 0.5858,
      "step": 3570
    },
    {
      "epoch": 0.25699928212491024,
      "grad_norm": 10.752120971679688,
      "learning_rate": 0.00017128499641062456,
      "loss": 0.5115,
      "step": 3580
    },
    {
      "epoch": 0.25771715721464467,
      "grad_norm": 2.6648061275482178,
      "learning_rate": 0.00017176357980378082,
      "loss": 0.55,
      "step": 3590
    },
    {
      "epoch": 0.25843503230437903,
      "grad_norm": 1.0402302742004395,
      "learning_rate": 0.00017224216319693708,
      "loss": 0.4172,
      "step": 3600
    },
    {
      "epoch": 0.25915290739411345,
      "grad_norm": 7.9062628746032715,
      "learning_rate": 0.00017272074659009334,
      "loss": 0.461,
      "step": 3610
    },
    {
      "epoch": 0.2598707824838478,
      "grad_norm": 2.090742826461792,
      "learning_rate": 0.0001731993299832496,
      "loss": 0.2986,
      "step": 3620
    },
    {
      "epoch": 0.2605886575735822,
      "grad_norm": 4.3103437423706055,
      "learning_rate": 0.00017367791337640585,
      "loss": 0.4783,
      "step": 3630
    },
    {
      "epoch": 0.2613065326633166,
      "grad_norm": 18.174556732177734,
      "learning_rate": 0.00017415649676956208,
      "loss": 0.4286,
      "step": 3640
    },
    {
      "epoch": 0.26202440775305097,
      "grad_norm": 7.257902145385742,
      "learning_rate": 0.00017463508016271837,
      "loss": 0.4998,
      "step": 3650
    },
    {
      "epoch": 0.26274228284278534,
      "grad_norm": 7.776939868927002,
      "learning_rate": 0.00017511366355587462,
      "loss": 0.7306,
      "step": 3660
    },
    {
      "epoch": 0.26346015793251976,
      "grad_norm": 12.777061462402344,
      "learning_rate": 0.00017559224694903088,
      "loss": 0.4998,
      "step": 3670
    },
    {
      "epoch": 0.2641780330222541,
      "grad_norm": 11.837723731994629,
      "learning_rate": 0.00017607083034218714,
      "loss": 0.5112,
      "step": 3680
    },
    {
      "epoch": 0.2648959081119885,
      "grad_norm": 11.965012550354004,
      "learning_rate": 0.0001765494137353434,
      "loss": 0.592,
      "step": 3690
    },
    {
      "epoch": 0.2656137832017229,
      "grad_norm": 6.230838775634766,
      "learning_rate": 0.00017702799712849965,
      "loss": 0.6484,
      "step": 3700
    },
    {
      "epoch": 0.2663316582914573,
      "grad_norm": 10.860459327697754,
      "learning_rate": 0.00017750658052165588,
      "loss": 0.4413,
      "step": 3710
    },
    {
      "epoch": 0.2670495333811917,
      "grad_norm": 16.325971603393555,
      "learning_rate": 0.00017798516391481217,
      "loss": 0.7094,
      "step": 3720
    },
    {
      "epoch": 0.26776740847092606,
      "grad_norm": 15.324586868286133,
      "learning_rate": 0.00017846374730796843,
      "loss": 0.5569,
      "step": 3730
    },
    {
      "epoch": 0.2684852835606604,
      "grad_norm": 13.545082092285156,
      "learning_rate": 0.00017894233070112468,
      "loss": 0.4676,
      "step": 3740
    },
    {
      "epoch": 0.26920315865039485,
      "grad_norm": 6.281307220458984,
      "learning_rate": 0.00017942091409428094,
      "loss": 0.3922,
      "step": 3750
    },
    {
      "epoch": 0.2699210337401292,
      "grad_norm": 6.586626052856445,
      "learning_rate": 0.0001798994974874372,
      "loss": 0.693,
      "step": 3760
    },
    {
      "epoch": 0.2706389088298636,
      "grad_norm": 11.07156753540039,
      "learning_rate": 0.00018037808088059346,
      "loss": 0.4992,
      "step": 3770
    },
    {
      "epoch": 0.271356783919598,
      "grad_norm": 9.745314598083496,
      "learning_rate": 0.00018085666427374969,
      "loss": 0.6839,
      "step": 3780
    },
    {
      "epoch": 0.27207465900933236,
      "grad_norm": 4.076261520385742,
      "learning_rate": 0.00018133524766690597,
      "loss": 0.3949,
      "step": 3790
    },
    {
      "epoch": 0.2727925340990668,
      "grad_norm": 9.87498950958252,
      "learning_rate": 0.00018181383106006223,
      "loss": 0.5057,
      "step": 3800
    },
    {
      "epoch": 0.27351040918880115,
      "grad_norm": 10.1710844039917,
      "learning_rate": 0.00018229241445321848,
      "loss": 0.384,
      "step": 3810
    },
    {
      "epoch": 0.2742282842785355,
      "grad_norm": 11.352396011352539,
      "learning_rate": 0.00018277099784637474,
      "loss": 0.6622,
      "step": 3820
    },
    {
      "epoch": 0.27494615936826994,
      "grad_norm": 1.4675006866455078,
      "learning_rate": 0.000183249581239531,
      "loss": 0.545,
      "step": 3830
    },
    {
      "epoch": 0.2756640344580043,
      "grad_norm": 12.436784744262695,
      "learning_rate": 0.00018372816463268726,
      "loss": 0.4749,
      "step": 3840
    },
    {
      "epoch": 0.27638190954773867,
      "grad_norm": 5.627370834350586,
      "learning_rate": 0.00018420674802584351,
      "loss": 0.4601,
      "step": 3850
    },
    {
      "epoch": 0.2770997846374731,
      "grad_norm": 18.027587890625,
      "learning_rate": 0.00018468533141899977,
      "loss": 0.6146,
      "step": 3860
    },
    {
      "epoch": 0.27781765972720746,
      "grad_norm": 6.252625942230225,
      "learning_rate": 0.00018516391481215603,
      "loss": 0.6059,
      "step": 3870
    },
    {
      "epoch": 0.2785355348169419,
      "grad_norm": 2.083648443222046,
      "learning_rate": 0.0001856424982053123,
      "loss": 0.5965,
      "step": 3880
    },
    {
      "epoch": 0.27925340990667624,
      "grad_norm": 7.173253059387207,
      "learning_rate": 0.00018612108159846854,
      "loss": 0.7397,
      "step": 3890
    },
    {
      "epoch": 0.2799712849964106,
      "grad_norm": 1.056167721748352,
      "learning_rate": 0.0001865996649916248,
      "loss": 0.4122,
      "step": 3900
    },
    {
      "epoch": 0.28068916008614503,
      "grad_norm": 12.015295028686523,
      "learning_rate": 0.00018707824838478106,
      "loss": 0.394,
      "step": 3910
    },
    {
      "epoch": 0.2814070351758794,
      "grad_norm": 4.733788967132568,
      "learning_rate": 0.00018755683177793732,
      "loss": 0.6311,
      "step": 3920
    },
    {
      "epoch": 0.28212491026561376,
      "grad_norm": 3.69978666305542,
      "learning_rate": 0.00018803541517109357,
      "loss": 0.4062,
      "step": 3930
    },
    {
      "epoch": 0.2828427853553482,
      "grad_norm": 8.89496898651123,
      "learning_rate": 0.00018851399856424983,
      "loss": 0.4593,
      "step": 3940
    },
    {
      "epoch": 0.28356066044508255,
      "grad_norm": 3.671772003173828,
      "learning_rate": 0.0001889925819574061,
      "loss": 0.5274,
      "step": 3950
    },
    {
      "epoch": 0.28427853553481697,
      "grad_norm": 8.96165657043457,
      "learning_rate": 0.00018947116535056235,
      "loss": 0.7185,
      "step": 3960
    },
    {
      "epoch": 0.28499641062455133,
      "grad_norm": 20.563365936279297,
      "learning_rate": 0.0001899497487437186,
      "loss": 0.6894,
      "step": 3970
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 18.392757415771484,
      "learning_rate": 0.00019042833213687486,
      "loss": 0.6563,
      "step": 3980
    },
    {
      "epoch": 0.2864321608040201,
      "grad_norm": 25.522319793701172,
      "learning_rate": 0.00019090691553003112,
      "loss": 0.5701,
      "step": 3990
    },
    {
      "epoch": 0.2871500358937545,
      "grad_norm": 3.513577699661255,
      "learning_rate": 0.00019138549892318738,
      "loss": 0.3793,
      "step": 4000
    },
    {
      "epoch": 0.28786791098348885,
      "grad_norm": 10.548158645629883,
      "learning_rate": 0.00019186408231634363,
      "loss": 0.4542,
      "step": 4010
    },
    {
      "epoch": 0.28858578607322327,
      "grad_norm": 16.99068260192871,
      "learning_rate": 0.0001923426657094999,
      "loss": 0.5015,
      "step": 4020
    },
    {
      "epoch": 0.28930366116295764,
      "grad_norm": 19.260284423828125,
      "learning_rate": 0.00019282124910265615,
      "loss": 0.4511,
      "step": 4030
    },
    {
      "epoch": 0.29002153625269206,
      "grad_norm": 12.762491226196289,
      "learning_rate": 0.0001932998324958124,
      "loss": 0.8726,
      "step": 4040
    },
    {
      "epoch": 0.2907394113424264,
      "grad_norm": 7.884005069732666,
      "learning_rate": 0.00019377841588896866,
      "loss": 0.4742,
      "step": 4050
    },
    {
      "epoch": 0.2914572864321608,
      "grad_norm": 6.882601737976074,
      "learning_rate": 0.00019425699928212492,
      "loss": 0.5,
      "step": 4060
    },
    {
      "epoch": 0.2921751615218952,
      "grad_norm": 8.216170310974121,
      "learning_rate": 0.00019473558267528118,
      "loss": 0.3462,
      "step": 4070
    },
    {
      "epoch": 0.2928930366116296,
      "grad_norm": 3.567963123321533,
      "learning_rate": 0.00019521416606843744,
      "loss": 0.5323,
      "step": 4080
    },
    {
      "epoch": 0.29361091170136394,
      "grad_norm": 11.320813179016113,
      "learning_rate": 0.0001956927494615937,
      "loss": 0.6787,
      "step": 4090
    },
    {
      "epoch": 0.29432878679109836,
      "grad_norm": 9.663959503173828,
      "learning_rate": 0.00019617133285474995,
      "loss": 0.3647,
      "step": 4100
    },
    {
      "epoch": 0.29504666188083273,
      "grad_norm": 12.001446723937988,
      "learning_rate": 0.0001966499162479062,
      "loss": 0.5981,
      "step": 4110
    },
    {
      "epoch": 0.29576453697056715,
      "grad_norm": 15.260436058044434,
      "learning_rate": 0.00019712849964106247,
      "loss": 0.4666,
      "step": 4120
    },
    {
      "epoch": 0.2964824120603015,
      "grad_norm": 4.417855262756348,
      "learning_rate": 0.00019760708303421872,
      "loss": 0.7709,
      "step": 4130
    },
    {
      "epoch": 0.2972002871500359,
      "grad_norm": 8.791413307189941,
      "learning_rate": 0.00019808566642737498,
      "loss": 0.6106,
      "step": 4140
    },
    {
      "epoch": 0.2979181622397703,
      "grad_norm": 19.40513038635254,
      "learning_rate": 0.00019856424982053124,
      "loss": 0.6537,
      "step": 4150
    },
    {
      "epoch": 0.29863603732950467,
      "grad_norm": 11.812305450439453,
      "learning_rate": 0.0001990428332136875,
      "loss": 0.4014,
      "step": 4160
    },
    {
      "epoch": 0.29935391241923903,
      "grad_norm": 8.936590194702148,
      "learning_rate": 0.00019952141660684375,
      "loss": 0.5279,
      "step": 4170
    },
    {
      "epoch": 0.30007178750897345,
      "grad_norm": 10.349360466003418,
      "learning_rate": 0.0002,
      "loss": 0.491,
      "step": 4180
    },
    {
      "epoch": 0.3007896625987078,
      "grad_norm": 15.231633186340332,
      "learning_rate": 0.0001999468240674271,
      "loss": 0.433,
      "step": 4190
    },
    {
      "epoch": 0.3015075376884422,
      "grad_norm": 8.817174911499023,
      "learning_rate": 0.0001998936481348542,
      "loss": 0.526,
      "step": 4200
    },
    {
      "epoch": 0.3022254127781766,
      "grad_norm": 3.7432806491851807,
      "learning_rate": 0.00019984047220228125,
      "loss": 0.4834,
      "step": 4210
    },
    {
      "epoch": 0.30294328786791097,
      "grad_norm": 11.71324348449707,
      "learning_rate": 0.00019978729626970833,
      "loss": 0.3154,
      "step": 4220
    },
    {
      "epoch": 0.3036611629576454,
      "grad_norm": 1.6932083368301392,
      "learning_rate": 0.00019973412033713543,
      "loss": 0.3894,
      "step": 4230
    },
    {
      "epoch": 0.30437903804737976,
      "grad_norm": 8.231146812438965,
      "learning_rate": 0.00019968094440456251,
      "loss": 0.3465,
      "step": 4240
    },
    {
      "epoch": 0.3050969131371141,
      "grad_norm": 13.177322387695312,
      "learning_rate": 0.0001996277684719896,
      "loss": 0.4788,
      "step": 4250
    },
    {
      "epoch": 0.30581478822684854,
      "grad_norm": 2.885084867477417,
      "learning_rate": 0.00019957459253941667,
      "loss": 0.4858,
      "step": 4260
    },
    {
      "epoch": 0.3065326633165829,
      "grad_norm": 8.836213111877441,
      "learning_rate": 0.00019952141660684375,
      "loss": 0.6456,
      "step": 4270
    },
    {
      "epoch": 0.3072505384063173,
      "grad_norm": 11.01154899597168,
      "learning_rate": 0.00019946824067427083,
      "loss": 0.2946,
      "step": 4280
    },
    {
      "epoch": 0.3079684134960517,
      "grad_norm": 9.641129493713379,
      "learning_rate": 0.0001994150647416979,
      "loss": 0.6268,
      "step": 4290
    },
    {
      "epoch": 0.30868628858578606,
      "grad_norm": 8.154520988464355,
      "learning_rate": 0.00019936188880912502,
      "loss": 0.5708,
      "step": 4300
    },
    {
      "epoch": 0.3094041636755205,
      "grad_norm": 25.057580947875977,
      "learning_rate": 0.0001993087128765521,
      "loss": 0.3294,
      "step": 4310
    },
    {
      "epoch": 0.31012203876525485,
      "grad_norm": 15.723724365234375,
      "learning_rate": 0.00019925553694397915,
      "loss": 0.4448,
      "step": 4320
    },
    {
      "epoch": 0.3108399138549892,
      "grad_norm": 11.27841567993164,
      "learning_rate": 0.00019920236101140626,
      "loss": 0.5049,
      "step": 4330
    },
    {
      "epoch": 0.31155778894472363,
      "grad_norm": 9.43625259399414,
      "learning_rate": 0.00019914918507883334,
      "loss": 0.444,
      "step": 4340
    },
    {
      "epoch": 0.312275664034458,
      "grad_norm": 1.8812917470932007,
      "learning_rate": 0.00019909600914626042,
      "loss": 0.2587,
      "step": 4350
    },
    {
      "epoch": 0.31299353912419237,
      "grad_norm": 9.11155891418457,
      "learning_rate": 0.0001990428332136875,
      "loss": 0.7205,
      "step": 4360
    },
    {
      "epoch": 0.3137114142139268,
      "grad_norm": 6.588705539703369,
      "learning_rate": 0.00019898965728111457,
      "loss": 0.5135,
      "step": 4370
    },
    {
      "epoch": 0.31442928930366115,
      "grad_norm": 9.708564758300781,
      "learning_rate": 0.00019893648134854165,
      "loss": 0.6049,
      "step": 4380
    },
    {
      "epoch": 0.3151471643933956,
      "grad_norm": 13.31852912902832,
      "learning_rate": 0.00019888330541596873,
      "loss": 0.5223,
      "step": 4390
    },
    {
      "epoch": 0.31586503948312994,
      "grad_norm": 5.972749710083008,
      "learning_rate": 0.00019883012948339584,
      "loss": 0.517,
      "step": 4400
    },
    {
      "epoch": 0.3165829145728643,
      "grad_norm": 9.92796802520752,
      "learning_rate": 0.00019877695355082292,
      "loss": 0.4914,
      "step": 4410
    },
    {
      "epoch": 0.3173007896625987,
      "grad_norm": 15.248334884643555,
      "learning_rate": 0.00019872377761824997,
      "loss": 0.3907,
      "step": 4420
    },
    {
      "epoch": 0.3180186647523331,
      "grad_norm": 13.498915672302246,
      "learning_rate": 0.00019867060168567708,
      "loss": 0.3457,
      "step": 4430
    },
    {
      "epoch": 0.31873653984206746,
      "grad_norm": 2.3066651821136475,
      "learning_rate": 0.00019861742575310416,
      "loss": 0.3718,
      "step": 4440
    },
    {
      "epoch": 0.3194544149318019,
      "grad_norm": 5.880839824676514,
      "learning_rate": 0.00019856424982053124,
      "loss": 0.4727,
      "step": 4450
    },
    {
      "epoch": 0.32017229002153624,
      "grad_norm": 9.876163482666016,
      "learning_rate": 0.00019851107388795832,
      "loss": 0.5365,
      "step": 4460
    },
    {
      "epoch": 0.32089016511127066,
      "grad_norm": 15.622401237487793,
      "learning_rate": 0.0001984578979553854,
      "loss": 0.2588,
      "step": 4470
    },
    {
      "epoch": 0.32160804020100503,
      "grad_norm": 13.399739265441895,
      "learning_rate": 0.00019840472202281248,
      "loss": 0.618,
      "step": 4480
    },
    {
      "epoch": 0.3223259152907394,
      "grad_norm": 7.869649410247803,
      "learning_rate": 0.00019835154609023956,
      "loss": 0.3703,
      "step": 4490
    },
    {
      "epoch": 0.3230437903804738,
      "grad_norm": 6.75095796585083,
      "learning_rate": 0.00019829837015766666,
      "loss": 0.6275,
      "step": 4500
    },
    {
      "epoch": 0.3237616654702082,
      "grad_norm": 8.558040618896484,
      "learning_rate": 0.00019824519422509374,
      "loss": 0.4831,
      "step": 4510
    },
    {
      "epoch": 0.32447954055994255,
      "grad_norm": 4.255273342132568,
      "learning_rate": 0.00019819201829252082,
      "loss": 0.6005,
      "step": 4520
    },
    {
      "epoch": 0.32519741564967697,
      "grad_norm": 6.950681686401367,
      "learning_rate": 0.0001981388423599479,
      "loss": 0.3219,
      "step": 4530
    },
    {
      "epoch": 0.32591529073941133,
      "grad_norm": 23.886686325073242,
      "learning_rate": 0.00019808566642737498,
      "loss": 0.4578,
      "step": 4540
    },
    {
      "epoch": 0.32663316582914576,
      "grad_norm": 11.119402885437012,
      "learning_rate": 0.00019803249049480206,
      "loss": 0.6407,
      "step": 4550
    },
    {
      "epoch": 0.3273510409188801,
      "grad_norm": 9.753374099731445,
      "learning_rate": 0.00019797931456222914,
      "loss": 0.3479,
      "step": 4560
    },
    {
      "epoch": 0.3280689160086145,
      "grad_norm": 12.628405570983887,
      "learning_rate": 0.00019792613862965625,
      "loss": 0.5699,
      "step": 4570
    },
    {
      "epoch": 0.3287867910983489,
      "grad_norm": 5.51292610168457,
      "learning_rate": 0.0001978729626970833,
      "loss": 0.5802,
      "step": 4580
    },
    {
      "epoch": 0.3295046661880833,
      "grad_norm": 6.5605010986328125,
      "learning_rate": 0.00019781978676451038,
      "loss": 0.5288,
      "step": 4590
    },
    {
      "epoch": 0.33022254127781764,
      "grad_norm": 0.93780916929245,
      "learning_rate": 0.00019776661083193748,
      "loss": 0.3001,
      "step": 4600
    },
    {
      "epoch": 0.33094041636755206,
      "grad_norm": 7.287946701049805,
      "learning_rate": 0.00019771343489936456,
      "loss": 0.645,
      "step": 4610
    },
    {
      "epoch": 0.3316582914572864,
      "grad_norm": 12.339908599853516,
      "learning_rate": 0.00019766025896679164,
      "loss": 0.5312,
      "step": 4620
    },
    {
      "epoch": 0.33237616654702085,
      "grad_norm": 5.90490198135376,
      "learning_rate": 0.00019760708303421872,
      "loss": 0.3485,
      "step": 4630
    },
    {
      "epoch": 0.3330940416367552,
      "grad_norm": 2.8920788764953613,
      "learning_rate": 0.0001975539071016458,
      "loss": 0.467,
      "step": 4640
    },
    {
      "epoch": 0.3338119167264896,
      "grad_norm": 11.631108283996582,
      "learning_rate": 0.00019750073116907288,
      "loss": 0.5198,
      "step": 4650
    },
    {
      "epoch": 0.334529791816224,
      "grad_norm": 21.798887252807617,
      "learning_rate": 0.00019744755523649996,
      "loss": 0.5797,
      "step": 4660
    },
    {
      "epoch": 0.33524766690595836,
      "grad_norm": 4.397892475128174,
      "learning_rate": 0.00019739437930392707,
      "loss": 0.4747,
      "step": 4670
    },
    {
      "epoch": 0.33596554199569273,
      "grad_norm": 8.400347709655762,
      "learning_rate": 0.00019734120337135412,
      "loss": 0.5269,
      "step": 4680
    },
    {
      "epoch": 0.33668341708542715,
      "grad_norm": 12.330069541931152,
      "learning_rate": 0.0001972880274387812,
      "loss": 0.7241,
      "step": 4690
    },
    {
      "epoch": 0.3374012921751615,
      "grad_norm": 7.536892414093018,
      "learning_rate": 0.0001972348515062083,
      "loss": 0.3498,
      "step": 4700
    },
    {
      "epoch": 0.3381191672648959,
      "grad_norm": 5.63746976852417,
      "learning_rate": 0.00019718167557363539,
      "loss": 0.3424,
      "step": 4710
    },
    {
      "epoch": 0.3388370423546303,
      "grad_norm": 12.748370170593262,
      "learning_rate": 0.00019712849964106247,
      "loss": 0.5958,
      "step": 4720
    },
    {
      "epoch": 0.33955491744436467,
      "grad_norm": 4.763253688812256,
      "learning_rate": 0.00019707532370848954,
      "loss": 0.2449,
      "step": 4730
    },
    {
      "epoch": 0.3402727925340991,
      "grad_norm": 19.1805362701416,
      "learning_rate": 0.00019702214777591662,
      "loss": 0.5567,
      "step": 4740
    },
    {
      "epoch": 0.34099066762383345,
      "grad_norm": 3.5447518825531006,
      "learning_rate": 0.0001969689718433437,
      "loss": 0.2443,
      "step": 4750
    },
    {
      "epoch": 0.3417085427135678,
      "grad_norm": 14.088926315307617,
      "learning_rate": 0.00019691579591077078,
      "loss": 0.3841,
      "step": 4760
    },
    {
      "epoch": 0.34242641780330224,
      "grad_norm": 22.563657760620117,
      "learning_rate": 0.0001968626199781979,
      "loss": 0.4225,
      "step": 4770
    },
    {
      "epoch": 0.3431442928930366,
      "grad_norm": 10.883078575134277,
      "learning_rate": 0.00019680944404562497,
      "loss": 0.4275,
      "step": 4780
    },
    {
      "epoch": 0.34386216798277097,
      "grad_norm": 21.19317054748535,
      "learning_rate": 0.00019675626811305202,
      "loss": 0.4818,
      "step": 4790
    },
    {
      "epoch": 0.3445800430725054,
      "grad_norm": 15.403902053833008,
      "learning_rate": 0.00019670309218047913,
      "loss": 0.5448,
      "step": 4800
    },
    {
      "epoch": 0.34529791816223976,
      "grad_norm": 15.030860900878906,
      "learning_rate": 0.0001966499162479062,
      "loss": 0.4471,
      "step": 4810
    },
    {
      "epoch": 0.3460157932519742,
      "grad_norm": 15.02364730834961,
      "learning_rate": 0.0001965967403153333,
      "loss": 0.3075,
      "step": 4820
    },
    {
      "epoch": 0.34673366834170855,
      "grad_norm": 11.558250427246094,
      "learning_rate": 0.0001965435643827604,
      "loss": 0.44,
      "step": 4830
    },
    {
      "epoch": 0.3474515434314429,
      "grad_norm": 10.083312034606934,
      "learning_rate": 0.00019649038845018745,
      "loss": 0.451,
      "step": 4840
    },
    {
      "epoch": 0.34816941852117733,
      "grad_norm": 4.943320274353027,
      "learning_rate": 0.00019643721251761453,
      "loss": 0.326,
      "step": 4850
    },
    {
      "epoch": 0.3488872936109117,
      "grad_norm": 8.093174934387207,
      "learning_rate": 0.0001963840365850416,
      "loss": 0.5513,
      "step": 4860
    },
    {
      "epoch": 0.34960516870064606,
      "grad_norm": 1.1377662420272827,
      "learning_rate": 0.0001963308606524687,
      "loss": 0.1973,
      "step": 4870
    },
    {
      "epoch": 0.3503230437903805,
      "grad_norm": 10.857267379760742,
      "learning_rate": 0.0001962776847198958,
      "loss": 0.4409,
      "step": 4880
    },
    {
      "epoch": 0.35104091888011485,
      "grad_norm": 10.852984428405762,
      "learning_rate": 0.00019622450878732287,
      "loss": 0.3151,
      "step": 4890
    },
    {
      "epoch": 0.35175879396984927,
      "grad_norm": 8.47148323059082,
      "learning_rate": 0.00019617133285474995,
      "loss": 0.4907,
      "step": 4900
    },
    {
      "epoch": 0.35247666905958364,
      "grad_norm": 7.478244781494141,
      "learning_rate": 0.00019611815692217703,
      "loss": 0.4335,
      "step": 4910
    },
    {
      "epoch": 0.353194544149318,
      "grad_norm": 7.338900089263916,
      "learning_rate": 0.0001960649809896041,
      "loss": 0.7454,
      "step": 4920
    },
    {
      "epoch": 0.3539124192390524,
      "grad_norm": 16.597192764282227,
      "learning_rate": 0.00019601180505703122,
      "loss": 0.3693,
      "step": 4930
    },
    {
      "epoch": 0.3546302943287868,
      "grad_norm": 19.557016372680664,
      "learning_rate": 0.00019595862912445827,
      "loss": 0.397,
      "step": 4940
    },
    {
      "epoch": 0.35534816941852115,
      "grad_norm": 5.940458297729492,
      "learning_rate": 0.00019590545319188535,
      "loss": 0.5849,
      "step": 4950
    },
    {
      "epoch": 0.3560660445082556,
      "grad_norm": 6.183773040771484,
      "learning_rate": 0.00019585227725931245,
      "loss": 0.5309,
      "step": 4960
    },
    {
      "epoch": 0.35678391959798994,
      "grad_norm": 5.606039524078369,
      "learning_rate": 0.00019579910132673953,
      "loss": 0.6263,
      "step": 4970
    },
    {
      "epoch": 0.35750179468772436,
      "grad_norm": 11.279147148132324,
      "learning_rate": 0.0001957459253941666,
      "loss": 0.4145,
      "step": 4980
    },
    {
      "epoch": 0.3582196697774587,
      "grad_norm": 5.077211856842041,
      "learning_rate": 0.0001956927494615937,
      "loss": 0.4006,
      "step": 4990
    },
    {
      "epoch": 0.3589375448671931,
      "grad_norm": 6.7522501945495605,
      "learning_rate": 0.00019563957352902077,
      "loss": 0.4635,
      "step": 5000
    },
    {
      "epoch": 0.3596554199569275,
      "grad_norm": 4.1663737297058105,
      "learning_rate": 0.00019558639759644785,
      "loss": 0.3869,
      "step": 5010
    },
    {
      "epoch": 0.3603732950466619,
      "grad_norm": 11.639829635620117,
      "learning_rate": 0.00019553322166387493,
      "loss": 0.5771,
      "step": 5020
    },
    {
      "epoch": 0.36109117013639624,
      "grad_norm": 4.6015119552612305,
      "learning_rate": 0.00019548004573130204,
      "loss": 0.2189,
      "step": 5030
    },
    {
      "epoch": 0.36180904522613067,
      "grad_norm": 8.544771194458008,
      "learning_rate": 0.00019542686979872912,
      "loss": 0.42,
      "step": 5040
    },
    {
      "epoch": 0.36252692031586503,
      "grad_norm": 12.798040390014648,
      "learning_rate": 0.00019537369386615617,
      "loss": 0.3761,
      "step": 5050
    },
    {
      "epoch": 0.36324479540559945,
      "grad_norm": 16.509723663330078,
      "learning_rate": 0.00019532051793358328,
      "loss": 0.6062,
      "step": 5060
    },
    {
      "epoch": 0.3639626704953338,
      "grad_norm": 9.2608003616333,
      "learning_rate": 0.00019526734200101036,
      "loss": 0.4694,
      "step": 5070
    },
    {
      "epoch": 0.3646805455850682,
      "grad_norm": 8.28105354309082,
      "learning_rate": 0.00019521416606843744,
      "loss": 0.2348,
      "step": 5080
    },
    {
      "epoch": 0.3653984206748026,
      "grad_norm": 13.462971687316895,
      "learning_rate": 0.00019516099013586451,
      "loss": 0.7954,
      "step": 5090
    },
    {
      "epoch": 0.36611629576453697,
      "grad_norm": 12.313349723815918,
      "learning_rate": 0.0001951078142032916,
      "loss": 0.5315,
      "step": 5100
    },
    {
      "epoch": 0.36683417085427134,
      "grad_norm": 8.542134284973145,
      "learning_rate": 0.00019505463827071867,
      "loss": 0.5542,
      "step": 5110
    },
    {
      "epoch": 0.36755204594400576,
      "grad_norm": 12.923853874206543,
      "learning_rate": 0.00019500146233814575,
      "loss": 0.6191,
      "step": 5120
    },
    {
      "epoch": 0.3682699210337401,
      "grad_norm": 20.287561416625977,
      "learning_rate": 0.00019494828640557286,
      "loss": 0.6066,
      "step": 5130
    },
    {
      "epoch": 0.36898779612347454,
      "grad_norm": 14.147100448608398,
      "learning_rate": 0.00019489511047299994,
      "loss": 0.4863,
      "step": 5140
    },
    {
      "epoch": 0.3697056712132089,
      "grad_norm": 5.0502519607543945,
      "learning_rate": 0.00019484193454042702,
      "loss": 0.2242,
      "step": 5150
    },
    {
      "epoch": 0.3704235463029433,
      "grad_norm": 3.1671619415283203,
      "learning_rate": 0.0001947887586078541,
      "loss": 0.321,
      "step": 5160
    },
    {
      "epoch": 0.3711414213926777,
      "grad_norm": 10.874890327453613,
      "learning_rate": 0.00019473558267528118,
      "loss": 0.5395,
      "step": 5170
    },
    {
      "epoch": 0.37185929648241206,
      "grad_norm": 8.386327743530273,
      "learning_rate": 0.00019468240674270826,
      "loss": 0.4615,
      "step": 5180
    },
    {
      "epoch": 0.3725771715721464,
      "grad_norm": 7.35896635055542,
      "learning_rate": 0.00019462923081013534,
      "loss": 0.4152,
      "step": 5190
    },
    {
      "epoch": 0.37329504666188085,
      "grad_norm": 2.0454447269439697,
      "learning_rate": 0.00019457605487756244,
      "loss": 0.1986,
      "step": 5200
    },
    {
      "epoch": 0.3740129217516152,
      "grad_norm": 8.753235816955566,
      "learning_rate": 0.0001945228789449895,
      "loss": 0.2013,
      "step": 5210
    },
    {
      "epoch": 0.3747307968413496,
      "grad_norm": 17.900259017944336,
      "learning_rate": 0.00019446970301241658,
      "loss": 0.6464,
      "step": 5220
    },
    {
      "epoch": 0.375448671931084,
      "grad_norm": 12.577472686767578,
      "learning_rate": 0.00019441652707984368,
      "loss": 0.6416,
      "step": 5230
    },
    {
      "epoch": 0.37616654702081836,
      "grad_norm": 2.650564432144165,
      "learning_rate": 0.00019436335114727076,
      "loss": 0.2329,
      "step": 5240
    },
    {
      "epoch": 0.3768844221105528,
      "grad_norm": 10.979334831237793,
      "learning_rate": 0.00019431017521469784,
      "loss": 0.5854,
      "step": 5250
    },
    {
      "epoch": 0.37760229720028715,
      "grad_norm": 4.514771461486816,
      "learning_rate": 0.00019425699928212492,
      "loss": 0.4185,
      "step": 5260
    },
    {
      "epoch": 0.3783201722900215,
      "grad_norm": 7.794284343719482,
      "learning_rate": 0.000194203823349552,
      "loss": 0.2701,
      "step": 5270
    },
    {
      "epoch": 0.37903804737975594,
      "grad_norm": 8.345793724060059,
      "learning_rate": 0.00019415064741697908,
      "loss": 0.3064,
      "step": 5280
    },
    {
      "epoch": 0.3797559224694903,
      "grad_norm": 3.9765987396240234,
      "learning_rate": 0.00019409747148440616,
      "loss": 0.4509,
      "step": 5290
    },
    {
      "epoch": 0.38047379755922467,
      "grad_norm": 7.38222074508667,
      "learning_rate": 0.00019404429555183327,
      "loss": 0.5488,
      "step": 5300
    },
    {
      "epoch": 0.3811916726489591,
      "grad_norm": 7.677720546722412,
      "learning_rate": 0.00019399111961926032,
      "loss": 0.3234,
      "step": 5310
    },
    {
      "epoch": 0.38190954773869346,
      "grad_norm": 12.818082809448242,
      "learning_rate": 0.0001939379436866874,
      "loss": 0.5429,
      "step": 5320
    },
    {
      "epoch": 0.3826274228284279,
      "grad_norm": 13.303092002868652,
      "learning_rate": 0.0001938847677541145,
      "loss": 0.6036,
      "step": 5330
    },
    {
      "epoch": 0.38334529791816224,
      "grad_norm": 5.107619762420654,
      "learning_rate": 0.00019383159182154158,
      "loss": 0.1822,
      "step": 5340
    },
    {
      "epoch": 0.3840631730078966,
      "grad_norm": 11.150349617004395,
      "learning_rate": 0.00019377841588896866,
      "loss": 0.5202,
      "step": 5350
    },
    {
      "epoch": 0.38478104809763103,
      "grad_norm": 6.818055152893066,
      "learning_rate": 0.00019372523995639574,
      "loss": 0.402,
      "step": 5360
    },
    {
      "epoch": 0.3854989231873654,
      "grad_norm": 8.858460426330566,
      "learning_rate": 0.00019367206402382282,
      "loss": 0.4618,
      "step": 5370
    },
    {
      "epoch": 0.38621679827709976,
      "grad_norm": 12.6859712600708,
      "learning_rate": 0.0001936188880912499,
      "loss": 0.6189,
      "step": 5380
    },
    {
      "epoch": 0.3869346733668342,
      "grad_norm": 13.560842514038086,
      "learning_rate": 0.00019356571215867698,
      "loss": 0.3607,
      "step": 5390
    },
    {
      "epoch": 0.38765254845656855,
      "grad_norm": 9.796453475952148,
      "learning_rate": 0.0001935125362261041,
      "loss": 0.3119,
      "step": 5400
    },
    {
      "epoch": 0.38837042354630297,
      "grad_norm": 0.4368685781955719,
      "learning_rate": 0.00019345936029353117,
      "loss": 0.2479,
      "step": 5410
    },
    {
      "epoch": 0.38908829863603733,
      "grad_norm": 0.3289455771446228,
      "learning_rate": 0.00019340618436095822,
      "loss": 0.3899,
      "step": 5420
    },
    {
      "epoch": 0.3898061737257717,
      "grad_norm": 6.108245849609375,
      "learning_rate": 0.00019335300842838533,
      "loss": 0.1871,
      "step": 5430
    },
    {
      "epoch": 0.3905240488155061,
      "grad_norm": 18.913681030273438,
      "learning_rate": 0.0001932998324958124,
      "loss": 0.4912,
      "step": 5440
    },
    {
      "epoch": 0.3912419239052405,
      "grad_norm": 7.049358367919922,
      "learning_rate": 0.00019324665656323949,
      "loss": 0.4857,
      "step": 5450
    },
    {
      "epoch": 0.39195979899497485,
      "grad_norm": 3.6453819274902344,
      "learning_rate": 0.0001931934806306666,
      "loss": 0.4473,
      "step": 5460
    },
    {
      "epoch": 0.39267767408470927,
      "grad_norm": 18.030048370361328,
      "learning_rate": 0.00019314030469809364,
      "loss": 0.6971,
      "step": 5470
    },
    {
      "epoch": 0.39339554917444364,
      "grad_norm": 3.3645641803741455,
      "learning_rate": 0.00019308712876552072,
      "loss": 0.344,
      "step": 5480
    },
    {
      "epoch": 0.39411342426417806,
      "grad_norm": 0.4566713571548462,
      "learning_rate": 0.0001930339528329478,
      "loss": 0.706,
      "step": 5490
    },
    {
      "epoch": 0.3948312993539124,
      "grad_norm": 10.644160270690918,
      "learning_rate": 0.0001929807769003749,
      "loss": 0.2938,
      "step": 5500
    },
    {
      "epoch": 0.3955491744436468,
      "grad_norm": 15.868194580078125,
      "learning_rate": 0.000192927600967802,
      "loss": 0.3967,
      "step": 5510
    },
    {
      "epoch": 0.3962670495333812,
      "grad_norm": 12.511391639709473,
      "learning_rate": 0.00019287442503522907,
      "loss": 0.2871,
      "step": 5520
    },
    {
      "epoch": 0.3969849246231156,
      "grad_norm": 8.974679946899414,
      "learning_rate": 0.00019282124910265615,
      "loss": 0.287,
      "step": 5530
    },
    {
      "epoch": 0.39770279971284994,
      "grad_norm": 10.916380882263184,
      "learning_rate": 0.00019276807317008323,
      "loss": 0.3222,
      "step": 5540
    },
    {
      "epoch": 0.39842067480258436,
      "grad_norm": 20.757667541503906,
      "learning_rate": 0.0001927148972375103,
      "loss": 0.5902,
      "step": 5550
    },
    {
      "epoch": 0.39913854989231873,
      "grad_norm": 0.08000332117080688,
      "learning_rate": 0.00019266172130493741,
      "loss": 0.3604,
      "step": 5560
    },
    {
      "epoch": 0.39985642498205315,
      "grad_norm": 5.108635425567627,
      "learning_rate": 0.00019260854537236447,
      "loss": 0.179,
      "step": 5570
    },
    {
      "epoch": 0.4005743000717875,
      "grad_norm": 16.281661987304688,
      "learning_rate": 0.00019255536943979155,
      "loss": 0.6113,
      "step": 5580
    },
    {
      "epoch": 0.4012921751615219,
      "grad_norm": 8.11465835571289,
      "learning_rate": 0.00019250219350721863,
      "loss": 0.7441,
      "step": 5590
    },
    {
      "epoch": 0.4020100502512563,
      "grad_norm": 6.220599174499512,
      "learning_rate": 0.00019244901757464573,
      "loss": 0.5548,
      "step": 5600
    },
    {
      "epoch": 0.40272792534099067,
      "grad_norm": 16.756446838378906,
      "learning_rate": 0.0001923958416420728,
      "loss": 0.5356,
      "step": 5610
    },
    {
      "epoch": 0.40344580043072503,
      "grad_norm": 2.5775113105773926,
      "learning_rate": 0.0001923426657094999,
      "loss": 0.3704,
      "step": 5620
    },
    {
      "epoch": 0.40416367552045945,
      "grad_norm": 9.347640037536621,
      "learning_rate": 0.00019228948977692697,
      "loss": 0.6637,
      "step": 5630
    },
    {
      "epoch": 0.4048815506101938,
      "grad_norm": 6.568061828613281,
      "learning_rate": 0.00019223631384435405,
      "loss": 0.4897,
      "step": 5640
    },
    {
      "epoch": 0.40559942569992824,
      "grad_norm": 11.5574951171875,
      "learning_rate": 0.00019218313791178113,
      "loss": 0.4035,
      "step": 5650
    },
    {
      "epoch": 0.4063173007896626,
      "grad_norm": 5.570053577423096,
      "learning_rate": 0.00019212996197920824,
      "loss": 0.2762,
      "step": 5660
    },
    {
      "epoch": 0.40703517587939697,
      "grad_norm": 5.441007614135742,
      "learning_rate": 0.00019207678604663532,
      "loss": 0.5357,
      "step": 5670
    },
    {
      "epoch": 0.4077530509691314,
      "grad_norm": 3.05793833732605,
      "learning_rate": 0.00019202361011406237,
      "loss": 0.3223,
      "step": 5680
    },
    {
      "epoch": 0.40847092605886576,
      "grad_norm": 9.944670677185059,
      "learning_rate": 0.00019197043418148947,
      "loss": 0.6236,
      "step": 5690
    },
    {
      "epoch": 0.4091888011486001,
      "grad_norm": 7.713029861450195,
      "learning_rate": 0.00019191725824891655,
      "loss": 0.601,
      "step": 5700
    },
    {
      "epoch": 0.40990667623833454,
      "grad_norm": 5.449270725250244,
      "learning_rate": 0.00019186408231634363,
      "loss": 0.2632,
      "step": 5710
    },
    {
      "epoch": 0.4106245513280689,
      "grad_norm": 5.400418758392334,
      "learning_rate": 0.0001918109063837707,
      "loss": 0.4408,
      "step": 5720
    },
    {
      "epoch": 0.4113424264178033,
      "grad_norm": 18.909055709838867,
      "learning_rate": 0.0001917577304511978,
      "loss": 0.4743,
      "step": 5730
    },
    {
      "epoch": 0.4120603015075377,
      "grad_norm": 2.8091673851013184,
      "learning_rate": 0.00019170455451862487,
      "loss": 0.7182,
      "step": 5740
    },
    {
      "epoch": 0.41277817659727206,
      "grad_norm": 20.4910831451416,
      "learning_rate": 0.00019165137858605195,
      "loss": 0.339,
      "step": 5750
    },
    {
      "epoch": 0.4134960516870065,
      "grad_norm": 5.457846164703369,
      "learning_rate": 0.00019159820265347906,
      "loss": 0.3596,
      "step": 5760
    },
    {
      "epoch": 0.41421392677674085,
      "grad_norm": 6.282767295837402,
      "learning_rate": 0.00019154502672090614,
      "loss": 0.5098,
      "step": 5770
    },
    {
      "epoch": 0.4149318018664752,
      "grad_norm": 0.5734180808067322,
      "learning_rate": 0.00019149185078833322,
      "loss": 0.2693,
      "step": 5780
    },
    {
      "epoch": 0.41564967695620963,
      "grad_norm": 6.3380208015441895,
      "learning_rate": 0.0001914386748557603,
      "loss": 0.3557,
      "step": 5790
    },
    {
      "epoch": 0.416367552045944,
      "grad_norm": 2.1359846591949463,
      "learning_rate": 0.00019138549892318738,
      "loss": 0.3991,
      "step": 5800
    },
    {
      "epoch": 0.41708542713567837,
      "grad_norm": 6.145281791687012,
      "learning_rate": 0.00019133232299061446,
      "loss": 0.4695,
      "step": 5810
    },
    {
      "epoch": 0.4178033022254128,
      "grad_norm": 1.0400123596191406,
      "learning_rate": 0.00019127914705804153,
      "loss": 0.2105,
      "step": 5820
    },
    {
      "epoch": 0.41852117731514715,
      "grad_norm": 4.196476459503174,
      "learning_rate": 0.00019122597112546864,
      "loss": 0.4844,
      "step": 5830
    },
    {
      "epoch": 0.4192390524048816,
      "grad_norm": 14.732769966125488,
      "learning_rate": 0.0001911727951928957,
      "loss": 0.3686,
      "step": 5840
    },
    {
      "epoch": 0.41995692749461594,
      "grad_norm": 9.705808639526367,
      "learning_rate": 0.00019111961926032277,
      "loss": 0.3445,
      "step": 5850
    },
    {
      "epoch": 0.4206748025843503,
      "grad_norm": 8.968667030334473,
      "learning_rate": 0.00019106644332774988,
      "loss": 0.3547,
      "step": 5860
    },
    {
      "epoch": 0.4213926776740847,
      "grad_norm": 11.756317138671875,
      "learning_rate": 0.00019101326739517696,
      "loss": 0.1988,
      "step": 5870
    },
    {
      "epoch": 0.4221105527638191,
      "grad_norm": 3.2433321475982666,
      "learning_rate": 0.00019096009146260404,
      "loss": 0.4383,
      "step": 5880
    },
    {
      "epoch": 0.42282842785355346,
      "grad_norm": 10.101899147033691,
      "learning_rate": 0.00019090691553003112,
      "loss": 0.486,
      "step": 5890
    },
    {
      "epoch": 0.4235463029432879,
      "grad_norm": 7.738649368286133,
      "learning_rate": 0.0001908537395974582,
      "loss": 0.3654,
      "step": 5900
    },
    {
      "epoch": 0.42426417803302224,
      "grad_norm": 4.316162109375,
      "learning_rate": 0.00019080056366488528,
      "loss": 0.1999,
      "step": 5910
    },
    {
      "epoch": 0.42498205312275666,
      "grad_norm": 18.913829803466797,
      "learning_rate": 0.00019074738773231236,
      "loss": 0.6624,
      "step": 5920
    },
    {
      "epoch": 0.42569992821249103,
      "grad_norm": 12.339452743530273,
      "learning_rate": 0.00019069421179973946,
      "loss": 0.4171,
      "step": 5930
    },
    {
      "epoch": 0.4264178033022254,
      "grad_norm": 1.5401605367660522,
      "learning_rate": 0.00019064103586716652,
      "loss": 0.3809,
      "step": 5940
    },
    {
      "epoch": 0.4271356783919598,
      "grad_norm": 12.726974487304688,
      "learning_rate": 0.0001905878599345936,
      "loss": 0.2511,
      "step": 5950
    },
    {
      "epoch": 0.4278535534816942,
      "grad_norm": 1.6857913732528687,
      "learning_rate": 0.0001905346840020207,
      "loss": 0.2439,
      "step": 5960
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.9145579934120178,
      "learning_rate": 0.00019048150806944778,
      "loss": 0.3648,
      "step": 5970
    },
    {
      "epoch": 0.42928930366116297,
      "grad_norm": 0.48263072967529297,
      "learning_rate": 0.00019042833213687486,
      "loss": 0.3856,
      "step": 5980
    },
    {
      "epoch": 0.43000717875089733,
      "grad_norm": 7.279073238372803,
      "learning_rate": 0.00019037515620430194,
      "loss": 0.3278,
      "step": 5990
    },
    {
      "epoch": 0.43072505384063176,
      "grad_norm": 12.230714797973633,
      "learning_rate": 0.00019032198027172902,
      "loss": 0.5808,
      "step": 6000
    },
    {
      "epoch": 0.4314429289303661,
      "grad_norm": 18.369977951049805,
      "learning_rate": 0.0001902688043391561,
      "loss": 0.4693,
      "step": 6010
    },
    {
      "epoch": 0.4321608040201005,
      "grad_norm": 10.01547622680664,
      "learning_rate": 0.00019021562840658318,
      "loss": 0.3026,
      "step": 6020
    },
    {
      "epoch": 0.4328786791098349,
      "grad_norm": 1.9660993814468384,
      "learning_rate": 0.00019016245247401029,
      "loss": 0.4154,
      "step": 6030
    },
    {
      "epoch": 0.4335965541995693,
      "grad_norm": 15.829273223876953,
      "learning_rate": 0.00019010927654143737,
      "loss": 0.2193,
      "step": 6040
    },
    {
      "epoch": 0.43431442928930364,
      "grad_norm": 10.128904342651367,
      "learning_rate": 0.00019005610060886442,
      "loss": 0.4371,
      "step": 6050
    },
    {
      "epoch": 0.43503230437903806,
      "grad_norm": 8.871116638183594,
      "learning_rate": 0.00019000292467629152,
      "loss": 0.2934,
      "step": 6060
    },
    {
      "epoch": 0.4357501794687724,
      "grad_norm": 12.289554595947266,
      "learning_rate": 0.0001899497487437186,
      "loss": 0.3466,
      "step": 6070
    },
    {
      "epoch": 0.43646805455850685,
      "grad_norm": 12.905600547790527,
      "learning_rate": 0.00018989657281114568,
      "loss": 0.3259,
      "step": 6080
    },
    {
      "epoch": 0.4371859296482412,
      "grad_norm": 4.699131965637207,
      "learning_rate": 0.0001898433968785728,
      "loss": 0.3896,
      "step": 6090
    },
    {
      "epoch": 0.4379038047379756,
      "grad_norm": 12.362549781799316,
      "learning_rate": 0.00018979022094599984,
      "loss": 0.2261,
      "step": 6100
    },
    {
      "epoch": 0.43862167982771,
      "grad_norm": 9.608431816101074,
      "learning_rate": 0.00018973704501342692,
      "loss": 0.6053,
      "step": 6110
    },
    {
      "epoch": 0.43933955491744436,
      "grad_norm": 4.108154296875,
      "learning_rate": 0.000189683869080854,
      "loss": 0.286,
      "step": 6120
    },
    {
      "epoch": 0.44005743000717873,
      "grad_norm": 10.134041786193848,
      "learning_rate": 0.0001896306931482811,
      "loss": 0.4266,
      "step": 6130
    },
    {
      "epoch": 0.44077530509691315,
      "grad_norm": 10.39553451538086,
      "learning_rate": 0.0001895775172157082,
      "loss": 0.4247,
      "step": 6140
    },
    {
      "epoch": 0.4414931801866475,
      "grad_norm": 0.886890172958374,
      "learning_rate": 0.00018952434128313527,
      "loss": 0.3649,
      "step": 6150
    },
    {
      "epoch": 0.44221105527638194,
      "grad_norm": 19.1607666015625,
      "learning_rate": 0.00018947116535056235,
      "loss": 0.3209,
      "step": 6160
    },
    {
      "epoch": 0.4429289303661163,
      "grad_norm": 3.6741297245025635,
      "learning_rate": 0.00018941798941798943,
      "loss": 0.6046,
      "step": 6170
    },
    {
      "epoch": 0.44364680545585067,
      "grad_norm": 9.068282127380371,
      "learning_rate": 0.0001893648134854165,
      "loss": 0.1751,
      "step": 6180
    },
    {
      "epoch": 0.4443646805455851,
      "grad_norm": 14.862722396850586,
      "learning_rate": 0.0001893116375528436,
      "loss": 0.4676,
      "step": 6190
    },
    {
      "epoch": 0.44508255563531945,
      "grad_norm": 3.279519557952881,
      "learning_rate": 0.00018925846162027066,
      "loss": 0.3579,
      "step": 6200
    },
    {
      "epoch": 0.4458004307250538,
      "grad_norm": 6.97314453125,
      "learning_rate": 0.00018920528568769774,
      "loss": 0.305,
      "step": 6210
    },
    {
      "epoch": 0.44651830581478824,
      "grad_norm": 0.4417423903942108,
      "learning_rate": 0.00018915210975512482,
      "loss": 0.3203,
      "step": 6220
    },
    {
      "epoch": 0.4472361809045226,
      "grad_norm": 7.092375755310059,
      "learning_rate": 0.00018909893382255193,
      "loss": 0.5458,
      "step": 6230
    },
    {
      "epoch": 0.44795405599425697,
      "grad_norm": 1.6025340557098389,
      "learning_rate": 0.000189045757889979,
      "loss": 0.3566,
      "step": 6240
    },
    {
      "epoch": 0.4486719310839914,
      "grad_norm": 2.5099899768829346,
      "learning_rate": 0.0001889925819574061,
      "loss": 0.4303,
      "step": 6250
    },
    {
      "epoch": 0.44938980617372576,
      "grad_norm": 9.029745101928711,
      "learning_rate": 0.00018893940602483317,
      "loss": 0.4401,
      "step": 6260
    },
    {
      "epoch": 0.4501076812634602,
      "grad_norm": 7.850565433502197,
      "learning_rate": 0.00018888623009226025,
      "loss": 0.3198,
      "step": 6270
    },
    {
      "epoch": 0.45082555635319455,
      "grad_norm": 20.868484497070312,
      "learning_rate": 0.00018883305415968733,
      "loss": 0.4384,
      "step": 6280
    },
    {
      "epoch": 0.4515434314429289,
      "grad_norm": 7.925074100494385,
      "learning_rate": 0.00018877987822711443,
      "loss": 0.3184,
      "step": 6290
    },
    {
      "epoch": 0.45226130653266333,
      "grad_norm": 11.883323669433594,
      "learning_rate": 0.0001887267022945415,
      "loss": 0.2464,
      "step": 6300
    },
    {
      "epoch": 0.4529791816223977,
      "grad_norm": 16.936016082763672,
      "learning_rate": 0.00018867352636196857,
      "loss": 0.2517,
      "step": 6310
    },
    {
      "epoch": 0.45369705671213206,
      "grad_norm": 9.792603492736816,
      "learning_rate": 0.00018862035042939565,
      "loss": 0.3038,
      "step": 6320
    },
    {
      "epoch": 0.4544149318018665,
      "grad_norm": 6.019199371337891,
      "learning_rate": 0.00018856717449682275,
      "loss": 0.2826,
      "step": 6330
    },
    {
      "epoch": 0.45513280689160085,
      "grad_norm": 0.16951590776443481,
      "learning_rate": 0.00018851399856424983,
      "loss": 0.4083,
      "step": 6340
    },
    {
      "epoch": 0.45585068198133527,
      "grad_norm": 9.582266807556152,
      "learning_rate": 0.0001884608226316769,
      "loss": 0.4131,
      "step": 6350
    },
    {
      "epoch": 0.45656855707106964,
      "grad_norm": 4.012490749359131,
      "learning_rate": 0.000188407646699104,
      "loss": 0.51,
      "step": 6360
    },
    {
      "epoch": 0.457286432160804,
      "grad_norm": 13.282256126403809,
      "learning_rate": 0.00018835447076653107,
      "loss": 0.3406,
      "step": 6370
    },
    {
      "epoch": 0.4580043072505384,
      "grad_norm": 4.134006500244141,
      "learning_rate": 0.00018830129483395815,
      "loss": 0.3022,
      "step": 6380
    },
    {
      "epoch": 0.4587221823402728,
      "grad_norm": 15.983673095703125,
      "learning_rate": 0.00018824811890138526,
      "loss": 0.3505,
      "step": 6390
    },
    {
      "epoch": 0.45944005743000715,
      "grad_norm": 4.695316314697266,
      "learning_rate": 0.00018819494296881234,
      "loss": 0.3607,
      "step": 6400
    },
    {
      "epoch": 0.4601579325197416,
      "grad_norm": 10.677056312561035,
      "learning_rate": 0.00018814176703623942,
      "loss": 0.3803,
      "step": 6410
    },
    {
      "epoch": 0.46087580760947594,
      "grad_norm": 7.203023910522461,
      "learning_rate": 0.0001880885911036665,
      "loss": 0.3697,
      "step": 6420
    },
    {
      "epoch": 0.46159368269921036,
      "grad_norm": 1.926374077796936,
      "learning_rate": 0.00018803541517109357,
      "loss": 0.2925,
      "step": 6430
    },
    {
      "epoch": 0.4623115577889447,
      "grad_norm": 3.7185091972351074,
      "learning_rate": 0.00018798223923852065,
      "loss": 0.2662,
      "step": 6440
    },
    {
      "epoch": 0.4630294328786791,
      "grad_norm": 8.342915534973145,
      "learning_rate": 0.00018792906330594773,
      "loss": 0.4234,
      "step": 6450
    },
    {
      "epoch": 0.4637473079684135,
      "grad_norm": 10.45308780670166,
      "learning_rate": 0.0001878758873733748,
      "loss": 0.2766,
      "step": 6460
    },
    {
      "epoch": 0.4644651830581479,
      "grad_norm": 20.496828079223633,
      "learning_rate": 0.0001878227114408019,
      "loss": 0.2704,
      "step": 6470
    },
    {
      "epoch": 0.46518305814788224,
      "grad_norm": 9.836297988891602,
      "learning_rate": 0.00018776953550822897,
      "loss": 0.6412,
      "step": 6480
    },
    {
      "epoch": 0.46590093323761667,
      "grad_norm": 1.7075589895248413,
      "learning_rate": 0.00018771635957565608,
      "loss": 0.5082,
      "step": 6490
    },
    {
      "epoch": 0.46661880832735103,
      "grad_norm": 6.057934284210205,
      "learning_rate": 0.00018766318364308316,
      "loss": 0.3769,
      "step": 6500
    },
    {
      "epoch": 0.46733668341708545,
      "grad_norm": 14.791902542114258,
      "learning_rate": 0.00018761000771051024,
      "loss": 0.3393,
      "step": 6510
    },
    {
      "epoch": 0.4680545585068198,
      "grad_norm": 10.102235794067383,
      "learning_rate": 0.00018755683177793732,
      "loss": 0.417,
      "step": 6520
    },
    {
      "epoch": 0.4687724335965542,
      "grad_norm": 1.2361880540847778,
      "learning_rate": 0.0001875036558453644,
      "loss": 0.3966,
      "step": 6530
    },
    {
      "epoch": 0.4694903086862886,
      "grad_norm": 2.5716307163238525,
      "learning_rate": 0.00018745047991279148,
      "loss": 0.5458,
      "step": 6540
    },
    {
      "epoch": 0.47020818377602297,
      "grad_norm": 3.456779956817627,
      "learning_rate": 0.00018739730398021856,
      "loss": 0.339,
      "step": 6550
    },
    {
      "epoch": 0.47092605886575734,
      "grad_norm": 10.23364543914795,
      "learning_rate": 0.00018734412804764566,
      "loss": 0.5789,
      "step": 6560
    },
    {
      "epoch": 0.47164393395549176,
      "grad_norm": 1.9407559633255005,
      "learning_rate": 0.00018729095211507271,
      "loss": 0.3717,
      "step": 6570
    },
    {
      "epoch": 0.4723618090452261,
      "grad_norm": 24.503944396972656,
      "learning_rate": 0.0001872377761824998,
      "loss": 0.6088,
      "step": 6580
    },
    {
      "epoch": 0.47307968413496054,
      "grad_norm": 26.721452713012695,
      "learning_rate": 0.0001871846002499269,
      "loss": 0.6956,
      "step": 6590
    },
    {
      "epoch": 0.4737975592246949,
      "grad_norm": 8.180331230163574,
      "learning_rate": 0.00018713142431735398,
      "loss": 0.5237,
      "step": 6600
    },
    {
      "epoch": 0.4745154343144293,
      "grad_norm": 7.865305423736572,
      "learning_rate": 0.00018707824838478106,
      "loss": 0.1864,
      "step": 6610
    },
    {
      "epoch": 0.4752333094041637,
      "grad_norm": 5.323720932006836,
      "learning_rate": 0.00018702507245220814,
      "loss": 0.4388,
      "step": 6620
    },
    {
      "epoch": 0.47595118449389806,
      "grad_norm": 12.890048027038574,
      "learning_rate": 0.00018697189651963522,
      "loss": 0.2993,
      "step": 6630
    },
    {
      "epoch": 0.4766690595836324,
      "grad_norm": 15.062538146972656,
      "learning_rate": 0.0001869187205870623,
      "loss": 0.3374,
      "step": 6640
    },
    {
      "epoch": 0.47738693467336685,
      "grad_norm": 0.2666378319263458,
      "learning_rate": 0.00018686554465448938,
      "loss": 0.2852,
      "step": 6650
    },
    {
      "epoch": 0.4781048097631012,
      "grad_norm": 8.282364845275879,
      "learning_rate": 0.00018681236872191648,
      "loss": 0.1746,
      "step": 6660
    },
    {
      "epoch": 0.47882268485283563,
      "grad_norm": 11.403924942016602,
      "learning_rate": 0.00018675919278934356,
      "loss": 0.2399,
      "step": 6670
    },
    {
      "epoch": 0.47954055994257,
      "grad_norm": 20.959558486938477,
      "learning_rate": 0.00018670601685677062,
      "loss": 0.2952,
      "step": 6680
    },
    {
      "epoch": 0.48025843503230436,
      "grad_norm": 13.743422508239746,
      "learning_rate": 0.00018665284092419772,
      "loss": 0.3306,
      "step": 6690
    },
    {
      "epoch": 0.4809763101220388,
      "grad_norm": 6.597092628479004,
      "learning_rate": 0.0001865996649916248,
      "loss": 0.2929,
      "step": 6700
    },
    {
      "epoch": 0.48169418521177315,
      "grad_norm": 8.648802757263184,
      "learning_rate": 0.00018654648905905188,
      "loss": 0.3724,
      "step": 6710
    },
    {
      "epoch": 0.4824120603015075,
      "grad_norm": 11.164175033569336,
      "learning_rate": 0.000186493313126479,
      "loss": 0.3619,
      "step": 6720
    },
    {
      "epoch": 0.48312993539124194,
      "grad_norm": 17.181758880615234,
      "learning_rate": 0.00018644013719390604,
      "loss": 0.6988,
      "step": 6730
    },
    {
      "epoch": 0.4838478104809763,
      "grad_norm": 6.895853042602539,
      "learning_rate": 0.00018638696126133312,
      "loss": 0.2001,
      "step": 6740
    },
    {
      "epoch": 0.48456568557071067,
      "grad_norm": 13.067466735839844,
      "learning_rate": 0.0001863337853287602,
      "loss": 0.3467,
      "step": 6750
    },
    {
      "epoch": 0.4852835606604451,
      "grad_norm": 4.970389366149902,
      "learning_rate": 0.0001862806093961873,
      "loss": 0.2434,
      "step": 6760
    },
    {
      "epoch": 0.48600143575017946,
      "grad_norm": 2.2819292545318604,
      "learning_rate": 0.00018622743346361439,
      "loss": 0.4447,
      "step": 6770
    },
    {
      "epoch": 0.4867193108399139,
      "grad_norm": 7.270702362060547,
      "learning_rate": 0.00018617425753104146,
      "loss": 0.2147,
      "step": 6780
    },
    {
      "epoch": 0.48743718592964824,
      "grad_norm": 9.517415046691895,
      "learning_rate": 0.00018612108159846854,
      "loss": 0.4747,
      "step": 6790
    },
    {
      "epoch": 0.4881550610193826,
      "grad_norm": 21.30824851989746,
      "learning_rate": 0.00018606790566589562,
      "loss": 0.2918,
      "step": 6800
    },
    {
      "epoch": 0.48887293610911703,
      "grad_norm": 20.677383422851562,
      "learning_rate": 0.0001860147297333227,
      "loss": 0.5441,
      "step": 6810
    },
    {
      "epoch": 0.4895908111988514,
      "grad_norm": 14.380184173583984,
      "learning_rate": 0.0001859615538007498,
      "loss": 0.3911,
      "step": 6820
    },
    {
      "epoch": 0.49030868628858576,
      "grad_norm": 10.205326080322266,
      "learning_rate": 0.00018590837786817686,
      "loss": 0.3759,
      "step": 6830
    },
    {
      "epoch": 0.4910265613783202,
      "grad_norm": 7.931687831878662,
      "learning_rate": 0.00018585520193560394,
      "loss": 0.4387,
      "step": 6840
    },
    {
      "epoch": 0.49174443646805455,
      "grad_norm": 1.5726876258850098,
      "learning_rate": 0.00018580202600303102,
      "loss": 0.4125,
      "step": 6850
    },
    {
      "epoch": 0.49246231155778897,
      "grad_norm": 31.060827255249023,
      "learning_rate": 0.00018574885007045813,
      "loss": 0.5584,
      "step": 6860
    },
    {
      "epoch": 0.49318018664752333,
      "grad_norm": 1.9922171831130981,
      "learning_rate": 0.0001856956741378852,
      "loss": 0.3247,
      "step": 6870
    },
    {
      "epoch": 0.4938980617372577,
      "grad_norm": 21.219234466552734,
      "learning_rate": 0.0001856424982053123,
      "loss": 0.5046,
      "step": 6880
    },
    {
      "epoch": 0.4946159368269921,
      "grad_norm": 4.6312665939331055,
      "learning_rate": 0.00018558932227273937,
      "loss": 0.2325,
      "step": 6890
    },
    {
      "epoch": 0.4953338119167265,
      "grad_norm": 0.0615130253136158,
      "learning_rate": 0.00018553614634016645,
      "loss": 0.2838,
      "step": 6900
    },
    {
      "epoch": 0.49605168700646085,
      "grad_norm": 6.883524417877197,
      "learning_rate": 0.00018548297040759353,
      "loss": 0.5049,
      "step": 6910
    },
    {
      "epoch": 0.49676956209619527,
      "grad_norm": 15.710931777954102,
      "learning_rate": 0.00018542979447502063,
      "loss": 0.328,
      "step": 6920
    },
    {
      "epoch": 0.49748743718592964,
      "grad_norm": 11.216903686523438,
      "learning_rate": 0.0001853766185424477,
      "loss": 0.5538,
      "step": 6930
    },
    {
      "epoch": 0.49820531227566406,
      "grad_norm": 6.303172588348389,
      "learning_rate": 0.00018532344260987476,
      "loss": 0.3978,
      "step": 6940
    },
    {
      "epoch": 0.4989231873653984,
      "grad_norm": 4.211390972137451,
      "learning_rate": 0.00018527026667730184,
      "loss": 0.2876,
      "step": 6950
    },
    {
      "epoch": 0.4996410624551328,
      "grad_norm": 1.815003752708435,
      "learning_rate": 0.00018521709074472895,
      "loss": 0.4304,
      "step": 6960
    },
    {
      "epoch": 0.5003589375448672,
      "grad_norm": 8.757102012634277,
      "learning_rate": 0.00018516391481215603,
      "loss": 0.5127,
      "step": 6970
    },
    {
      "epoch": 0.5010768126346016,
      "grad_norm": 0.879818320274353,
      "learning_rate": 0.0001851107388795831,
      "loss": 0.5065,
      "step": 6980
    },
    {
      "epoch": 0.501794687724336,
      "grad_norm": 24.910625457763672,
      "learning_rate": 0.0001850575629470102,
      "loss": 0.4826,
      "step": 6990
    },
    {
      "epoch": 0.5025125628140703,
      "grad_norm": 18.188308715820312,
      "learning_rate": 0.00018500438701443727,
      "loss": 0.2213,
      "step": 7000
    },
    {
      "epoch": 0.5032304379038047,
      "grad_norm": 0.8715868592262268,
      "learning_rate": 0.00018495121108186435,
      "loss": 0.394,
      "step": 7010
    },
    {
      "epoch": 0.5039483129935391,
      "grad_norm": 0.7334087491035461,
      "learning_rate": 0.00018489803514929145,
      "loss": 0.3829,
      "step": 7020
    },
    {
      "epoch": 0.5046661880832735,
      "grad_norm": 8.50461483001709,
      "learning_rate": 0.00018484485921671853,
      "loss": 0.4576,
      "step": 7030
    },
    {
      "epoch": 0.5053840631730079,
      "grad_norm": 1.1896220445632935,
      "learning_rate": 0.0001847916832841456,
      "loss": 0.1344,
      "step": 7040
    },
    {
      "epoch": 0.5061019382627423,
      "grad_norm": 13.271541595458984,
      "learning_rate": 0.00018473850735157267,
      "loss": 0.4457,
      "step": 7050
    },
    {
      "epoch": 0.5068198133524767,
      "grad_norm": 6.624181747436523,
      "learning_rate": 0.00018468533141899977,
      "loss": 0.3256,
      "step": 7060
    },
    {
      "epoch": 0.507537688442211,
      "grad_norm": 0.4745083749294281,
      "learning_rate": 0.00018463215548642685,
      "loss": 0.1436,
      "step": 7070
    },
    {
      "epoch": 0.5082555635319455,
      "grad_norm": 11.775962829589844,
      "learning_rate": 0.00018457897955385393,
      "loss": 0.5311,
      "step": 7080
    },
    {
      "epoch": 0.5089734386216799,
      "grad_norm": 11.129064559936523,
      "learning_rate": 0.000184525803621281,
      "loss": 0.3,
      "step": 7090
    },
    {
      "epoch": 0.5096913137114142,
      "grad_norm": 7.835327625274658,
      "learning_rate": 0.0001844726276887081,
      "loss": 0.2847,
      "step": 7100
    },
    {
      "epoch": 0.5104091888011486,
      "grad_norm": 3.8714513778686523,
      "learning_rate": 0.00018441945175613517,
      "loss": 0.5969,
      "step": 7110
    },
    {
      "epoch": 0.511127063890883,
      "grad_norm": 5.74752140045166,
      "learning_rate": 0.00018436627582356228,
      "loss": 0.2994,
      "step": 7120
    },
    {
      "epoch": 0.5118449389806173,
      "grad_norm": 5.958890438079834,
      "learning_rate": 0.00018431309989098936,
      "loss": 0.2083,
      "step": 7130
    },
    {
      "epoch": 0.5125628140703518,
      "grad_norm": 1.980952262878418,
      "learning_rate": 0.00018425992395841644,
      "loss": 0.2068,
      "step": 7140
    },
    {
      "epoch": 0.5132806891600862,
      "grad_norm": 12.921666145324707,
      "learning_rate": 0.00018420674802584351,
      "loss": 0.4833,
      "step": 7150
    },
    {
      "epoch": 0.5139985642498205,
      "grad_norm": 4.4624223709106445,
      "learning_rate": 0.0001841535720932706,
      "loss": 0.2048,
      "step": 7160
    },
    {
      "epoch": 0.5147164393395549,
      "grad_norm": 0.41596418619155884,
      "learning_rate": 0.00018410039616069767,
      "loss": 0.2531,
      "step": 7170
    },
    {
      "epoch": 0.5154343144292893,
      "grad_norm": 1.4481117725372314,
      "learning_rate": 0.00018404722022812475,
      "loss": 0.3136,
      "step": 7180
    },
    {
      "epoch": 0.5161521895190236,
      "grad_norm": 30.065052032470703,
      "learning_rate": 0.00018399404429555186,
      "loss": 0.5978,
      "step": 7190
    },
    {
      "epoch": 0.5168700646087581,
      "grad_norm": 13.085323333740234,
      "learning_rate": 0.0001839408683629789,
      "loss": 0.3051,
      "step": 7200
    },
    {
      "epoch": 0.5175879396984925,
      "grad_norm": 6.692070960998535,
      "learning_rate": 0.000183887692430406,
      "loss": 0.3455,
      "step": 7210
    },
    {
      "epoch": 0.5183058147882269,
      "grad_norm": 11.379188537597656,
      "learning_rate": 0.0001838345164978331,
      "loss": 0.4828,
      "step": 7220
    },
    {
      "epoch": 0.5190236898779612,
      "grad_norm": 14.25040340423584,
      "learning_rate": 0.00018378134056526018,
      "loss": 0.3991,
      "step": 7230
    },
    {
      "epoch": 0.5197415649676956,
      "grad_norm": 15.598342895507812,
      "learning_rate": 0.00018372816463268726,
      "loss": 0.3284,
      "step": 7240
    },
    {
      "epoch": 0.5204594400574301,
      "grad_norm": 5.514676094055176,
      "learning_rate": 0.00018367498870011434,
      "loss": 0.3179,
      "step": 7250
    },
    {
      "epoch": 0.5211773151471644,
      "grad_norm": 7.509927749633789,
      "learning_rate": 0.00018362181276754142,
      "loss": 0.2004,
      "step": 7260
    },
    {
      "epoch": 0.5218951902368988,
      "grad_norm": 23.074337005615234,
      "learning_rate": 0.0001835686368349685,
      "loss": 0.5071,
      "step": 7270
    },
    {
      "epoch": 0.5226130653266332,
      "grad_norm": 0.17698577046394348,
      "learning_rate": 0.00018351546090239558,
      "loss": 0.3183,
      "step": 7280
    },
    {
      "epoch": 0.5233309404163675,
      "grad_norm": 11.557934761047363,
      "learning_rate": 0.00018346228496982268,
      "loss": 0.1521,
      "step": 7290
    },
    {
      "epoch": 0.5240488155061019,
      "grad_norm": 1.0473084449768066,
      "learning_rate": 0.00018340910903724976,
      "loss": 0.1187,
      "step": 7300
    },
    {
      "epoch": 0.5247666905958364,
      "grad_norm": 5.703590393066406,
      "learning_rate": 0.00018335593310467681,
      "loss": 0.3093,
      "step": 7310
    },
    {
      "epoch": 0.5254845656855707,
      "grad_norm": 4.205304145812988,
      "learning_rate": 0.00018330275717210392,
      "loss": 0.2332,
      "step": 7320
    },
    {
      "epoch": 0.5262024407753051,
      "grad_norm": 0.11484265327453613,
      "learning_rate": 0.000183249581239531,
      "loss": 0.2384,
      "step": 7330
    },
    {
      "epoch": 0.5269203158650395,
      "grad_norm": 18.661115646362305,
      "learning_rate": 0.00018319640530695808,
      "loss": 0.1277,
      "step": 7340
    },
    {
      "epoch": 0.5276381909547738,
      "grad_norm": 0.32066431641578674,
      "learning_rate": 0.00018314322937438519,
      "loss": 0.2611,
      "step": 7350
    },
    {
      "epoch": 0.5283560660445082,
      "grad_norm": 10.60971736907959,
      "learning_rate": 0.00018309005344181224,
      "loss": 0.3656,
      "step": 7360
    },
    {
      "epoch": 0.5290739411342427,
      "grad_norm": 15.253334999084473,
      "learning_rate": 0.00018303687750923932,
      "loss": 0.3565,
      "step": 7370
    },
    {
      "epoch": 0.529791816223977,
      "grad_norm": 1.7197651863098145,
      "learning_rate": 0.0001829837015766664,
      "loss": 0.1936,
      "step": 7380
    },
    {
      "epoch": 0.5305096913137114,
      "grad_norm": 9.170820236206055,
      "learning_rate": 0.0001829305256440935,
      "loss": 0.4309,
      "step": 7390
    },
    {
      "epoch": 0.5312275664034458,
      "grad_norm": 16.604373931884766,
      "learning_rate": 0.00018287734971152058,
      "loss": 0.5063,
      "step": 7400
    },
    {
      "epoch": 0.5319454414931802,
      "grad_norm": 0.08176188915967941,
      "learning_rate": 0.00018282417377894766,
      "loss": 0.1979,
      "step": 7410
    },
    {
      "epoch": 0.5326633165829145,
      "grad_norm": 1.5371818542480469,
      "learning_rate": 0.00018277099784637474,
      "loss": 0.3311,
      "step": 7420
    },
    {
      "epoch": 0.533381191672649,
      "grad_norm": 7.0558671951293945,
      "learning_rate": 0.00018271782191380182,
      "loss": 0.3402,
      "step": 7430
    },
    {
      "epoch": 0.5340990667623834,
      "grad_norm": 11.027749061584473,
      "learning_rate": 0.0001826646459812289,
      "loss": 0.3103,
      "step": 7440
    },
    {
      "epoch": 0.5348169418521177,
      "grad_norm": 9.483819961547852,
      "learning_rate": 0.000182611470048656,
      "loss": 0.2057,
      "step": 7450
    },
    {
      "epoch": 0.5355348169418521,
      "grad_norm": 14.773197174072266,
      "learning_rate": 0.00018255829411608306,
      "loss": 0.3617,
      "step": 7460
    },
    {
      "epoch": 0.5362526920315865,
      "grad_norm": 1.5820244550704956,
      "learning_rate": 0.00018250511818351014,
      "loss": 0.3039,
      "step": 7470
    },
    {
      "epoch": 0.5369705671213209,
      "grad_norm": 1.9285969734191895,
      "learning_rate": 0.00018245194225093722,
      "loss": 0.4605,
      "step": 7480
    },
    {
      "epoch": 0.5376884422110553,
      "grad_norm": 9.404413223266602,
      "learning_rate": 0.00018239876631836433,
      "loss": 0.3484,
      "step": 7490
    },
    {
      "epoch": 0.5384063173007897,
      "grad_norm": 2.5934054851531982,
      "learning_rate": 0.0001823455903857914,
      "loss": 0.1943,
      "step": 7500
    },
    {
      "epoch": 0.539124192390524,
      "grad_norm": 9.121219635009766,
      "learning_rate": 0.00018229241445321848,
      "loss": 0.3659,
      "step": 7510
    },
    {
      "epoch": 0.5398420674802584,
      "grad_norm": 3.8526062965393066,
      "learning_rate": 0.00018223923852064556,
      "loss": 0.243,
      "step": 7520
    },
    {
      "epoch": 0.5405599425699928,
      "grad_norm": 31.86806297302246,
      "learning_rate": 0.00018218606258807264,
      "loss": 0.3456,
      "step": 7530
    },
    {
      "epoch": 0.5412778176597272,
      "grad_norm": 0.2135343998670578,
      "learning_rate": 0.00018213288665549972,
      "loss": 0.4311,
      "step": 7540
    },
    {
      "epoch": 0.5419956927494616,
      "grad_norm": 2.9055802822113037,
      "learning_rate": 0.00018207971072292683,
      "loss": 0.3157,
      "step": 7550
    },
    {
      "epoch": 0.542713567839196,
      "grad_norm": 4.288837909698486,
      "learning_rate": 0.0001820265347903539,
      "loss": 0.3029,
      "step": 7560
    },
    {
      "epoch": 0.5434314429289304,
      "grad_norm": 3.124582529067993,
      "learning_rate": 0.00018197335885778096,
      "loss": 0.2368,
      "step": 7570
    },
    {
      "epoch": 0.5441493180186647,
      "grad_norm": 1.670087456703186,
      "learning_rate": 0.00018192018292520804,
      "loss": 0.2675,
      "step": 7580
    },
    {
      "epoch": 0.5448671931083992,
      "grad_norm": 4.106883525848389,
      "learning_rate": 0.00018186700699263515,
      "loss": 0.3752,
      "step": 7590
    },
    {
      "epoch": 0.5455850681981336,
      "grad_norm": 15.55656623840332,
      "learning_rate": 0.00018181383106006223,
      "loss": 0.0716,
      "step": 7600
    },
    {
      "epoch": 0.5463029432878679,
      "grad_norm": 10.070992469787598,
      "learning_rate": 0.0001817606551274893,
      "loss": 0.4364,
      "step": 7610
    },
    {
      "epoch": 0.5470208183776023,
      "grad_norm": 41.96803283691406,
      "learning_rate": 0.0001817074791949164,
      "loss": 0.2467,
      "step": 7620
    },
    {
      "epoch": 0.5477386934673367,
      "grad_norm": 22.236003875732422,
      "learning_rate": 0.00018165430326234347,
      "loss": 0.5083,
      "step": 7630
    },
    {
      "epoch": 0.548456568557071,
      "grad_norm": 11.634846687316895,
      "learning_rate": 0.00018160112732977055,
      "loss": 0.3455,
      "step": 7640
    },
    {
      "epoch": 0.5491744436468055,
      "grad_norm": 27.386991500854492,
      "learning_rate": 0.00018154795139719765,
      "loss": 0.172,
      "step": 7650
    },
    {
      "epoch": 0.5498923187365399,
      "grad_norm": 18.939231872558594,
      "learning_rate": 0.00018149477546462473,
      "loss": 0.3113,
      "step": 7660
    },
    {
      "epoch": 0.5506101938262742,
      "grad_norm": 14.583816528320312,
      "learning_rate": 0.0001814415995320518,
      "loss": 0.2336,
      "step": 7670
    },
    {
      "epoch": 0.5513280689160086,
      "grad_norm": 0.6548125743865967,
      "learning_rate": 0.00018138842359947886,
      "loss": 0.4457,
      "step": 7680
    },
    {
      "epoch": 0.552045944005743,
      "grad_norm": 8.74632740020752,
      "learning_rate": 0.00018133524766690597,
      "loss": 0.2332,
      "step": 7690
    },
    {
      "epoch": 0.5527638190954773,
      "grad_norm": 6.0458221435546875,
      "learning_rate": 0.00018128207173433305,
      "loss": 0.4842,
      "step": 7700
    },
    {
      "epoch": 0.5534816941852118,
      "grad_norm": 1.0713298320770264,
      "learning_rate": 0.00018122889580176013,
      "loss": 0.3515,
      "step": 7710
    },
    {
      "epoch": 0.5541995692749462,
      "grad_norm": 8.86851692199707,
      "learning_rate": 0.0001811757198691872,
      "loss": 0.2326,
      "step": 7720
    },
    {
      "epoch": 0.5549174443646806,
      "grad_norm": 18.453617095947266,
      "learning_rate": 0.0001811225439366143,
      "loss": 0.3455,
      "step": 7730
    },
    {
      "epoch": 0.5556353194544149,
      "grad_norm": 8.18680191040039,
      "learning_rate": 0.00018106936800404137,
      "loss": 0.4657,
      "step": 7740
    },
    {
      "epoch": 0.5563531945441493,
      "grad_norm": 8.105086326599121,
      "learning_rate": 0.00018101619207146847,
      "loss": 0.25,
      "step": 7750
    },
    {
      "epoch": 0.5570710696338838,
      "grad_norm": 13.787933349609375,
      "learning_rate": 0.00018096301613889555,
      "loss": 0.3634,
      "step": 7760
    },
    {
      "epoch": 0.5577889447236181,
      "grad_norm": 4.5936455726623535,
      "learning_rate": 0.00018090984020632263,
      "loss": 0.2315,
      "step": 7770
    },
    {
      "epoch": 0.5585068198133525,
      "grad_norm": 3.137397289276123,
      "learning_rate": 0.00018085666427374969,
      "loss": 0.225,
      "step": 7780
    },
    {
      "epoch": 0.5592246949030869,
      "grad_norm": 0.7156670093536377,
      "learning_rate": 0.0001808034883411768,
      "loss": 0.6384,
      "step": 7790
    },
    {
      "epoch": 0.5599425699928212,
      "grad_norm": 13.14498233795166,
      "learning_rate": 0.00018075031240860387,
      "loss": 0.3145,
      "step": 7800
    },
    {
      "epoch": 0.5606604450825556,
      "grad_norm": 9.264433860778809,
      "learning_rate": 0.00018069713647603095,
      "loss": 0.2142,
      "step": 7810
    },
    {
      "epoch": 0.5613783201722901,
      "grad_norm": 6.438668251037598,
      "learning_rate": 0.00018064396054345806,
      "loss": 0.5307,
      "step": 7820
    },
    {
      "epoch": 0.5620961952620244,
      "grad_norm": 1.3125633001327515,
      "learning_rate": 0.0001805907846108851,
      "loss": 0.2506,
      "step": 7830
    },
    {
      "epoch": 0.5628140703517588,
      "grad_norm": 16.184833526611328,
      "learning_rate": 0.0001805376086783122,
      "loss": 0.3108,
      "step": 7840
    },
    {
      "epoch": 0.5635319454414932,
      "grad_norm": 1.100014090538025,
      "learning_rate": 0.0001804844327457393,
      "loss": 0.4637,
      "step": 7850
    },
    {
      "epoch": 0.5642498205312275,
      "grad_norm": 4.357879161834717,
      "learning_rate": 0.00018043125681316638,
      "loss": 0.4155,
      "step": 7860
    },
    {
      "epoch": 0.5649676956209619,
      "grad_norm": 0.6675605177879333,
      "learning_rate": 0.00018037808088059346,
      "loss": 0.4099,
      "step": 7870
    },
    {
      "epoch": 0.5656855707106964,
      "grad_norm": 6.183744430541992,
      "learning_rate": 0.00018032490494802053,
      "loss": 0.4011,
      "step": 7880
    },
    {
      "epoch": 0.5664034458004307,
      "grad_norm": 11.524213790893555,
      "learning_rate": 0.00018027172901544761,
      "loss": 0.425,
      "step": 7890
    },
    {
      "epoch": 0.5671213208901651,
      "grad_norm": 10.025464057922363,
      "learning_rate": 0.0001802185530828747,
      "loss": 0.1541,
      "step": 7900
    },
    {
      "epoch": 0.5678391959798995,
      "grad_norm": 17.208826065063477,
      "learning_rate": 0.00018016537715030177,
      "loss": 0.4327,
      "step": 7910
    },
    {
      "epoch": 0.5685570710696339,
      "grad_norm": 5.269631385803223,
      "learning_rate": 0.00018011220121772888,
      "loss": 0.2951,
      "step": 7920
    },
    {
      "epoch": 0.5692749461593682,
      "grad_norm": 0.3604435920715332,
      "learning_rate": 0.00018005902528515596,
      "loss": 0.3043,
      "step": 7930
    },
    {
      "epoch": 0.5699928212491027,
      "grad_norm": 9.375688552856445,
      "learning_rate": 0.000180005849352583,
      "loss": 0.2502,
      "step": 7940
    },
    {
      "epoch": 0.5707106963388371,
      "grad_norm": 9.251527786254883,
      "learning_rate": 0.00017995267342001012,
      "loss": 0.445,
      "step": 7950
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 8.096343994140625,
      "learning_rate": 0.0001798994974874372,
      "loss": 0.4091,
      "step": 7960
    },
    {
      "epoch": 0.5721464465183058,
      "grad_norm": 8.195535659790039,
      "learning_rate": 0.00017984632155486428,
      "loss": 0.1693,
      "step": 7970
    },
    {
      "epoch": 0.5728643216080402,
      "grad_norm": 13.120677947998047,
      "learning_rate": 0.00017979314562229136,
      "loss": 0.2568,
      "step": 7980
    },
    {
      "epoch": 0.5735821966977745,
      "grad_norm": 0.5129302144050598,
      "learning_rate": 0.00017973996968971844,
      "loss": 0.134,
      "step": 7990
    },
    {
      "epoch": 0.574300071787509,
      "grad_norm": 1.3797030448913574,
      "learning_rate": 0.00017968679375714552,
      "loss": 0.2443,
      "step": 8000
    },
    {
      "epoch": 0.5750179468772434,
      "grad_norm": 19.18665313720703,
      "learning_rate": 0.0001796336178245726,
      "loss": 0.4918,
      "step": 8010
    },
    {
      "epoch": 0.5757358219669777,
      "grad_norm": 17.05031394958496,
      "learning_rate": 0.0001795804418919997,
      "loss": 0.3707,
      "step": 8020
    },
    {
      "epoch": 0.5764536970567121,
      "grad_norm": 26.0999755859375,
      "learning_rate": 0.00017952726595942678,
      "loss": 0.4534,
      "step": 8030
    },
    {
      "epoch": 0.5771715721464465,
      "grad_norm": 8.066903114318848,
      "learning_rate": 0.00017947409002685383,
      "loss": 0.4345,
      "step": 8040
    },
    {
      "epoch": 0.5778894472361809,
      "grad_norm": 3.015598773956299,
      "learning_rate": 0.00017942091409428094,
      "loss": 0.311,
      "step": 8050
    },
    {
      "epoch": 0.5786073223259153,
      "grad_norm": 0.5249974131584167,
      "learning_rate": 0.00017936773816170802,
      "loss": 0.2022,
      "step": 8060
    },
    {
      "epoch": 0.5793251974156497,
      "grad_norm": 3.914290428161621,
      "learning_rate": 0.0001793145622291351,
      "loss": 0.1583,
      "step": 8070
    },
    {
      "epoch": 0.5800430725053841,
      "grad_norm": 0.7158316969871521,
      "learning_rate": 0.0001792613862965622,
      "loss": 0.2986,
      "step": 8080
    },
    {
      "epoch": 0.5807609475951184,
      "grad_norm": 7.268983840942383,
      "learning_rate": 0.00017920821036398926,
      "loss": 0.2811,
      "step": 8090
    },
    {
      "epoch": 0.5814788226848528,
      "grad_norm": 6.891279220581055,
      "learning_rate": 0.00017915503443141634,
      "loss": 0.3467,
      "step": 8100
    },
    {
      "epoch": 0.5821966977745873,
      "grad_norm": 1.3878010511398315,
      "learning_rate": 0.00017910185849884342,
      "loss": 0.4691,
      "step": 8110
    },
    {
      "epoch": 0.5829145728643216,
      "grad_norm": 4.4973320960998535,
      "learning_rate": 0.00017904868256627052,
      "loss": 0.2404,
      "step": 8120
    },
    {
      "epoch": 0.583632447954056,
      "grad_norm": 13.150846481323242,
      "learning_rate": 0.0001789955066336976,
      "loss": 0.2201,
      "step": 8130
    },
    {
      "epoch": 0.5843503230437904,
      "grad_norm": 14.04060173034668,
      "learning_rate": 0.00017894233070112468,
      "loss": 0.3139,
      "step": 8140
    },
    {
      "epoch": 0.5850681981335247,
      "grad_norm": 0.3661705553531647,
      "learning_rate": 0.00017888915476855176,
      "loss": 0.3424,
      "step": 8150
    },
    {
      "epoch": 0.5857860732232592,
      "grad_norm": 6.090330600738525,
      "learning_rate": 0.00017883597883597884,
      "loss": 0.2535,
      "step": 8160
    },
    {
      "epoch": 0.5865039483129936,
      "grad_norm": 12.859427452087402,
      "learning_rate": 0.00017878280290340592,
      "loss": 0.2114,
      "step": 8170
    },
    {
      "epoch": 0.5872218234027279,
      "grad_norm": 2.551968812942505,
      "learning_rate": 0.00017872962697083303,
      "loss": 0.1728,
      "step": 8180
    },
    {
      "epoch": 0.5879396984924623,
      "grad_norm": 0.12885107100009918,
      "learning_rate": 0.0001786764510382601,
      "loss": 0.1497,
      "step": 8190
    },
    {
      "epoch": 0.5886575735821967,
      "grad_norm": 0.4709925353527069,
      "learning_rate": 0.00017862327510568716,
      "loss": 0.3319,
      "step": 8200
    },
    {
      "epoch": 0.589375448671931,
      "grad_norm": 10.837961196899414,
      "learning_rate": 0.00017857009917311424,
      "loss": 0.137,
      "step": 8210
    },
    {
      "epoch": 0.5900933237616655,
      "grad_norm": 5.722709655761719,
      "learning_rate": 0.00017851692324054135,
      "loss": 0.4754,
      "step": 8220
    },
    {
      "epoch": 0.5908111988513999,
      "grad_norm": 0.058360811322927475,
      "learning_rate": 0.00017846374730796843,
      "loss": 0.0975,
      "step": 8230
    },
    {
      "epoch": 0.5915290739411343,
      "grad_norm": 0.19297169148921967,
      "learning_rate": 0.0001784105713753955,
      "loss": 0.2717,
      "step": 8240
    },
    {
      "epoch": 0.5922469490308686,
      "grad_norm": 4.407927989959717,
      "learning_rate": 0.00017835739544282258,
      "loss": 0.3045,
      "step": 8250
    },
    {
      "epoch": 0.592964824120603,
      "grad_norm": 10.905023574829102,
      "learning_rate": 0.00017830421951024966,
      "loss": 0.2611,
      "step": 8260
    },
    {
      "epoch": 0.5936826992103375,
      "grad_norm": 6.120312690734863,
      "learning_rate": 0.00017825104357767674,
      "loss": 0.4377,
      "step": 8270
    },
    {
      "epoch": 0.5944005743000718,
      "grad_norm": 0.7145756483078003,
      "learning_rate": 0.00017819786764510385,
      "loss": 0.2969,
      "step": 8280
    },
    {
      "epoch": 0.5951184493898062,
      "grad_norm": 16.68655014038086,
      "learning_rate": 0.00017814469171253093,
      "loss": 0.3383,
      "step": 8290
    },
    {
      "epoch": 0.5958363244795406,
      "grad_norm": 2.368863582611084,
      "learning_rate": 0.000178091515779958,
      "loss": 0.4603,
      "step": 8300
    },
    {
      "epoch": 0.5965541995692749,
      "grad_norm": 14.671781539916992,
      "learning_rate": 0.00017803833984738506,
      "loss": 0.3097,
      "step": 8310
    },
    {
      "epoch": 0.5972720746590093,
      "grad_norm": 6.921037197113037,
      "learning_rate": 0.00017798516391481217,
      "loss": 0.3012,
      "step": 8320
    },
    {
      "epoch": 0.5979899497487438,
      "grad_norm": 11.324048042297363,
      "learning_rate": 0.00017793198798223925,
      "loss": 0.2807,
      "step": 8330
    },
    {
      "epoch": 0.5987078248384781,
      "grad_norm": 15.03763198852539,
      "learning_rate": 0.00017787881204966633,
      "loss": 0.5466,
      "step": 8340
    },
    {
      "epoch": 0.5994256999282125,
      "grad_norm": 20.074153900146484,
      "learning_rate": 0.0001778256361170934,
      "loss": 0.3262,
      "step": 8350
    },
    {
      "epoch": 0.6001435750179469,
      "grad_norm": 0.4855998456478119,
      "learning_rate": 0.00017777246018452049,
      "loss": 0.3952,
      "step": 8360
    },
    {
      "epoch": 0.6008614501076812,
      "grad_norm": 14.143453598022461,
      "learning_rate": 0.00017771928425194757,
      "loss": 0.2807,
      "step": 8370
    },
    {
      "epoch": 0.6015793251974156,
      "grad_norm": 18.772689819335938,
      "learning_rate": 0.00017766610831937467,
      "loss": 0.2736,
      "step": 8380
    },
    {
      "epoch": 0.6022972002871501,
      "grad_norm": 0.09592466801404953,
      "learning_rate": 0.00017761293238680175,
      "loss": 0.3176,
      "step": 8390
    },
    {
      "epoch": 0.6030150753768844,
      "grad_norm": 19.15901756286621,
      "learning_rate": 0.00017755975645422883,
      "loss": 0.4507,
      "step": 8400
    },
    {
      "epoch": 0.6037329504666188,
      "grad_norm": 1.8654391765594482,
      "learning_rate": 0.00017750658052165588,
      "loss": 0.3119,
      "step": 8410
    },
    {
      "epoch": 0.6044508255563532,
      "grad_norm": 8.797539710998535,
      "learning_rate": 0.000177453404589083,
      "loss": 0.1979,
      "step": 8420
    },
    {
      "epoch": 0.6051687006460876,
      "grad_norm": 16.402692794799805,
      "learning_rate": 0.00017740022865651007,
      "loss": 0.334,
      "step": 8430
    },
    {
      "epoch": 0.6058865757358219,
      "grad_norm": 14.092290878295898,
      "learning_rate": 0.00017734705272393715,
      "loss": 0.2646,
      "step": 8440
    },
    {
      "epoch": 0.6066044508255564,
      "grad_norm": 5.128342151641846,
      "learning_rate": 0.00017729387679136426,
      "loss": 0.2797,
      "step": 8450
    },
    {
      "epoch": 0.6073223259152908,
      "grad_norm": 19.49176597595215,
      "learning_rate": 0.0001772407008587913,
      "loss": 0.2832,
      "step": 8460
    },
    {
      "epoch": 0.6080402010050251,
      "grad_norm": 3.5231618881225586,
      "learning_rate": 0.0001771875249262184,
      "loss": 0.4528,
      "step": 8470
    },
    {
      "epoch": 0.6087580760947595,
      "grad_norm": 5.990913391113281,
      "learning_rate": 0.0001771343489936455,
      "loss": 0.3571,
      "step": 8480
    },
    {
      "epoch": 0.6094759511844939,
      "grad_norm": 5.7816925048828125,
      "learning_rate": 0.00017708117306107257,
      "loss": 0.5214,
      "step": 8490
    },
    {
      "epoch": 0.6101938262742282,
      "grad_norm": 3.8822970390319824,
      "learning_rate": 0.00017702799712849965,
      "loss": 0.4173,
      "step": 8500
    },
    {
      "epoch": 0.6109117013639627,
      "grad_norm": 3.531802177429199,
      "learning_rate": 0.00017697482119592673,
      "loss": 0.1232,
      "step": 8510
    },
    {
      "epoch": 0.6116295764536971,
      "grad_norm": 15.26954174041748,
      "learning_rate": 0.0001769216452633538,
      "loss": 0.3826,
      "step": 8520
    },
    {
      "epoch": 0.6123474515434314,
      "grad_norm": 0.0872708112001419,
      "learning_rate": 0.0001768684693307809,
      "loss": 0.2799,
      "step": 8530
    },
    {
      "epoch": 0.6130653266331658,
      "grad_norm": 21.2775936126709,
      "learning_rate": 0.00017681529339820797,
      "loss": 0.4618,
      "step": 8540
    },
    {
      "epoch": 0.6137832017229002,
      "grad_norm": 1.7872984409332275,
      "learning_rate": 0.00017676211746563508,
      "loss": 0.3589,
      "step": 8550
    },
    {
      "epoch": 0.6145010768126346,
      "grad_norm": 2.4370806217193604,
      "learning_rate": 0.00017670894153306216,
      "loss": 0.2573,
      "step": 8560
    },
    {
      "epoch": 0.615218951902369,
      "grad_norm": 2.8897061347961426,
      "learning_rate": 0.0001766557656004892,
      "loss": 0.1819,
      "step": 8570
    },
    {
      "epoch": 0.6159368269921034,
      "grad_norm": 20.965139389038086,
      "learning_rate": 0.00017660258966791632,
      "loss": 0.2635,
      "step": 8580
    },
    {
      "epoch": 0.6166547020818378,
      "grad_norm": 1.378088116645813,
      "learning_rate": 0.0001765494137353434,
      "loss": 0.1021,
      "step": 8590
    },
    {
      "epoch": 0.6173725771715721,
      "grad_norm": 9.429272651672363,
      "learning_rate": 0.00017649623780277048,
      "loss": 0.5774,
      "step": 8600
    },
    {
      "epoch": 0.6180904522613065,
      "grad_norm": 8.38917064666748,
      "learning_rate": 0.00017644306187019755,
      "loss": 0.1957,
      "step": 8610
    },
    {
      "epoch": 0.618808327351041,
      "grad_norm": 13.814610481262207,
      "learning_rate": 0.00017638988593762463,
      "loss": 0.5308,
      "step": 8620
    },
    {
      "epoch": 0.6195262024407753,
      "grad_norm": 0.2344159632921219,
      "learning_rate": 0.00017633671000505171,
      "loss": 0.0539,
      "step": 8630
    },
    {
      "epoch": 0.6202440775305097,
      "grad_norm": 20.101890563964844,
      "learning_rate": 0.0001762835340724788,
      "loss": 0.4559,
      "step": 8640
    },
    {
      "epoch": 0.6209619526202441,
      "grad_norm": 4.236398696899414,
      "learning_rate": 0.0001762303581399059,
      "loss": 0.4035,
      "step": 8650
    },
    {
      "epoch": 0.6216798277099784,
      "grad_norm": 0.09215864539146423,
      "learning_rate": 0.00017617718220733298,
      "loss": 0.4344,
      "step": 8660
    },
    {
      "epoch": 0.6223977027997128,
      "grad_norm": 7.352196216583252,
      "learning_rate": 0.00017612400627476003,
      "loss": 0.1986,
      "step": 8670
    },
    {
      "epoch": 0.6231155778894473,
      "grad_norm": 9.239477157592773,
      "learning_rate": 0.00017607083034218714,
      "loss": 0.4006,
      "step": 8680
    },
    {
      "epoch": 0.6238334529791816,
      "grad_norm": 13.957605361938477,
      "learning_rate": 0.00017601765440961422,
      "loss": 0.2249,
      "step": 8690
    },
    {
      "epoch": 0.624551328068916,
      "grad_norm": 3.895437717437744,
      "learning_rate": 0.0001759644784770413,
      "loss": 0.5093,
      "step": 8700
    },
    {
      "epoch": 0.6252692031586504,
      "grad_norm": 0.627009928226471,
      "learning_rate": 0.0001759113025444684,
      "loss": 0.1144,
      "step": 8710
    },
    {
      "epoch": 0.6259870782483847,
      "grad_norm": 1.301298975944519,
      "learning_rate": 0.00017585812661189546,
      "loss": 0.174,
      "step": 8720
    },
    {
      "epoch": 0.6267049533381192,
      "grad_norm": 2.6652510166168213,
      "learning_rate": 0.00017580495067932254,
      "loss": 0.1827,
      "step": 8730
    },
    {
      "epoch": 0.6274228284278536,
      "grad_norm": 16.146312713623047,
      "learning_rate": 0.00017575177474674962,
      "loss": 0.2767,
      "step": 8740
    },
    {
      "epoch": 0.628140703517588,
      "grad_norm": 15.8358736038208,
      "learning_rate": 0.00017569859881417672,
      "loss": 0.1878,
      "step": 8750
    },
    {
      "epoch": 0.6288585786073223,
      "grad_norm": 0.05401540547609329,
      "learning_rate": 0.0001756454228816038,
      "loss": 0.2466,
      "step": 8760
    },
    {
      "epoch": 0.6295764536970567,
      "grad_norm": 0.6656820178031921,
      "learning_rate": 0.00017559224694903088,
      "loss": 0.2937,
      "step": 8770
    },
    {
      "epoch": 0.6302943287867911,
      "grad_norm": 2.74638295173645,
      "learning_rate": 0.00017553907101645796,
      "loss": 0.4342,
      "step": 8780
    },
    {
      "epoch": 0.6310122038765255,
      "grad_norm": 12.01982307434082,
      "learning_rate": 0.00017548589508388504,
      "loss": 0.5275,
      "step": 8790
    },
    {
      "epoch": 0.6317300789662599,
      "grad_norm": 3.6108109951019287,
      "learning_rate": 0.00017543271915131212,
      "loss": 0.3576,
      "step": 8800
    },
    {
      "epoch": 0.6324479540559943,
      "grad_norm": 3.881333112716675,
      "learning_rate": 0.00017537954321873923,
      "loss": 0.5669,
      "step": 8810
    },
    {
      "epoch": 0.6331658291457286,
      "grad_norm": 0.2610112428665161,
      "learning_rate": 0.0001753263672861663,
      "loss": 0.2433,
      "step": 8820
    },
    {
      "epoch": 0.633883704235463,
      "grad_norm": 5.344810485839844,
      "learning_rate": 0.00017527319135359336,
      "loss": 0.2854,
      "step": 8830
    },
    {
      "epoch": 0.6346015793251975,
      "grad_norm": 2.7879090309143066,
      "learning_rate": 0.00017522001542102044,
      "loss": 0.311,
      "step": 8840
    },
    {
      "epoch": 0.6353194544149318,
      "grad_norm": 1.6339541673660278,
      "learning_rate": 0.00017516683948844754,
      "loss": 0.2121,
      "step": 8850
    },
    {
      "epoch": 0.6360373295046662,
      "grad_norm": 26.39142417907715,
      "learning_rate": 0.00017511366355587462,
      "loss": 0.3328,
      "step": 8860
    },
    {
      "epoch": 0.6367552045944006,
      "grad_norm": 8.974946975708008,
      "learning_rate": 0.0001750604876233017,
      "loss": 0.2577,
      "step": 8870
    },
    {
      "epoch": 0.6374730796841349,
      "grad_norm": 11.471726417541504,
      "learning_rate": 0.00017500731169072878,
      "loss": 0.2782,
      "step": 8880
    },
    {
      "epoch": 0.6381909547738693,
      "grad_norm": 10.824934959411621,
      "learning_rate": 0.00017495413575815586,
      "loss": 0.1885,
      "step": 8890
    },
    {
      "epoch": 0.6389088298636038,
      "grad_norm": 19.080371856689453,
      "learning_rate": 0.00017490095982558294,
      "loss": 0.2191,
      "step": 8900
    },
    {
      "epoch": 0.6396267049533381,
      "grad_norm": 11.358440399169922,
      "learning_rate": 0.00017484778389301005,
      "loss": 0.2705,
      "step": 8910
    },
    {
      "epoch": 0.6403445800430725,
      "grad_norm": 11.22589111328125,
      "learning_rate": 0.00017479460796043713,
      "loss": 0.2882,
      "step": 8920
    },
    {
      "epoch": 0.6410624551328069,
      "grad_norm": 8.899341583251953,
      "learning_rate": 0.0001747414320278642,
      "loss": 0.4708,
      "step": 8930
    },
    {
      "epoch": 0.6417803302225413,
      "grad_norm": 0.7948843240737915,
      "learning_rate": 0.00017468825609529126,
      "loss": 0.2256,
      "step": 8940
    },
    {
      "epoch": 0.6424982053122756,
      "grad_norm": 12.29107666015625,
      "learning_rate": 0.00017463508016271837,
      "loss": 0.2229,
      "step": 8950
    },
    {
      "epoch": 0.6432160804020101,
      "grad_norm": 0.3914202153682709,
      "learning_rate": 0.00017458190423014545,
      "loss": 0.2854,
      "step": 8960
    },
    {
      "epoch": 0.6439339554917445,
      "grad_norm": 0.23056747019290924,
      "learning_rate": 0.00017452872829757253,
      "loss": 0.3485,
      "step": 8970
    },
    {
      "epoch": 0.6446518305814788,
      "grad_norm": 11.435912132263184,
      "learning_rate": 0.0001744755523649996,
      "loss": 0.4086,
      "step": 8980
    },
    {
      "epoch": 0.6453697056712132,
      "grad_norm": 11.007671356201172,
      "learning_rate": 0.00017442237643242668,
      "loss": 0.4609,
      "step": 8990
    },
    {
      "epoch": 0.6460875807609476,
      "grad_norm": 21.965883255004883,
      "learning_rate": 0.00017436920049985376,
      "loss": 0.2957,
      "step": 9000
    },
    {
      "epoch": 0.6468054558506819,
      "grad_norm": 18.765323638916016,
      "learning_rate": 0.00017431602456728087,
      "loss": 0.1758,
      "step": 9010
    },
    {
      "epoch": 0.6475233309404164,
      "grad_norm": 14.802559852600098,
      "learning_rate": 0.00017426284863470795,
      "loss": 0.3224,
      "step": 9020
    },
    {
      "epoch": 0.6482412060301508,
      "grad_norm": 15.835829734802246,
      "learning_rate": 0.00017420967270213503,
      "loss": 0.3578,
      "step": 9030
    },
    {
      "epoch": 0.6489590811198851,
      "grad_norm": 13.060470581054688,
      "learning_rate": 0.00017415649676956208,
      "loss": 0.3731,
      "step": 9040
    },
    {
      "epoch": 0.6496769562096195,
      "grad_norm": 4.752431869506836,
      "learning_rate": 0.0001741033208369892,
      "loss": 0.1508,
      "step": 9050
    },
    {
      "epoch": 0.6503948312993539,
      "grad_norm": 0.06818567961454391,
      "learning_rate": 0.00017405014490441627,
      "loss": 0.2563,
      "step": 9060
    },
    {
      "epoch": 0.6511127063890882,
      "grad_norm": 14.850653648376465,
      "learning_rate": 0.00017399696897184335,
      "loss": 0.3196,
      "step": 9070
    },
    {
      "epoch": 0.6518305814788227,
      "grad_norm": 8.61403751373291,
      "learning_rate": 0.00017394379303927045,
      "loss": 0.5296,
      "step": 9080
    },
    {
      "epoch": 0.6525484565685571,
      "grad_norm": 2.675006628036499,
      "learning_rate": 0.0001738906171066975,
      "loss": 0.4305,
      "step": 9090
    },
    {
      "epoch": 0.6532663316582915,
      "grad_norm": 4.083374500274658,
      "learning_rate": 0.00017383744117412459,
      "loss": 0.1122,
      "step": 9100
    },
    {
      "epoch": 0.6539842067480258,
      "grad_norm": 5.363386154174805,
      "learning_rate": 0.0001737842652415517,
      "loss": 0.3324,
      "step": 9110
    },
    {
      "epoch": 0.6547020818377602,
      "grad_norm": 2.5468688011169434,
      "learning_rate": 0.00017373108930897877,
      "loss": 0.2472,
      "step": 9120
    },
    {
      "epoch": 0.6554199569274947,
      "grad_norm": 10.452987670898438,
      "learning_rate": 0.00017367791337640585,
      "loss": 0.2694,
      "step": 9130
    },
    {
      "epoch": 0.656137832017229,
      "grad_norm": 23.29120445251465,
      "learning_rate": 0.00017362473744383293,
      "loss": 0.5791,
      "step": 9140
    },
    {
      "epoch": 0.6568557071069634,
      "grad_norm": 12.81174087524414,
      "learning_rate": 0.00017357156151126,
      "loss": 0.7999,
      "step": 9150
    },
    {
      "epoch": 0.6575735821966978,
      "grad_norm": 2.074673891067505,
      "learning_rate": 0.0001735183855786871,
      "loss": 0.3818,
      "step": 9160
    },
    {
      "epoch": 0.6582914572864321,
      "grad_norm": 6.238882541656494,
      "learning_rate": 0.00017346520964611417,
      "loss": 0.3501,
      "step": 9170
    },
    {
      "epoch": 0.6590093323761665,
      "grad_norm": 2.1450181007385254,
      "learning_rate": 0.00017341203371354128,
      "loss": 0.2246,
      "step": 9180
    },
    {
      "epoch": 0.659727207465901,
      "grad_norm": 4.229192733764648,
      "learning_rate": 0.00017335885778096836,
      "loss": 0.5727,
      "step": 9190
    },
    {
      "epoch": 0.6604450825556353,
      "grad_norm": 1.4049838781356812,
      "learning_rate": 0.0001733056818483954,
      "loss": 0.2672,
      "step": 9200
    },
    {
      "epoch": 0.6611629576453697,
      "grad_norm": 14.79945182800293,
      "learning_rate": 0.00017325250591582251,
      "loss": 0.3411,
      "step": 9210
    },
    {
      "epoch": 0.6618808327351041,
      "grad_norm": 0.026502300053834915,
      "learning_rate": 0.0001731993299832496,
      "loss": 0.2323,
      "step": 9220
    },
    {
      "epoch": 0.6625987078248384,
      "grad_norm": 19.3157958984375,
      "learning_rate": 0.00017314615405067667,
      "loss": 0.5089,
      "step": 9230
    },
    {
      "epoch": 0.6633165829145728,
      "grad_norm": 10.986977577209473,
      "learning_rate": 0.00017309297811810375,
      "loss": 0.3236,
      "step": 9240
    },
    {
      "epoch": 0.6640344580043073,
      "grad_norm": 0.16828089952468872,
      "learning_rate": 0.00017303980218553083,
      "loss": 0.4155,
      "step": 9250
    },
    {
      "epoch": 0.6647523330940417,
      "grad_norm": 12.03813362121582,
      "learning_rate": 0.0001729866262529579,
      "loss": 0.0628,
      "step": 9260
    },
    {
      "epoch": 0.665470208183776,
      "grad_norm": 23.05914878845215,
      "learning_rate": 0.000172933450320385,
      "loss": 0.4246,
      "step": 9270
    },
    {
      "epoch": 0.6661880832735104,
      "grad_norm": 7.084920883178711,
      "learning_rate": 0.0001728802743878121,
      "loss": 0.4404,
      "step": 9280
    },
    {
      "epoch": 0.6669059583632448,
      "grad_norm": 9.730473518371582,
      "learning_rate": 0.00017282709845523918,
      "loss": 0.2492,
      "step": 9290
    },
    {
      "epoch": 0.6676238334529792,
      "grad_norm": 0.41035106778144836,
      "learning_rate": 0.00017277392252266623,
      "loss": 0.2321,
      "step": 9300
    },
    {
      "epoch": 0.6683417085427136,
      "grad_norm": 7.662316799163818,
      "learning_rate": 0.00017272074659009334,
      "loss": 0.2501,
      "step": 9310
    },
    {
      "epoch": 0.669059583632448,
      "grad_norm": 17.971271514892578,
      "learning_rate": 0.00017266757065752042,
      "loss": 0.4066,
      "step": 9320
    },
    {
      "epoch": 0.6697774587221823,
      "grad_norm": 13.041863441467285,
      "learning_rate": 0.0001726143947249475,
      "loss": 0.3341,
      "step": 9330
    },
    {
      "epoch": 0.6704953338119167,
      "grad_norm": 1.6810587644577026,
      "learning_rate": 0.0001725612187923746,
      "loss": 0.4079,
      "step": 9340
    },
    {
      "epoch": 0.6712132089016511,
      "grad_norm": 1.6203476190567017,
      "learning_rate": 0.00017250804285980165,
      "loss": 0.1534,
      "step": 9350
    },
    {
      "epoch": 0.6719310839913855,
      "grad_norm": 8.109549522399902,
      "learning_rate": 0.00017245486692722873,
      "loss": 0.5256,
      "step": 9360
    },
    {
      "epoch": 0.6726489590811199,
      "grad_norm": 4.645214557647705,
      "learning_rate": 0.0001724016909946558,
      "loss": 0.2891,
      "step": 9370
    },
    {
      "epoch": 0.6733668341708543,
      "grad_norm": 9.033782958984375,
      "learning_rate": 0.00017234851506208292,
      "loss": 0.2584,
      "step": 9380
    },
    {
      "epoch": 0.6740847092605886,
      "grad_norm": 4.921586513519287,
      "learning_rate": 0.00017229533912951,
      "loss": 0.1369,
      "step": 9390
    },
    {
      "epoch": 0.674802584350323,
      "grad_norm": 13.187591552734375,
      "learning_rate": 0.00017224216319693708,
      "loss": 0.6204,
      "step": 9400
    },
    {
      "epoch": 0.6755204594400575,
      "grad_norm": 6.703019142150879,
      "learning_rate": 0.00017218898726436416,
      "loss": 0.1844,
      "step": 9410
    },
    {
      "epoch": 0.6762383345297918,
      "grad_norm": 9.190531730651855,
      "learning_rate": 0.00017213581133179124,
      "loss": 0.4002,
      "step": 9420
    },
    {
      "epoch": 0.6769562096195262,
      "grad_norm": 2.5559751987457275,
      "learning_rate": 0.00017208263539921832,
      "loss": 0.2541,
      "step": 9430
    },
    {
      "epoch": 0.6776740847092606,
      "grad_norm": 11.00485610961914,
      "learning_rate": 0.00017202945946664542,
      "loss": 0.2789,
      "step": 9440
    },
    {
      "epoch": 0.678391959798995,
      "grad_norm": 15.722553253173828,
      "learning_rate": 0.0001719762835340725,
      "loss": 0.4058,
      "step": 9450
    },
    {
      "epoch": 0.6791098348887293,
      "grad_norm": 0.06710057705640793,
      "learning_rate": 0.00017192310760149956,
      "loss": 0.1885,
      "step": 9460
    },
    {
      "epoch": 0.6798277099784638,
      "grad_norm": 15.078625679016113,
      "learning_rate": 0.00017186993166892664,
      "loss": 0.3522,
      "step": 9470
    },
    {
      "epoch": 0.6805455850681982,
      "grad_norm": 6.6453423500061035,
      "learning_rate": 0.00017181675573635374,
      "loss": 0.5285,
      "step": 9480
    },
    {
      "epoch": 0.6812634601579325,
      "grad_norm": 0.9451792240142822,
      "learning_rate": 0.00017176357980378082,
      "loss": 0.3903,
      "step": 9490
    },
    {
      "epoch": 0.6819813352476669,
      "grad_norm": 0.6865790486335754,
      "learning_rate": 0.0001717104038712079,
      "loss": 0.2564,
      "step": 9500
    },
    {
      "epoch": 0.6826992103374013,
      "grad_norm": 4.5422539710998535,
      "learning_rate": 0.00017165722793863498,
      "loss": 0.1851,
      "step": 9510
    },
    {
      "epoch": 0.6834170854271356,
      "grad_norm": 16.847719192504883,
      "learning_rate": 0.00017160405200606206,
      "loss": 0.3203,
      "step": 9520
    },
    {
      "epoch": 0.6841349605168701,
      "grad_norm": 6.745655536651611,
      "learning_rate": 0.00017155087607348914,
      "loss": 0.1283,
      "step": 9530
    },
    {
      "epoch": 0.6848528356066045,
      "grad_norm": 14.901955604553223,
      "learning_rate": 0.00017149770014091625,
      "loss": 0.4156,
      "step": 9540
    },
    {
      "epoch": 0.6855707106963388,
      "grad_norm": 15.179346084594727,
      "learning_rate": 0.00017144452420834333,
      "loss": 0.554,
      "step": 9550
    },
    {
      "epoch": 0.6862885857860732,
      "grad_norm": 4.059324264526367,
      "learning_rate": 0.00017139134827577038,
      "loss": 0.5139,
      "step": 9560
    },
    {
      "epoch": 0.6870064608758076,
      "grad_norm": 33.413963317871094,
      "learning_rate": 0.00017133817234319746,
      "loss": 0.2614,
      "step": 9570
    },
    {
      "epoch": 0.6877243359655419,
      "grad_norm": 23.94222640991211,
      "learning_rate": 0.00017128499641062456,
      "loss": 0.2431,
      "step": 9580
    },
    {
      "epoch": 0.6884422110552764,
      "grad_norm": 0.1288120448589325,
      "learning_rate": 0.00017123182047805164,
      "loss": 0.301,
      "step": 9590
    },
    {
      "epoch": 0.6891600861450108,
      "grad_norm": 4.691328525543213,
      "learning_rate": 0.00017117864454547872,
      "loss": 0.2936,
      "step": 9600
    },
    {
      "epoch": 0.6898779612347452,
      "grad_norm": 2.1188066005706787,
      "learning_rate": 0.0001711254686129058,
      "loss": 0.4494,
      "step": 9610
    },
    {
      "epoch": 0.6905958363244795,
      "grad_norm": 0.46536195278167725,
      "learning_rate": 0.00017107229268033288,
      "loss": 0.42,
      "step": 9620
    },
    {
      "epoch": 0.6913137114142139,
      "grad_norm": 1.6674171686172485,
      "learning_rate": 0.00017101911674775996,
      "loss": 0.4924,
      "step": 9630
    },
    {
      "epoch": 0.6920315865039484,
      "grad_norm": 20.113502502441406,
      "learning_rate": 0.00017096594081518707,
      "loss": 0.4594,
      "step": 9640
    },
    {
      "epoch": 0.6927494615936827,
      "grad_norm": 0.12894511222839355,
      "learning_rate": 0.00017091276488261415,
      "loss": 0.2618,
      "step": 9650
    },
    {
      "epoch": 0.6934673366834171,
      "grad_norm": 0.839422345161438,
      "learning_rate": 0.00017085958895004123,
      "loss": 0.2651,
      "step": 9660
    },
    {
      "epoch": 0.6941852117731515,
      "grad_norm": 12.465484619140625,
      "learning_rate": 0.00017080641301746828,
      "loss": 0.3123,
      "step": 9670
    },
    {
      "epoch": 0.6949030868628858,
      "grad_norm": 0.12127242237329483,
      "learning_rate": 0.00017075323708489539,
      "loss": 0.0838,
      "step": 9680
    },
    {
      "epoch": 0.6956209619526202,
      "grad_norm": 4.888281345367432,
      "learning_rate": 0.00017070006115232247,
      "loss": 0.0755,
      "step": 9690
    },
    {
      "epoch": 0.6963388370423547,
      "grad_norm": 15.558061599731445,
      "learning_rate": 0.00017064688521974955,
      "loss": 0.5429,
      "step": 9700
    },
    {
      "epoch": 0.697056712132089,
      "grad_norm": 15.277076721191406,
      "learning_rate": 0.00017059370928717665,
      "loss": 0.427,
      "step": 9710
    },
    {
      "epoch": 0.6977745872218234,
      "grad_norm": 7.524124622344971,
      "learning_rate": 0.0001705405333546037,
      "loss": 0.346,
      "step": 9720
    },
    {
      "epoch": 0.6984924623115578,
      "grad_norm": 17.153005599975586,
      "learning_rate": 0.00017048735742203078,
      "loss": 0.2243,
      "step": 9730
    },
    {
      "epoch": 0.6992103374012921,
      "grad_norm": 2.3564720153808594,
      "learning_rate": 0.0001704341814894579,
      "loss": 0.5925,
      "step": 9740
    },
    {
      "epoch": 0.6999282124910265,
      "grad_norm": 14.127296447753906,
      "learning_rate": 0.00017038100555688497,
      "loss": 0.3387,
      "step": 9750
    },
    {
      "epoch": 0.700646087580761,
      "grad_norm": 12.44150161743164,
      "learning_rate": 0.00017032782962431205,
      "loss": 0.2613,
      "step": 9760
    },
    {
      "epoch": 0.7013639626704954,
      "grad_norm": 22.353017807006836,
      "learning_rate": 0.00017027465369173913,
      "loss": 0.2157,
      "step": 9770
    },
    {
      "epoch": 0.7020818377602297,
      "grad_norm": 12.9564790725708,
      "learning_rate": 0.0001702214777591662,
      "loss": 0.2956,
      "step": 9780
    },
    {
      "epoch": 0.7027997128499641,
      "grad_norm": 10.202808380126953,
      "learning_rate": 0.0001701683018265933,
      "loss": 0.2218,
      "step": 9790
    },
    {
      "epoch": 0.7035175879396985,
      "grad_norm": 1.9493613243103027,
      "learning_rate": 0.00017011512589402037,
      "loss": 0.2596,
      "step": 9800
    },
    {
      "epoch": 0.7042354630294329,
      "grad_norm": 1.459073543548584,
      "learning_rate": 0.00017006194996144747,
      "loss": 0.1115,
      "step": 9810
    },
    {
      "epoch": 0.7049533381191673,
      "grad_norm": 12.76464557647705,
      "learning_rate": 0.00017000877402887455,
      "loss": 0.1781,
      "step": 9820
    },
    {
      "epoch": 0.7056712132089017,
      "grad_norm": 7.408548355102539,
      "learning_rate": 0.0001699555980963016,
      "loss": 0.2085,
      "step": 9830
    },
    {
      "epoch": 0.706389088298636,
      "grad_norm": 0.5971412062644958,
      "learning_rate": 0.0001699024221637287,
      "loss": 0.0933,
      "step": 9840
    },
    {
      "epoch": 0.7071069633883704,
      "grad_norm": 5.762805461883545,
      "learning_rate": 0.0001698492462311558,
      "loss": 0.2357,
      "step": 9850
    },
    {
      "epoch": 0.7078248384781048,
      "grad_norm": 10.723072052001953,
      "learning_rate": 0.00016979607029858287,
      "loss": 0.1923,
      "step": 9860
    },
    {
      "epoch": 0.7085427135678392,
      "grad_norm": 11.057768821716309,
      "learning_rate": 0.00016974289436600995,
      "loss": 0.2975,
      "step": 9870
    },
    {
      "epoch": 0.7092605886575736,
      "grad_norm": 2.699977397918701,
      "learning_rate": 0.00016968971843343703,
      "loss": 0.1278,
      "step": 9880
    },
    {
      "epoch": 0.709978463747308,
      "grad_norm": 5.196922302246094,
      "learning_rate": 0.0001696365425008641,
      "loss": 0.131,
      "step": 9890
    },
    {
      "epoch": 0.7106963388370423,
      "grad_norm": 9.06864070892334,
      "learning_rate": 0.0001695833665682912,
      "loss": 0.4398,
      "step": 9900
    },
    {
      "epoch": 0.7114142139267767,
      "grad_norm": 0.56918865442276,
      "learning_rate": 0.0001695301906357183,
      "loss": 0.2827,
      "step": 9910
    },
    {
      "epoch": 0.7121320890165111,
      "grad_norm": 11.488222122192383,
      "learning_rate": 0.00016947701470314538,
      "loss": 0.3381,
      "step": 9920
    },
    {
      "epoch": 0.7128499641062455,
      "grad_norm": 11.085681915283203,
      "learning_rate": 0.00016942383877057243,
      "loss": 0.1728,
      "step": 9930
    },
    {
      "epoch": 0.7135678391959799,
      "grad_norm": 1.680679202079773,
      "learning_rate": 0.00016937066283799953,
      "loss": 0.2366,
      "step": 9940
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 4.357662200927734,
      "learning_rate": 0.00016931748690542661,
      "loss": 0.3023,
      "step": 9950
    },
    {
      "epoch": 0.7150035893754487,
      "grad_norm": 1.1868274211883545,
      "learning_rate": 0.0001692643109728537,
      "loss": 0.2842,
      "step": 9960
    },
    {
      "epoch": 0.715721464465183,
      "grad_norm": 0.8082106113433838,
      "learning_rate": 0.00016921113504028077,
      "loss": 0.1947,
      "step": 9970
    },
    {
      "epoch": 0.7164393395549175,
      "grad_norm": 17.863683700561523,
      "learning_rate": 0.00016915795910770785,
      "loss": 0.4781,
      "step": 9980
    },
    {
      "epoch": 0.7171572146446519,
      "grad_norm": 0.3033179044723511,
      "learning_rate": 0.00016910478317513493,
      "loss": 0.1708,
      "step": 9990
    },
    {
      "epoch": 0.7178750897343862,
      "grad_norm": 17.916526794433594,
      "learning_rate": 0.000169051607242562,
      "loss": 0.4133,
      "step": 10000
    },
    {
      "epoch": 0.7185929648241206,
      "grad_norm": 4.150047779083252,
      "learning_rate": 0.00016899843130998912,
      "loss": 0.2604,
      "step": 10010
    },
    {
      "epoch": 0.719310839913855,
      "grad_norm": 12.317670822143555,
      "learning_rate": 0.0001689452553774162,
      "loss": 0.119,
      "step": 10020
    },
    {
      "epoch": 0.7200287150035893,
      "grad_norm": 5.361465930938721,
      "learning_rate": 0.00016889207944484328,
      "loss": 0.1905,
      "step": 10030
    },
    {
      "epoch": 0.7207465900933238,
      "grad_norm": 5.370115756988525,
      "learning_rate": 0.00016883890351227036,
      "loss": 0.1183,
      "step": 10040
    },
    {
      "epoch": 0.7214644651830582,
      "grad_norm": 3.569206714630127,
      "learning_rate": 0.00016878572757969744,
      "loss": 0.258,
      "step": 10050
    },
    {
      "epoch": 0.7221823402727925,
      "grad_norm": 8.965453147888184,
      "learning_rate": 0.00016873255164712452,
      "loss": 0.2644,
      "step": 10060
    },
    {
      "epoch": 0.7229002153625269,
      "grad_norm": 15.652944564819336,
      "learning_rate": 0.00016867937571455162,
      "loss": 0.1926,
      "step": 10070
    },
    {
      "epoch": 0.7236180904522613,
      "grad_norm": 0.3885261118412018,
      "learning_rate": 0.0001686261997819787,
      "loss": 0.3527,
      "step": 10080
    },
    {
      "epoch": 0.7243359655419956,
      "grad_norm": 4.314914226531982,
      "learning_rate": 0.00016857302384940575,
      "loss": 0.2357,
      "step": 10090
    },
    {
      "epoch": 0.7250538406317301,
      "grad_norm": 7.0363569259643555,
      "learning_rate": 0.00016851984791683283,
      "loss": 0.4186,
      "step": 10100
    },
    {
      "epoch": 0.7257717157214645,
      "grad_norm": 7.2662739753723145,
      "learning_rate": 0.00016846667198425994,
      "loss": 0.3843,
      "step": 10110
    },
    {
      "epoch": 0.7264895908111989,
      "grad_norm": 0.02892650105059147,
      "learning_rate": 0.00016841349605168702,
      "loss": 0.2342,
      "step": 10120
    },
    {
      "epoch": 0.7272074659009332,
      "grad_norm": 20.868518829345703,
      "learning_rate": 0.0001683603201191141,
      "loss": 0.3611,
      "step": 10130
    },
    {
      "epoch": 0.7279253409906676,
      "grad_norm": 19.57709503173828,
      "learning_rate": 0.00016830714418654118,
      "loss": 0.2097,
      "step": 10140
    },
    {
      "epoch": 0.7286432160804021,
      "grad_norm": 20.6524715423584,
      "learning_rate": 0.00016825396825396826,
      "loss": 0.5084,
      "step": 10150
    },
    {
      "epoch": 0.7293610911701364,
      "grad_norm": 8.146682739257812,
      "learning_rate": 0.00016820079232139534,
      "loss": 0.2497,
      "step": 10160
    },
    {
      "epoch": 0.7300789662598708,
      "grad_norm": 5.258018493652344,
      "learning_rate": 0.00016814761638882244,
      "loss": 0.397,
      "step": 10170
    },
    {
      "epoch": 0.7307968413496052,
      "grad_norm": 0.6661662459373474,
      "learning_rate": 0.00016809444045624952,
      "loss": 0.2193,
      "step": 10180
    },
    {
      "epoch": 0.7315147164393395,
      "grad_norm": 0.4245293140411377,
      "learning_rate": 0.00016804126452367658,
      "loss": 0.2963,
      "step": 10190
    },
    {
      "epoch": 0.7322325915290739,
      "grad_norm": 7.470398902893066,
      "learning_rate": 0.00016798808859110366,
      "loss": 0.2497,
      "step": 10200
    },
    {
      "epoch": 0.7329504666188084,
      "grad_norm": 0.04798496887087822,
      "learning_rate": 0.00016793491265853076,
      "loss": 0.3504,
      "step": 10210
    },
    {
      "epoch": 0.7336683417085427,
      "grad_norm": 1.9325610399246216,
      "learning_rate": 0.00016788173672595784,
      "loss": 0.1088,
      "step": 10220
    },
    {
      "epoch": 0.7343862167982771,
      "grad_norm": 1.0514167547225952,
      "learning_rate": 0.00016782856079338492,
      "loss": 0.1653,
      "step": 10230
    },
    {
      "epoch": 0.7351040918880115,
      "grad_norm": 11.179141998291016,
      "learning_rate": 0.000167775384860812,
      "loss": 0.5685,
      "step": 10240
    },
    {
      "epoch": 0.7358219669777458,
      "grad_norm": 0.026947498321533203,
      "learning_rate": 0.00016772220892823908,
      "loss": 0.3112,
      "step": 10250
    },
    {
      "epoch": 0.7365398420674802,
      "grad_norm": 16.75489044189453,
      "learning_rate": 0.00016766903299566616,
      "loss": 0.2339,
      "step": 10260
    },
    {
      "epoch": 0.7372577171572147,
      "grad_norm": 0.3937341868877411,
      "learning_rate": 0.00016761585706309327,
      "loss": 0.0946,
      "step": 10270
    },
    {
      "epoch": 0.7379755922469491,
      "grad_norm": 8.581510543823242,
      "learning_rate": 0.00016756268113052035,
      "loss": 0.2093,
      "step": 10280
    },
    {
      "epoch": 0.7386934673366834,
      "grad_norm": 9.7059965133667,
      "learning_rate": 0.00016750950519794743,
      "loss": 0.169,
      "step": 10290
    },
    {
      "epoch": 0.7394113424264178,
      "grad_norm": 23.468055725097656,
      "learning_rate": 0.00016745632926537448,
      "loss": 0.4583,
      "step": 10300
    },
    {
      "epoch": 0.7401292175161522,
      "grad_norm": 4.011721611022949,
      "learning_rate": 0.00016740315333280158,
      "loss": 0.2465,
      "step": 10310
    },
    {
      "epoch": 0.7408470926058865,
      "grad_norm": 17.61904525756836,
      "learning_rate": 0.00016734997740022866,
      "loss": 0.2794,
      "step": 10320
    },
    {
      "epoch": 0.741564967695621,
      "grad_norm": 20.052608489990234,
      "learning_rate": 0.00016729680146765574,
      "loss": 0.2064,
      "step": 10330
    },
    {
      "epoch": 0.7422828427853554,
      "grad_norm": 25.37265396118164,
      "learning_rate": 0.00016724362553508285,
      "loss": 0.5499,
      "step": 10340
    },
    {
      "epoch": 0.7430007178750897,
      "grad_norm": 25.916501998901367,
      "learning_rate": 0.0001671904496025099,
      "loss": 0.4715,
      "step": 10350
    },
    {
      "epoch": 0.7437185929648241,
      "grad_norm": 22.182435989379883,
      "learning_rate": 0.00016713727366993698,
      "loss": 0.465,
      "step": 10360
    },
    {
      "epoch": 0.7444364680545585,
      "grad_norm": 0.13431422412395477,
      "learning_rate": 0.0001670840977373641,
      "loss": 0.0782,
      "step": 10370
    },
    {
      "epoch": 0.7451543431442929,
      "grad_norm": 13.701704978942871,
      "learning_rate": 0.00016703092180479117,
      "loss": 0.4703,
      "step": 10380
    },
    {
      "epoch": 0.7458722182340273,
      "grad_norm": 0.8262835144996643,
      "learning_rate": 0.00016697774587221825,
      "loss": 0.5642,
      "step": 10390
    },
    {
      "epoch": 0.7465900933237617,
      "grad_norm": 13.975122451782227,
      "learning_rate": 0.00016692456993964533,
      "loss": 0.2477,
      "step": 10400
    },
    {
      "epoch": 0.747307968413496,
      "grad_norm": 1.3679217100143433,
      "learning_rate": 0.0001668713940070724,
      "loss": 0.2274,
      "step": 10410
    },
    {
      "epoch": 0.7480258435032304,
      "grad_norm": 9.82023811340332,
      "learning_rate": 0.00016681821807449949,
      "loss": 0.2084,
      "step": 10420
    },
    {
      "epoch": 0.7487437185929648,
      "grad_norm": 11.815079689025879,
      "learning_rate": 0.00016676504214192657,
      "loss": 0.2365,
      "step": 10430
    },
    {
      "epoch": 0.7494615936826992,
      "grad_norm": 6.432394027709961,
      "learning_rate": 0.00016671186620935367,
      "loss": 0.185,
      "step": 10440
    },
    {
      "epoch": 0.7501794687724336,
      "grad_norm": 1.448486566543579,
      "learning_rate": 0.00016665869027678072,
      "loss": 0.192,
      "step": 10450
    },
    {
      "epoch": 0.750897343862168,
      "grad_norm": 2.3427894115448,
      "learning_rate": 0.0001666055143442078,
      "loss": 0.2742,
      "step": 10460
    },
    {
      "epoch": 0.7516152189519024,
      "grad_norm": 3.1842308044433594,
      "learning_rate": 0.0001665523384116349,
      "loss": 0.2815,
      "step": 10470
    },
    {
      "epoch": 0.7523330940416367,
      "grad_norm": 7.577178478240967,
      "learning_rate": 0.000166499162479062,
      "loss": 0.0523,
      "step": 10480
    },
    {
      "epoch": 0.7530509691313712,
      "grad_norm": 13.25500774383545,
      "learning_rate": 0.00016644598654648907,
      "loss": 0.2026,
      "step": 10490
    },
    {
      "epoch": 0.7537688442211056,
      "grad_norm": 10.54909896850586,
      "learning_rate": 0.00016639281061391615,
      "loss": 0.4853,
      "step": 10500
    },
    {
      "epoch": 0.7544867193108399,
      "grad_norm": 1.1639049053192139,
      "learning_rate": 0.00016633963468134323,
      "loss": 0.3975,
      "step": 10510
    },
    {
      "epoch": 0.7552045944005743,
      "grad_norm": 1.181279182434082,
      "learning_rate": 0.0001662864587487703,
      "loss": 0.2391,
      "step": 10520
    },
    {
      "epoch": 0.7559224694903087,
      "grad_norm": 1.8679962158203125,
      "learning_rate": 0.0001662332828161974,
      "loss": 0.4084,
      "step": 10530
    },
    {
      "epoch": 0.756640344580043,
      "grad_norm": 22.061614990234375,
      "learning_rate": 0.0001661801068836245,
      "loss": 0.3736,
      "step": 10540
    },
    {
      "epoch": 0.7573582196697775,
      "grad_norm": 0.05299094319343567,
      "learning_rate": 0.00016612693095105157,
      "loss": 0.28,
      "step": 10550
    },
    {
      "epoch": 0.7580760947595119,
      "grad_norm": 19.594947814941406,
      "learning_rate": 0.00016607375501847863,
      "loss": 0.4271,
      "step": 10560
    },
    {
      "epoch": 0.7587939698492462,
      "grad_norm": 6.35526704788208,
      "learning_rate": 0.00016602057908590573,
      "loss": 0.2011,
      "step": 10570
    },
    {
      "epoch": 0.7595118449389806,
      "grad_norm": 3.8330681324005127,
      "learning_rate": 0.0001659674031533328,
      "loss": 0.1771,
      "step": 10580
    },
    {
      "epoch": 0.760229720028715,
      "grad_norm": 21.09161949157715,
      "learning_rate": 0.0001659142272207599,
      "loss": 0.2717,
      "step": 10590
    },
    {
      "epoch": 0.7609475951184493,
      "grad_norm": 4.571539402008057,
      "learning_rate": 0.00016586105128818697,
      "loss": 0.4205,
      "step": 10600
    },
    {
      "epoch": 0.7616654702081838,
      "grad_norm": 11.47598648071289,
      "learning_rate": 0.00016580787535561405,
      "loss": 0.2247,
      "step": 10610
    },
    {
      "epoch": 0.7623833452979182,
      "grad_norm": 18.29061508178711,
      "learning_rate": 0.00016575469942304113,
      "loss": 0.3489,
      "step": 10620
    },
    {
      "epoch": 0.7631012203876526,
      "grad_norm": 13.345130920410156,
      "learning_rate": 0.0001657015234904682,
      "loss": 0.3142,
      "step": 10630
    },
    {
      "epoch": 0.7638190954773869,
      "grad_norm": 15.90650749206543,
      "learning_rate": 0.00016564834755789532,
      "loss": 0.4686,
      "step": 10640
    },
    {
      "epoch": 0.7645369705671213,
      "grad_norm": 1.2461796998977661,
      "learning_rate": 0.0001655951716253224,
      "loss": 0.2874,
      "step": 10650
    },
    {
      "epoch": 0.7652548456568558,
      "grad_norm": 15.703997611999512,
      "learning_rate": 0.00016554199569274948,
      "loss": 0.2398,
      "step": 10660
    },
    {
      "epoch": 0.7659727207465901,
      "grad_norm": 0.14044752717018127,
      "learning_rate": 0.00016548881976017655,
      "loss": 0.1602,
      "step": 10670
    },
    {
      "epoch": 0.7666905958363245,
      "grad_norm": 6.092559337615967,
      "learning_rate": 0.00016543564382760363,
      "loss": 0.3304,
      "step": 10680
    },
    {
      "epoch": 0.7674084709260589,
      "grad_norm": 10.849255561828613,
      "learning_rate": 0.00016538246789503071,
      "loss": 0.2,
      "step": 10690
    },
    {
      "epoch": 0.7681263460157932,
      "grad_norm": 1.5749748945236206,
      "learning_rate": 0.0001653292919624578,
      "loss": 0.2471,
      "step": 10700
    },
    {
      "epoch": 0.7688442211055276,
      "grad_norm": 2.8925998210906982,
      "learning_rate": 0.0001652761160298849,
      "loss": 0.1599,
      "step": 10710
    },
    {
      "epoch": 0.7695620961952621,
      "grad_norm": 18.72650718688965,
      "learning_rate": 0.00016522294009731195,
      "loss": 0.4801,
      "step": 10720
    },
    {
      "epoch": 0.7702799712849964,
      "grad_norm": 18.013147354125977,
      "learning_rate": 0.00016516976416473903,
      "loss": 0.384,
      "step": 10730
    },
    {
      "epoch": 0.7709978463747308,
      "grad_norm": 7.621912002563477,
      "learning_rate": 0.00016511658823216614,
      "loss": 0.3685,
      "step": 10740
    },
    {
      "epoch": 0.7717157214644652,
      "grad_norm": 8.621289253234863,
      "learning_rate": 0.00016506341229959322,
      "loss": 0.3937,
      "step": 10750
    },
    {
      "epoch": 0.7724335965541995,
      "grad_norm": 0.0820804089307785,
      "learning_rate": 0.0001650102363670203,
      "loss": 0.2307,
      "step": 10760
    },
    {
      "epoch": 0.7731514716439339,
      "grad_norm": 8.137399673461914,
      "learning_rate": 0.00016495706043444738,
      "loss": 0.2685,
      "step": 10770
    },
    {
      "epoch": 0.7738693467336684,
      "grad_norm": 1.9682326316833496,
      "learning_rate": 0.00016490388450187446,
      "loss": 0.2809,
      "step": 10780
    },
    {
      "epoch": 0.7745872218234028,
      "grad_norm": 16.352052688598633,
      "learning_rate": 0.00016485070856930154,
      "loss": 0.1958,
      "step": 10790
    },
    {
      "epoch": 0.7753050969131371,
      "grad_norm": 0.24092881381511688,
      "learning_rate": 0.00016479753263672864,
      "loss": 0.3523,
      "step": 10800
    },
    {
      "epoch": 0.7760229720028715,
      "grad_norm": 1.2339781522750854,
      "learning_rate": 0.00016474435670415572,
      "loss": 0.2407,
      "step": 10810
    },
    {
      "epoch": 0.7767408470926059,
      "grad_norm": 3.2274460792541504,
      "learning_rate": 0.00016469118077158277,
      "loss": 0.1793,
      "step": 10820
    },
    {
      "epoch": 0.7774587221823402,
      "grad_norm": 7.367012977600098,
      "learning_rate": 0.00016463800483900985,
      "loss": 0.2584,
      "step": 10830
    },
    {
      "epoch": 0.7781765972720747,
      "grad_norm": 2.2549569606781006,
      "learning_rate": 0.00016458482890643696,
      "loss": 0.092,
      "step": 10840
    },
    {
      "epoch": 0.7788944723618091,
      "grad_norm": 0.25810474157333374,
      "learning_rate": 0.00016453165297386404,
      "loss": 0.3445,
      "step": 10850
    },
    {
      "epoch": 0.7796123474515434,
      "grad_norm": 0.5634281039237976,
      "learning_rate": 0.00016447847704129112,
      "loss": 0.4593,
      "step": 10860
    },
    {
      "epoch": 0.7803302225412778,
      "grad_norm": 6.859597206115723,
      "learning_rate": 0.0001644253011087182,
      "loss": 0.1634,
      "step": 10870
    },
    {
      "epoch": 0.7810480976310122,
      "grad_norm": 3.12161922454834,
      "learning_rate": 0.00016437212517614528,
      "loss": 0.3308,
      "step": 10880
    },
    {
      "epoch": 0.7817659727207465,
      "grad_norm": 0.024017170071601868,
      "learning_rate": 0.00016431894924357236,
      "loss": 0.1617,
      "step": 10890
    },
    {
      "epoch": 0.782483847810481,
      "grad_norm": 0.15585844218730927,
      "learning_rate": 0.00016426577331099946,
      "loss": 0.1804,
      "step": 10900
    },
    {
      "epoch": 0.7832017229002154,
      "grad_norm": 14.199904441833496,
      "learning_rate": 0.00016421259737842654,
      "loss": 0.2509,
      "step": 10910
    },
    {
      "epoch": 0.7839195979899497,
      "grad_norm": 5.502984046936035,
      "learning_rate": 0.00016415942144585362,
      "loss": 0.2737,
      "step": 10920
    },
    {
      "epoch": 0.7846374730796841,
      "grad_norm": 0.698007345199585,
      "learning_rate": 0.00016410624551328068,
      "loss": 0.1363,
      "step": 10930
    },
    {
      "epoch": 0.7853553481694185,
      "grad_norm": 0.6868225932121277,
      "learning_rate": 0.00016405306958070778,
      "loss": 0.0882,
      "step": 10940
    },
    {
      "epoch": 0.7860732232591529,
      "grad_norm": 8.08154582977295,
      "learning_rate": 0.00016399989364813486,
      "loss": 0.3848,
      "step": 10950
    },
    {
      "epoch": 0.7867910983488873,
      "grad_norm": 2.802954912185669,
      "learning_rate": 0.00016394671771556194,
      "loss": 0.1974,
      "step": 10960
    },
    {
      "epoch": 0.7875089734386217,
      "grad_norm": 8.592987060546875,
      "learning_rate": 0.00016389354178298905,
      "loss": 0.2793,
      "step": 10970
    },
    {
      "epoch": 0.7882268485283561,
      "grad_norm": 13.749303817749023,
      "learning_rate": 0.0001638403658504161,
      "loss": 0.2487,
      "step": 10980
    },
    {
      "epoch": 0.7889447236180904,
      "grad_norm": 10.063469886779785,
      "learning_rate": 0.00016378718991784318,
      "loss": 0.3332,
      "step": 10990
    },
    {
      "epoch": 0.7896625987078248,
      "grad_norm": 14.109420776367188,
      "learning_rate": 0.00016373401398527029,
      "loss": 0.299,
      "step": 11000
    },
    {
      "epoch": 0.7903804737975593,
      "grad_norm": 3.1694798469543457,
      "learning_rate": 0.00016368083805269737,
      "loss": 0.1805,
      "step": 11010
    },
    {
      "epoch": 0.7910983488872936,
      "grad_norm": 7.970658302307129,
      "learning_rate": 0.00016362766212012445,
      "loss": 0.1234,
      "step": 11020
    },
    {
      "epoch": 0.791816223977028,
      "grad_norm": 4.790167808532715,
      "learning_rate": 0.00016357448618755152,
      "loss": 0.0817,
      "step": 11030
    },
    {
      "epoch": 0.7925340990667624,
      "grad_norm": 0.18514958024024963,
      "learning_rate": 0.0001635213102549786,
      "loss": 0.2973,
      "step": 11040
    },
    {
      "epoch": 0.7932519741564967,
      "grad_norm": 17.626632690429688,
      "learning_rate": 0.00016346813432240568,
      "loss": 0.3665,
      "step": 11050
    },
    {
      "epoch": 0.7939698492462312,
      "grad_norm": 11.218132019042969,
      "learning_rate": 0.00016341495838983276,
      "loss": 0.515,
      "step": 11060
    },
    {
      "epoch": 0.7946877243359656,
      "grad_norm": 0.2175390124320984,
      "learning_rate": 0.00016336178245725987,
      "loss": 0.1883,
      "step": 11070
    },
    {
      "epoch": 0.7954055994256999,
      "grad_norm": 1.2895249128341675,
      "learning_rate": 0.00016330860652468692,
      "loss": 0.2212,
      "step": 11080
    },
    {
      "epoch": 0.7961234745154343,
      "grad_norm": 14.033848762512207,
      "learning_rate": 0.000163255430592114,
      "loss": 0.5284,
      "step": 11090
    },
    {
      "epoch": 0.7968413496051687,
      "grad_norm": 0.23150186240673065,
      "learning_rate": 0.0001632022546595411,
      "loss": 0.1669,
      "step": 11100
    },
    {
      "epoch": 0.797559224694903,
      "grad_norm": 0.5888727307319641,
      "learning_rate": 0.0001631490787269682,
      "loss": 0.1973,
      "step": 11110
    },
    {
      "epoch": 0.7982770997846375,
      "grad_norm": 0.8723751902580261,
      "learning_rate": 0.00016309590279439527,
      "loss": 0.4633,
      "step": 11120
    },
    {
      "epoch": 0.7989949748743719,
      "grad_norm": 0.18442149460315704,
      "learning_rate": 0.00016304272686182235,
      "loss": 0.1735,
      "step": 11130
    },
    {
      "epoch": 0.7997128499641063,
      "grad_norm": 0.45273593068122864,
      "learning_rate": 0.00016298955092924943,
      "loss": 0.281,
      "step": 11140
    },
    {
      "epoch": 0.8004307250538406,
      "grad_norm": 25.96979331970215,
      "learning_rate": 0.0001629363749966765,
      "loss": 0.4318,
      "step": 11150
    },
    {
      "epoch": 0.801148600143575,
      "grad_norm": 0.026830662041902542,
      "learning_rate": 0.00016288319906410359,
      "loss": 0.2248,
      "step": 11160
    },
    {
      "epoch": 0.8018664752333095,
      "grad_norm": 6.611185550689697,
      "learning_rate": 0.0001628300231315307,
      "loss": 0.1253,
      "step": 11170
    },
    {
      "epoch": 0.8025843503230438,
      "grad_norm": 0.5214711427688599,
      "learning_rate": 0.00016277684719895777,
      "loss": 0.2632,
      "step": 11180
    },
    {
      "epoch": 0.8033022254127782,
      "grad_norm": 0.024716263636946678,
      "learning_rate": 0.00016272367126638482,
      "loss": 0.3292,
      "step": 11190
    },
    {
      "epoch": 0.8040201005025126,
      "grad_norm": 2.2459542751312256,
      "learning_rate": 0.00016267049533381193,
      "loss": 0.0614,
      "step": 11200
    },
    {
      "epoch": 0.8047379755922469,
      "grad_norm": 14.77238941192627,
      "learning_rate": 0.000162617319401239,
      "loss": 0.3564,
      "step": 11210
    },
    {
      "epoch": 0.8054558506819813,
      "grad_norm": 1.7949235439300537,
      "learning_rate": 0.0001625641434686661,
      "loss": 0.104,
      "step": 11220
    },
    {
      "epoch": 0.8061737257717158,
      "grad_norm": 3.6622047424316406,
      "learning_rate": 0.00016251096753609317,
      "loss": 0.2851,
      "step": 11230
    },
    {
      "epoch": 0.8068916008614501,
      "grad_norm": 0.7249214053153992,
      "learning_rate": 0.00016245779160352025,
      "loss": 0.2418,
      "step": 11240
    },
    {
      "epoch": 0.8076094759511845,
      "grad_norm": 3.0705268383026123,
      "learning_rate": 0.00016240461567094733,
      "loss": 0.2784,
      "step": 11250
    },
    {
      "epoch": 0.8083273510409189,
      "grad_norm": 5.124979019165039,
      "learning_rate": 0.0001623514397383744,
      "loss": 0.2431,
      "step": 11260
    },
    {
      "epoch": 0.8090452261306532,
      "grad_norm": 0.05743919685482979,
      "learning_rate": 0.00016229826380580151,
      "loss": 0.2773,
      "step": 11270
    },
    {
      "epoch": 0.8097631012203876,
      "grad_norm": 1.2258236408233643,
      "learning_rate": 0.0001622450878732286,
      "loss": 0.4774,
      "step": 11280
    },
    {
      "epoch": 0.8104809763101221,
      "grad_norm": 9.755674362182617,
      "learning_rate": 0.00016219191194065567,
      "loss": 0.4152,
      "step": 11290
    },
    {
      "epoch": 0.8111988513998565,
      "grad_norm": 10.119333267211914,
      "learning_rate": 0.00016213873600808275,
      "loss": 0.1549,
      "step": 11300
    },
    {
      "epoch": 0.8119167264895908,
      "grad_norm": 7.014317512512207,
      "learning_rate": 0.00016208556007550983,
      "loss": 0.4031,
      "step": 11310
    },
    {
      "epoch": 0.8126346015793252,
      "grad_norm": 0.22776834666728973,
      "learning_rate": 0.0001620323841429369,
      "loss": 0.2484,
      "step": 11320
    },
    {
      "epoch": 0.8133524766690596,
      "grad_norm": 9.50921630859375,
      "learning_rate": 0.000161979208210364,
      "loss": 0.5116,
      "step": 11330
    },
    {
      "epoch": 0.8140703517587939,
      "grad_norm": 2.803264856338501,
      "learning_rate": 0.0001619260322777911,
      "loss": 0.1218,
      "step": 11340
    },
    {
      "epoch": 0.8147882268485284,
      "grad_norm": 27.730182647705078,
      "learning_rate": 0.00016187285634521815,
      "loss": 0.2423,
      "step": 11350
    },
    {
      "epoch": 0.8155061019382628,
      "grad_norm": 13.712970733642578,
      "learning_rate": 0.00016181968041264523,
      "loss": 0.4112,
      "step": 11360
    },
    {
      "epoch": 0.8162239770279971,
      "grad_norm": 6.918055534362793,
      "learning_rate": 0.00016176650448007234,
      "loss": 0.6016,
      "step": 11370
    },
    {
      "epoch": 0.8169418521177315,
      "grad_norm": 1.4553163051605225,
      "learning_rate": 0.00016171332854749942,
      "loss": 0.2111,
      "step": 11380
    },
    {
      "epoch": 0.8176597272074659,
      "grad_norm": 6.617661952972412,
      "learning_rate": 0.0001616601526149265,
      "loss": 0.1172,
      "step": 11390
    },
    {
      "epoch": 0.8183776022972002,
      "grad_norm": 2.2786550521850586,
      "learning_rate": 0.00016160697668235357,
      "loss": 0.2736,
      "step": 11400
    },
    {
      "epoch": 0.8190954773869347,
      "grad_norm": 10.946818351745605,
      "learning_rate": 0.00016155380074978065,
      "loss": 0.2317,
      "step": 11410
    },
    {
      "epoch": 0.8198133524766691,
      "grad_norm": 10.043144226074219,
      "learning_rate": 0.00016150062481720773,
      "loss": 0.1186,
      "step": 11420
    },
    {
      "epoch": 0.8205312275664034,
      "grad_norm": 1.5419844388961792,
      "learning_rate": 0.0001614474488846348,
      "loss": 0.1433,
      "step": 11430
    },
    {
      "epoch": 0.8212491026561378,
      "grad_norm": 9.066306114196777,
      "learning_rate": 0.00016139427295206192,
      "loss": 0.2285,
      "step": 11440
    },
    {
      "epoch": 0.8219669777458722,
      "grad_norm": 11.722107887268066,
      "learning_rate": 0.00016134109701948897,
      "loss": 0.2838,
      "step": 11450
    },
    {
      "epoch": 0.8226848528356066,
      "grad_norm": 0.301734983921051,
      "learning_rate": 0.00016128792108691605,
      "loss": 0.0415,
      "step": 11460
    },
    {
      "epoch": 0.823402727925341,
      "grad_norm": 10.248827934265137,
      "learning_rate": 0.00016123474515434316,
      "loss": 0.2617,
      "step": 11470
    },
    {
      "epoch": 0.8241206030150754,
      "grad_norm": 26.560466766357422,
      "learning_rate": 0.00016118156922177024,
      "loss": 0.2314,
      "step": 11480
    },
    {
      "epoch": 0.8248384781048098,
      "grad_norm": 0.10874493420124054,
      "learning_rate": 0.00016112839328919732,
      "loss": 0.2612,
      "step": 11490
    },
    {
      "epoch": 0.8255563531945441,
      "grad_norm": 3.242734909057617,
      "learning_rate": 0.0001610752173566244,
      "loss": 0.2034,
      "step": 11500
    },
    {
      "epoch": 0.8262742282842785,
      "grad_norm": 6.7151923179626465,
      "learning_rate": 0.00016102204142405148,
      "loss": 0.2265,
      "step": 11510
    },
    {
      "epoch": 0.826992103374013,
      "grad_norm": 0.18686506152153015,
      "learning_rate": 0.00016096886549147856,
      "loss": 0.2008,
      "step": 11520
    },
    {
      "epoch": 0.8277099784637473,
      "grad_norm": 0.10114151239395142,
      "learning_rate": 0.00016091568955890566,
      "loss": 0.2208,
      "step": 11530
    },
    {
      "epoch": 0.8284278535534817,
      "grad_norm": 1.9978865385055542,
      "learning_rate": 0.00016086251362633274,
      "loss": 0.2181,
      "step": 11540
    },
    {
      "epoch": 0.8291457286432161,
      "grad_norm": 13.975104331970215,
      "learning_rate": 0.00016080933769375982,
      "loss": 0.1103,
      "step": 11550
    },
    {
      "epoch": 0.8298636037329504,
      "grad_norm": 0.46009257435798645,
      "learning_rate": 0.00016075616176118687,
      "loss": 0.1055,
      "step": 11560
    },
    {
      "epoch": 0.8305814788226848,
      "grad_norm": 5.5142741203308105,
      "learning_rate": 0.00016070298582861398,
      "loss": 0.3242,
      "step": 11570
    },
    {
      "epoch": 0.8312993539124193,
      "grad_norm": 0.4117339253425598,
      "learning_rate": 0.00016064980989604106,
      "loss": 0.5249,
      "step": 11580
    },
    {
      "epoch": 0.8320172290021536,
      "grad_norm": 22.825572967529297,
      "learning_rate": 0.00016059663396346814,
      "loss": 0.2499,
      "step": 11590
    },
    {
      "epoch": 0.832735104091888,
      "grad_norm": 6.210501670837402,
      "learning_rate": 0.00016054345803089525,
      "loss": 0.1709,
      "step": 11600
    },
    {
      "epoch": 0.8334529791816224,
      "grad_norm": 2.4839158058166504,
      "learning_rate": 0.0001604902820983223,
      "loss": 0.2107,
      "step": 11610
    },
    {
      "epoch": 0.8341708542713567,
      "grad_norm": 3.1770131587982178,
      "learning_rate": 0.00016043710616574938,
      "loss": 0.156,
      "step": 11620
    },
    {
      "epoch": 0.8348887293610912,
      "grad_norm": 0.0721876323223114,
      "learning_rate": 0.00016038393023317648,
      "loss": 0.2147,
      "step": 11630
    },
    {
      "epoch": 0.8356066044508256,
      "grad_norm": 19.443510055541992,
      "learning_rate": 0.00016033075430060356,
      "loss": 0.2107,
      "step": 11640
    },
    {
      "epoch": 0.83632447954056,
      "grad_norm": 0.7423526644706726,
      "learning_rate": 0.00016027757836803064,
      "loss": 0.3403,
      "step": 11650
    },
    {
      "epoch": 0.8370423546302943,
      "grad_norm": 0.6757634282112122,
      "learning_rate": 0.00016022440243545772,
      "loss": 0.1844,
      "step": 11660
    },
    {
      "epoch": 0.8377602297200287,
      "grad_norm": 18.081626892089844,
      "learning_rate": 0.0001601712265028848,
      "loss": 0.2105,
      "step": 11670
    },
    {
      "epoch": 0.8384781048097631,
      "grad_norm": 4.770555019378662,
      "learning_rate": 0.00016011805057031188,
      "loss": 0.1991,
      "step": 11680
    },
    {
      "epoch": 0.8391959798994975,
      "grad_norm": 0.5983889698982239,
      "learning_rate": 0.00016006487463773896,
      "loss": 0.3871,
      "step": 11690
    },
    {
      "epoch": 0.8399138549892319,
      "grad_norm": 16.959489822387695,
      "learning_rate": 0.00016001169870516607,
      "loss": 0.194,
      "step": 11700
    },
    {
      "epoch": 0.8406317300789663,
      "grad_norm": 0.1347326636314392,
      "learning_rate": 0.00015995852277259312,
      "loss": 0.3286,
      "step": 11710
    },
    {
      "epoch": 0.8413496051687006,
      "grad_norm": 24.327144622802734,
      "learning_rate": 0.0001599053468400202,
      "loss": 0.4358,
      "step": 11720
    },
    {
      "epoch": 0.842067480258435,
      "grad_norm": 5.822369575500488,
      "learning_rate": 0.0001598521709074473,
      "loss": 0.2334,
      "step": 11730
    },
    {
      "epoch": 0.8427853553481695,
      "grad_norm": 10.726471900939941,
      "learning_rate": 0.00015979899497487439,
      "loss": 0.2653,
      "step": 11740
    },
    {
      "epoch": 0.8435032304379038,
      "grad_norm": 4.534488677978516,
      "learning_rate": 0.00015974581904230147,
      "loss": 0.3509,
      "step": 11750
    },
    {
      "epoch": 0.8442211055276382,
      "grad_norm": 1.4983329772949219,
      "learning_rate": 0.00015969264310972855,
      "loss": 0.2405,
      "step": 11760
    },
    {
      "epoch": 0.8449389806173726,
      "grad_norm": 14.283467292785645,
      "learning_rate": 0.00015963946717715562,
      "loss": 0.1389,
      "step": 11770
    },
    {
      "epoch": 0.8456568557071069,
      "grad_norm": 0.038082435727119446,
      "learning_rate": 0.0001595862912445827,
      "loss": 0.1098,
      "step": 11780
    },
    {
      "epoch": 0.8463747307968413,
      "grad_norm": 0.04640957713127136,
      "learning_rate": 0.00015953311531200978,
      "loss": 0.2841,
      "step": 11790
    },
    {
      "epoch": 0.8470926058865758,
      "grad_norm": 6.483451843261719,
      "learning_rate": 0.0001594799393794369,
      "loss": 0.2566,
      "step": 11800
    },
    {
      "epoch": 0.8478104809763102,
      "grad_norm": 8.3869047164917,
      "learning_rate": 0.00015942676344686397,
      "loss": 0.2818,
      "step": 11810
    },
    {
      "epoch": 0.8485283560660445,
      "grad_norm": 0.14692984521389008,
      "learning_rate": 0.00015937358751429102,
      "loss": 0.2687,
      "step": 11820
    },
    {
      "epoch": 0.8492462311557789,
      "grad_norm": 0.126887708902359,
      "learning_rate": 0.00015932041158171813,
      "loss": 0.212,
      "step": 11830
    },
    {
      "epoch": 0.8499641062455133,
      "grad_norm": 6.900025367736816,
      "learning_rate": 0.0001592672356491452,
      "loss": 0.3236,
      "step": 11840
    },
    {
      "epoch": 0.8506819813352476,
      "grad_norm": 9.059415817260742,
      "learning_rate": 0.0001592140597165723,
      "loss": 0.2338,
      "step": 11850
    },
    {
      "epoch": 0.8513998564249821,
      "grad_norm": 4.460964202880859,
      "learning_rate": 0.00015916088378399937,
      "loss": 0.1276,
      "step": 11860
    },
    {
      "epoch": 0.8521177315147165,
      "grad_norm": 0.56159907579422,
      "learning_rate": 0.00015910770785142645,
      "loss": 0.1438,
      "step": 11870
    },
    {
      "epoch": 0.8528356066044508,
      "grad_norm": 2.1708147525787354,
      "learning_rate": 0.00015905453191885353,
      "loss": 0.207,
      "step": 11880
    },
    {
      "epoch": 0.8535534816941852,
      "grad_norm": 3.922240972518921,
      "learning_rate": 0.0001590013559862806,
      "loss": 0.2588,
      "step": 11890
    },
    {
      "epoch": 0.8542713567839196,
      "grad_norm": 4.001335620880127,
      "learning_rate": 0.0001589481800537077,
      "loss": 0.2344,
      "step": 11900
    },
    {
      "epoch": 0.8549892318736539,
      "grad_norm": 3.9213919639587402,
      "learning_rate": 0.0001588950041211348,
      "loss": 0.1643,
      "step": 11910
    },
    {
      "epoch": 0.8557071069633884,
      "grad_norm": 16.65751075744629,
      "learning_rate": 0.00015884182818856187,
      "loss": 0.5648,
      "step": 11920
    },
    {
      "epoch": 0.8564249820531228,
      "grad_norm": 0.23605124652385712,
      "learning_rate": 0.00015878865225598895,
      "loss": 0.212,
      "step": 11930
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 6.953709602355957,
      "learning_rate": 0.00015873547632341603,
      "loss": 0.2489,
      "step": 11940
    },
    {
      "epoch": 0.8578607322325915,
      "grad_norm": 0.9419875741004944,
      "learning_rate": 0.0001586823003908431,
      "loss": 0.3126,
      "step": 11950
    },
    {
      "epoch": 0.8585786073223259,
      "grad_norm": 2.233118772506714,
      "learning_rate": 0.0001586291244582702,
      "loss": 0.1446,
      "step": 11960
    },
    {
      "epoch": 0.8592964824120602,
      "grad_norm": 0.48885729908943176,
      "learning_rate": 0.00015857594852569727,
      "loss": 0.0574,
      "step": 11970
    },
    {
      "epoch": 0.8600143575017947,
      "grad_norm": 6.186768054962158,
      "learning_rate": 0.00015852277259312435,
      "loss": 0.1673,
      "step": 11980
    },
    {
      "epoch": 0.8607322325915291,
      "grad_norm": 15.419243812561035,
      "learning_rate": 0.00015846959666055143,
      "loss": 0.1871,
      "step": 11990
    },
    {
      "epoch": 0.8614501076812635,
      "grad_norm": 16.163686752319336,
      "learning_rate": 0.00015841642072797853,
      "loss": 0.0616,
      "step": 12000
    },
    {
      "epoch": 0.8621679827709978,
      "grad_norm": 1.690240740776062,
      "learning_rate": 0.00015836324479540561,
      "loss": 0.2922,
      "step": 12010
    },
    {
      "epoch": 0.8628858578607322,
      "grad_norm": 24.54299545288086,
      "learning_rate": 0.0001583100688628327,
      "loss": 0.5818,
      "step": 12020
    },
    {
      "epoch": 0.8636037329504667,
      "grad_norm": 13.631690979003906,
      "learning_rate": 0.00015825689293025977,
      "loss": 0.2756,
      "step": 12030
    },
    {
      "epoch": 0.864321608040201,
      "grad_norm": 0.21452505886554718,
      "learning_rate": 0.00015820371699768685,
      "loss": 0.2031,
      "step": 12040
    },
    {
      "epoch": 0.8650394831299354,
      "grad_norm": 9.448668479919434,
      "learning_rate": 0.00015815054106511393,
      "loss": 0.1734,
      "step": 12050
    },
    {
      "epoch": 0.8657573582196698,
      "grad_norm": 0.05986086651682854,
      "learning_rate": 0.000158097365132541,
      "loss": 0.2327,
      "step": 12060
    },
    {
      "epoch": 0.8664752333094041,
      "grad_norm": 12.921780586242676,
      "learning_rate": 0.00015804418919996812,
      "loss": 0.3027,
      "step": 12070
    },
    {
      "epoch": 0.8671931083991385,
      "grad_norm": 0.14392034709453583,
      "learning_rate": 0.00015799101326739517,
      "loss": 0.2953,
      "step": 12080
    },
    {
      "epoch": 0.867910983488873,
      "grad_norm": 0.21796093881130219,
      "learning_rate": 0.00015793783733482225,
      "loss": 0.1931,
      "step": 12090
    },
    {
      "epoch": 0.8686288585786073,
      "grad_norm": 10.924053192138672,
      "learning_rate": 0.00015788466140224936,
      "loss": 0.5477,
      "step": 12100
    },
    {
      "epoch": 0.8693467336683417,
      "grad_norm": 6.958584308624268,
      "learning_rate": 0.00015783148546967644,
      "loss": 0.1262,
      "step": 12110
    },
    {
      "epoch": 0.8700646087580761,
      "grad_norm": 0.29190361499786377,
      "learning_rate": 0.00015777830953710352,
      "loss": 0.2456,
      "step": 12120
    },
    {
      "epoch": 0.8707824838478104,
      "grad_norm": 7.83608341217041,
      "learning_rate": 0.0001577251336045306,
      "loss": 0.2981,
      "step": 12130
    },
    {
      "epoch": 0.8715003589375449,
      "grad_norm": 11.02092170715332,
      "learning_rate": 0.00015767195767195767,
      "loss": 0.216,
      "step": 12140
    },
    {
      "epoch": 0.8722182340272793,
      "grad_norm": 3.2723774909973145,
      "learning_rate": 0.00015761878173938475,
      "loss": 0.1508,
      "step": 12150
    },
    {
      "epoch": 0.8729361091170137,
      "grad_norm": 9.222883224487305,
      "learning_rate": 0.00015756560580681183,
      "loss": 0.1989,
      "step": 12160
    },
    {
      "epoch": 0.873653984206748,
      "grad_norm": 5.307323455810547,
      "learning_rate": 0.00015751242987423894,
      "loss": 0.2837,
      "step": 12170
    },
    {
      "epoch": 0.8743718592964824,
      "grad_norm": 0.15995025634765625,
      "learning_rate": 0.00015745925394166602,
      "loss": 0.2535,
      "step": 12180
    },
    {
      "epoch": 0.8750897343862168,
      "grad_norm": 8.186609268188477,
      "learning_rate": 0.00015740607800909307,
      "loss": 0.1606,
      "step": 12190
    },
    {
      "epoch": 0.8758076094759512,
      "grad_norm": 0.06273337453603745,
      "learning_rate": 0.00015735290207652018,
      "loss": 0.3139,
      "step": 12200
    },
    {
      "epoch": 0.8765254845656856,
      "grad_norm": 1.2890137434005737,
      "learning_rate": 0.00015729972614394726,
      "loss": 0.1989,
      "step": 12210
    },
    {
      "epoch": 0.87724335965542,
      "grad_norm": 6.5065202713012695,
      "learning_rate": 0.00015724655021137434,
      "loss": 0.3744,
      "step": 12220
    },
    {
      "epoch": 0.8779612347451543,
      "grad_norm": 25.77219009399414,
      "learning_rate": 0.00015719337427880144,
      "loss": 0.2356,
      "step": 12230
    },
    {
      "epoch": 0.8786791098348887,
      "grad_norm": 20.291015625,
      "learning_rate": 0.0001571401983462285,
      "loss": 0.3455,
      "step": 12240
    },
    {
      "epoch": 0.8793969849246231,
      "grad_norm": 8.562131881713867,
      "learning_rate": 0.00015708702241365558,
      "loss": 0.231,
      "step": 12250
    },
    {
      "epoch": 0.8801148600143575,
      "grad_norm": 2.1889102458953857,
      "learning_rate": 0.00015703384648108268,
      "loss": 0.3575,
      "step": 12260
    },
    {
      "epoch": 0.8808327351040919,
      "grad_norm": 0.6942485570907593,
      "learning_rate": 0.00015698067054850976,
      "loss": 0.2537,
      "step": 12270
    },
    {
      "epoch": 0.8815506101938263,
      "grad_norm": 0.8226823806762695,
      "learning_rate": 0.00015692749461593684,
      "loss": 0.2873,
      "step": 12280
    },
    {
      "epoch": 0.8822684852835606,
      "grad_norm": 6.848333358764648,
      "learning_rate": 0.00015687431868336392,
      "loss": 0.3095,
      "step": 12290
    },
    {
      "epoch": 0.882986360373295,
      "grad_norm": 3.273402214050293,
      "learning_rate": 0.000156821142750791,
      "loss": 0.2749,
      "step": 12300
    },
    {
      "epoch": 0.8837042354630295,
      "grad_norm": 6.658769607543945,
      "learning_rate": 0.00015676796681821808,
      "loss": 0.2387,
      "step": 12310
    },
    {
      "epoch": 0.8844221105527639,
      "grad_norm": 15.53836441040039,
      "learning_rate": 0.00015671479088564516,
      "loss": 0.2856,
      "step": 12320
    },
    {
      "epoch": 0.8851399856424982,
      "grad_norm": 0.07248211652040482,
      "learning_rate": 0.00015666161495307227,
      "loss": 0.0219,
      "step": 12330
    },
    {
      "epoch": 0.8858578607322326,
      "grad_norm": 0.263200044631958,
      "learning_rate": 0.00015660843902049932,
      "loss": 0.2159,
      "step": 12340
    },
    {
      "epoch": 0.886575735821967,
      "grad_norm": 14.868254661560059,
      "learning_rate": 0.0001565552630879264,
      "loss": 0.2931,
      "step": 12350
    },
    {
      "epoch": 0.8872936109117013,
      "grad_norm": 1.4930493831634521,
      "learning_rate": 0.0001565020871553535,
      "loss": 0.0933,
      "step": 12360
    },
    {
      "epoch": 0.8880114860014358,
      "grad_norm": 16.115215301513672,
      "learning_rate": 0.00015644891122278058,
      "loss": 0.3359,
      "step": 12370
    },
    {
      "epoch": 0.8887293610911702,
      "grad_norm": 16.07269859313965,
      "learning_rate": 0.00015639573529020766,
      "loss": 0.2579,
      "step": 12380
    },
    {
      "epoch": 0.8894472361809045,
      "grad_norm": 5.805087089538574,
      "learning_rate": 0.00015634255935763474,
      "loss": 0.3849,
      "step": 12390
    },
    {
      "epoch": 0.8901651112706389,
      "grad_norm": 0.126991868019104,
      "learning_rate": 0.00015628938342506182,
      "loss": 0.1487,
      "step": 12400
    },
    {
      "epoch": 0.8908829863603733,
      "grad_norm": 0.022886594757437706,
      "learning_rate": 0.0001562362074924889,
      "loss": 0.3494,
      "step": 12410
    },
    {
      "epoch": 0.8916008614501076,
      "grad_norm": 13.008905410766602,
      "learning_rate": 0.00015618303155991598,
      "loss": 0.2098,
      "step": 12420
    },
    {
      "epoch": 0.8923187365398421,
      "grad_norm": 11.28788948059082,
      "learning_rate": 0.0001561298556273431,
      "loss": 0.2601,
      "step": 12430
    },
    {
      "epoch": 0.8930366116295765,
      "grad_norm": 9.147527694702148,
      "learning_rate": 0.00015607667969477017,
      "loss": 0.3378,
      "step": 12440
    },
    {
      "epoch": 0.8937544867193108,
      "grad_norm": 18.794004440307617,
      "learning_rate": 0.00015602350376219722,
      "loss": 0.1294,
      "step": 12450
    },
    {
      "epoch": 0.8944723618090452,
      "grad_norm": 11.528300285339355,
      "learning_rate": 0.00015597032782962433,
      "loss": 0.143,
      "step": 12460
    },
    {
      "epoch": 0.8951902368987796,
      "grad_norm": 1.9133341312408447,
      "learning_rate": 0.0001559171518970514,
      "loss": 0.3493,
      "step": 12470
    },
    {
      "epoch": 0.8959081119885139,
      "grad_norm": 12.008835792541504,
      "learning_rate": 0.00015586397596447849,
      "loss": 0.4669,
      "step": 12480
    },
    {
      "epoch": 0.8966259870782484,
      "grad_norm": 1.1481444835662842,
      "learning_rate": 0.00015581080003190557,
      "loss": 0.2223,
      "step": 12490
    },
    {
      "epoch": 0.8973438621679828,
      "grad_norm": 6.518469333648682,
      "learning_rate": 0.00015575762409933264,
      "loss": 0.1459,
      "step": 12500
    },
    {
      "epoch": 0.8980617372577172,
      "grad_norm": 1.3949527740478516,
      "learning_rate": 0.00015570444816675972,
      "loss": 0.4421,
      "step": 12510
    },
    {
      "epoch": 0.8987796123474515,
      "grad_norm": 26.9881649017334,
      "learning_rate": 0.0001556512722341868,
      "loss": 0.3624,
      "step": 12520
    },
    {
      "epoch": 0.8994974874371859,
      "grad_norm": 0.5950194001197815,
      "learning_rate": 0.0001555980963016139,
      "loss": 0.42,
      "step": 12530
    },
    {
      "epoch": 0.9002153625269204,
      "grad_norm": 15.944051742553711,
      "learning_rate": 0.000155544920369041,
      "loss": 0.3113,
      "step": 12540
    },
    {
      "epoch": 0.9009332376166547,
      "grad_norm": 16.04977035522461,
      "learning_rate": 0.00015549174443646807,
      "loss": 0.4127,
      "step": 12550
    },
    {
      "epoch": 0.9016511127063891,
      "grad_norm": 6.672685623168945,
      "learning_rate": 0.00015543856850389515,
      "loss": 0.3488,
      "step": 12560
    },
    {
      "epoch": 0.9023689877961235,
      "grad_norm": 0.5652123093605042,
      "learning_rate": 0.00015538539257132223,
      "loss": 0.3069,
      "step": 12570
    },
    {
      "epoch": 0.9030868628858578,
      "grad_norm": 9.009113311767578,
      "learning_rate": 0.0001553322166387493,
      "loss": 0.3809,
      "step": 12580
    },
    {
      "epoch": 0.9038047379755922,
      "grad_norm": 1.083612084388733,
      "learning_rate": 0.0001552790407061764,
      "loss": 0.2558,
      "step": 12590
    },
    {
      "epoch": 0.9045226130653267,
      "grad_norm": 0.9650187492370605,
      "learning_rate": 0.00015522586477360347,
      "loss": 0.2953,
      "step": 12600
    },
    {
      "epoch": 0.905240488155061,
      "grad_norm": 19.790279388427734,
      "learning_rate": 0.00015517268884103055,
      "loss": 0.2399,
      "step": 12610
    },
    {
      "epoch": 0.9059583632447954,
      "grad_norm": 14.829329490661621,
      "learning_rate": 0.00015511951290845763,
      "loss": 0.1126,
      "step": 12620
    },
    {
      "epoch": 0.9066762383345298,
      "grad_norm": 7.789073944091797,
      "learning_rate": 0.00015506633697588473,
      "loss": 0.236,
      "step": 12630
    },
    {
      "epoch": 0.9073941134242641,
      "grad_norm": 3.294743061065674,
      "learning_rate": 0.0001550131610433118,
      "loss": 0.2454,
      "step": 12640
    },
    {
      "epoch": 0.9081119885139985,
      "grad_norm": 9.836190223693848,
      "learning_rate": 0.0001549599851107389,
      "loss": 0.2145,
      "step": 12650
    },
    {
      "epoch": 0.908829863603733,
      "grad_norm": 18.526569366455078,
      "learning_rate": 0.00015490680917816597,
      "loss": 0.2649,
      "step": 12660
    },
    {
      "epoch": 0.9095477386934674,
      "grad_norm": 0.16923323273658752,
      "learning_rate": 0.00015485363324559305,
      "loss": 0.3588,
      "step": 12670
    },
    {
      "epoch": 0.9102656137832017,
      "grad_norm": 1.5355262756347656,
      "learning_rate": 0.00015480045731302013,
      "loss": 0.303,
      "step": 12680
    },
    {
      "epoch": 0.9109834888729361,
      "grad_norm": 0.17437708377838135,
      "learning_rate": 0.0001547472813804472,
      "loss": 0.1026,
      "step": 12690
    },
    {
      "epoch": 0.9117013639626705,
      "grad_norm": 25.248672485351562,
      "learning_rate": 0.00015469410544787432,
      "loss": 0.3138,
      "step": 12700
    },
    {
      "epoch": 0.9124192390524049,
      "grad_norm": 14.756101608276367,
      "learning_rate": 0.00015464092951530137,
      "loss": 0.5165,
      "step": 12710
    },
    {
      "epoch": 0.9131371141421393,
      "grad_norm": 1.3411636352539062,
      "learning_rate": 0.00015458775358272845,
      "loss": 0.5228,
      "step": 12720
    },
    {
      "epoch": 0.9138549892318737,
      "grad_norm": 0.031182849779725075,
      "learning_rate": 0.00015453457765015555,
      "loss": 0.2215,
      "step": 12730
    },
    {
      "epoch": 0.914572864321608,
      "grad_norm": 3.9149045944213867,
      "learning_rate": 0.00015448140171758263,
      "loss": 0.1086,
      "step": 12740
    },
    {
      "epoch": 0.9152907394113424,
      "grad_norm": 2.181964635848999,
      "learning_rate": 0.0001544282257850097,
      "loss": 0.1598,
      "step": 12750
    },
    {
      "epoch": 0.9160086145010768,
      "grad_norm": 20.561464309692383,
      "learning_rate": 0.0001543750498524368,
      "loss": 0.2171,
      "step": 12760
    },
    {
      "epoch": 0.9167264895908112,
      "grad_norm": 0.19785882532596588,
      "learning_rate": 0.00015432187391986387,
      "loss": 0.3617,
      "step": 12770
    },
    {
      "epoch": 0.9174443646805456,
      "grad_norm": 14.910714149475098,
      "learning_rate": 0.00015426869798729095,
      "loss": 0.2052,
      "step": 12780
    },
    {
      "epoch": 0.91816223977028,
      "grad_norm": 0.7213811278343201,
      "learning_rate": 0.00015421552205471803,
      "loss": 0.3946,
      "step": 12790
    },
    {
      "epoch": 0.9188801148600143,
      "grad_norm": 14.22236156463623,
      "learning_rate": 0.00015416234612214514,
      "loss": 0.0588,
      "step": 12800
    },
    {
      "epoch": 0.9195979899497487,
      "grad_norm": 10.493487358093262,
      "learning_rate": 0.00015410917018957222,
      "loss": 0.1973,
      "step": 12810
    },
    {
      "epoch": 0.9203158650394831,
      "grad_norm": 9.975790977478027,
      "learning_rate": 0.00015405599425699927,
      "loss": 0.1992,
      "step": 12820
    },
    {
      "epoch": 0.9210337401292176,
      "grad_norm": 0.7803455591201782,
      "learning_rate": 0.00015400281832442638,
      "loss": 0.1041,
      "step": 12830
    },
    {
      "epoch": 0.9217516152189519,
      "grad_norm": 11.704242706298828,
      "learning_rate": 0.00015394964239185346,
      "loss": 0.0813,
      "step": 12840
    },
    {
      "epoch": 0.9224694903086863,
      "grad_norm": 0.12353529781103134,
      "learning_rate": 0.00015389646645928054,
      "loss": 0.0987,
      "step": 12850
    },
    {
      "epoch": 0.9231873653984207,
      "grad_norm": 2.7082090377807617,
      "learning_rate": 0.00015384329052670764,
      "loss": 0.3356,
      "step": 12860
    },
    {
      "epoch": 0.923905240488155,
      "grad_norm": 6.397132396697998,
      "learning_rate": 0.0001537901145941347,
      "loss": 0.1241,
      "step": 12870
    },
    {
      "epoch": 0.9246231155778895,
      "grad_norm": 4.90989875793457,
      "learning_rate": 0.00015373693866156177,
      "loss": 0.4149,
      "step": 12880
    },
    {
      "epoch": 0.9253409906676239,
      "grad_norm": 0.6315723657608032,
      "learning_rate": 0.00015368376272898885,
      "loss": 0.2387,
      "step": 12890
    },
    {
      "epoch": 0.9260588657573582,
      "grad_norm": 10.03077220916748,
      "learning_rate": 0.00015363058679641596,
      "loss": 0.1363,
      "step": 12900
    },
    {
      "epoch": 0.9267767408470926,
      "grad_norm": 12.315449714660645,
      "learning_rate": 0.00015357741086384304,
      "loss": 0.4947,
      "step": 12910
    },
    {
      "epoch": 0.927494615936827,
      "grad_norm": 11.521056175231934,
      "learning_rate": 0.00015352423493127012,
      "loss": 0.2922,
      "step": 12920
    },
    {
      "epoch": 0.9282124910265613,
      "grad_norm": 7.273861408233643,
      "learning_rate": 0.0001534710589986972,
      "loss": 0.2255,
      "step": 12930
    },
    {
      "epoch": 0.9289303661162958,
      "grad_norm": 18.266746520996094,
      "learning_rate": 0.00015341788306612428,
      "loss": 0.3359,
      "step": 12940
    },
    {
      "epoch": 0.9296482412060302,
      "grad_norm": 17.350767135620117,
      "learning_rate": 0.00015336470713355136,
      "loss": 0.257,
      "step": 12950
    },
    {
      "epoch": 0.9303661162957645,
      "grad_norm": 20.402925491333008,
      "learning_rate": 0.00015331153120097846,
      "loss": 0.173,
      "step": 12960
    },
    {
      "epoch": 0.9310839913854989,
      "grad_norm": 6.693394660949707,
      "learning_rate": 0.00015325835526840552,
      "loss": 0.1618,
      "step": 12970
    },
    {
      "epoch": 0.9318018664752333,
      "grad_norm": 0.08472821116447449,
      "learning_rate": 0.0001532051793358326,
      "loss": 0.3581,
      "step": 12980
    },
    {
      "epoch": 0.9325197415649676,
      "grad_norm": 1.4329651594161987,
      "learning_rate": 0.0001531520034032597,
      "loss": 0.16,
      "step": 12990
    },
    {
      "epoch": 0.9332376166547021,
      "grad_norm": 0.1476219892501831,
      "learning_rate": 0.00015309882747068678,
      "loss": 0.1576,
      "step": 13000
    },
    {
      "epoch": 0.9339554917444365,
      "grad_norm": 11.423112869262695,
      "learning_rate": 0.00015304565153811386,
      "loss": 0.2951,
      "step": 13010
    },
    {
      "epoch": 0.9346733668341709,
      "grad_norm": 10.599957466125488,
      "learning_rate": 0.00015299247560554094,
      "loss": 0.5053,
      "step": 13020
    },
    {
      "epoch": 0.9353912419239052,
      "grad_norm": 0.27338847517967224,
      "learning_rate": 0.00015293929967296802,
      "loss": 0.0395,
      "step": 13030
    },
    {
      "epoch": 0.9361091170136396,
      "grad_norm": 1.4776841402053833,
      "learning_rate": 0.0001528861237403951,
      "loss": 0.0951,
      "step": 13040
    },
    {
      "epoch": 0.9368269921033741,
      "grad_norm": 0.1661064624786377,
      "learning_rate": 0.00015283294780782218,
      "loss": 0.2121,
      "step": 13050
    },
    {
      "epoch": 0.9375448671931084,
      "grad_norm": 28.066375732421875,
      "learning_rate": 0.00015277977187524929,
      "loss": 0.3887,
      "step": 13060
    },
    {
      "epoch": 0.9382627422828428,
      "grad_norm": 5.216662883758545,
      "learning_rate": 0.00015272659594267637,
      "loss": 0.2074,
      "step": 13070
    },
    {
      "epoch": 0.9389806173725772,
      "grad_norm": 0.21464675664901733,
      "learning_rate": 0.00015267342001010342,
      "loss": 0.1292,
      "step": 13080
    },
    {
      "epoch": 0.9396984924623115,
      "grad_norm": 10.692529678344727,
      "learning_rate": 0.00015262024407753052,
      "loss": 0.0965,
      "step": 13090
    },
    {
      "epoch": 0.9404163675520459,
      "grad_norm": 3.622119665145874,
      "learning_rate": 0.0001525670681449576,
      "loss": 0.2858,
      "step": 13100
    },
    {
      "epoch": 0.9411342426417804,
      "grad_norm": 9.0260009765625,
      "learning_rate": 0.00015251389221238468,
      "loss": 0.3433,
      "step": 13110
    },
    {
      "epoch": 0.9418521177315147,
      "grad_norm": 10.08508014678955,
      "learning_rate": 0.00015246071627981176,
      "loss": 0.4739,
      "step": 13120
    },
    {
      "epoch": 0.9425699928212491,
      "grad_norm": 27.981531143188477,
      "learning_rate": 0.00015240754034723884,
      "loss": 0.1744,
      "step": 13130
    },
    {
      "epoch": 0.9432878679109835,
      "grad_norm": 0.03637298569083214,
      "learning_rate": 0.00015235436441466592,
      "loss": 0.1827,
      "step": 13140
    },
    {
      "epoch": 0.9440057430007178,
      "grad_norm": 0.036912642419338226,
      "learning_rate": 0.000152301188482093,
      "loss": 0.1359,
      "step": 13150
    },
    {
      "epoch": 0.9447236180904522,
      "grad_norm": 13.243953704833984,
      "learning_rate": 0.0001522480125495201,
      "loss": 0.5445,
      "step": 13160
    },
    {
      "epoch": 0.9454414931801867,
      "grad_norm": 0.7170562744140625,
      "learning_rate": 0.0001521948366169472,
      "loss": 0.1688,
      "step": 13170
    },
    {
      "epoch": 0.9461593682699211,
      "grad_norm": 8.873669624328613,
      "learning_rate": 0.00015214166068437427,
      "loss": 0.3503,
      "step": 13180
    },
    {
      "epoch": 0.9468772433596554,
      "grad_norm": 0.1338668316602707,
      "learning_rate": 0.00015208848475180135,
      "loss": 0.3024,
      "step": 13190
    },
    {
      "epoch": 0.9475951184493898,
      "grad_norm": 2.469115734100342,
      "learning_rate": 0.00015203530881922843,
      "loss": 0.2323,
      "step": 13200
    },
    {
      "epoch": 0.9483129935391242,
      "grad_norm": 0.6403381824493408,
      "learning_rate": 0.0001519821328866555,
      "loss": 0.3151,
      "step": 13210
    },
    {
      "epoch": 0.9490308686288585,
      "grad_norm": 0.24782107770442963,
      "learning_rate": 0.00015192895695408259,
      "loss": 0.1439,
      "step": 13220
    },
    {
      "epoch": 0.949748743718593,
      "grad_norm": 2.3050003051757812,
      "learning_rate": 0.00015187578102150966,
      "loss": 0.0741,
      "step": 13230
    },
    {
      "epoch": 0.9504666188083274,
      "grad_norm": 13.073541641235352,
      "learning_rate": 0.00015182260508893674,
      "loss": 0.338,
      "step": 13240
    },
    {
      "epoch": 0.9511844938980617,
      "grad_norm": 19.75440216064453,
      "learning_rate": 0.00015176942915636382,
      "loss": 0.3475,
      "step": 13250
    },
    {
      "epoch": 0.9519023689877961,
      "grad_norm": 9.956701278686523,
      "learning_rate": 0.00015171625322379093,
      "loss": 0.3875,
      "step": 13260
    },
    {
      "epoch": 0.9526202440775305,
      "grad_norm": 18.7369384765625,
      "learning_rate": 0.000151663077291218,
      "loss": 0.3491,
      "step": 13270
    },
    {
      "epoch": 0.9533381191672649,
      "grad_norm": 18.07200050354004,
      "learning_rate": 0.0001516099013586451,
      "loss": 0.3256,
      "step": 13280
    },
    {
      "epoch": 0.9540559942569993,
      "grad_norm": 9.636195182800293,
      "learning_rate": 0.00015155672542607217,
      "loss": 0.3949,
      "step": 13290
    },
    {
      "epoch": 0.9547738693467337,
      "grad_norm": 9.509434700012207,
      "learning_rate": 0.00015150354949349925,
      "loss": 0.2316,
      "step": 13300
    },
    {
      "epoch": 0.955491744436468,
      "grad_norm": 11.158812522888184,
      "learning_rate": 0.00015145037356092633,
      "loss": 0.1716,
      "step": 13310
    },
    {
      "epoch": 0.9562096195262024,
      "grad_norm": 0.9315239191055298,
      "learning_rate": 0.0001513971976283534,
      "loss": 0.1394,
      "step": 13320
    },
    {
      "epoch": 0.9569274946159368,
      "grad_norm": 14.900269508361816,
      "learning_rate": 0.00015134402169578051,
      "loss": 0.2862,
      "step": 13330
    },
    {
      "epoch": 0.9576453697056713,
      "grad_norm": 0.09935925155878067,
      "learning_rate": 0.00015129084576320757,
      "loss": 0.1739,
      "step": 13340
    },
    {
      "epoch": 0.9583632447954056,
      "grad_norm": 2.293579339981079,
      "learning_rate": 0.00015123766983063465,
      "loss": 0.1571,
      "step": 13350
    },
    {
      "epoch": 0.95908111988514,
      "grad_norm": 9.825700759887695,
      "learning_rate": 0.00015118449389806175,
      "loss": 0.5714,
      "step": 13360
    },
    {
      "epoch": 0.9597989949748744,
      "grad_norm": 26.26235008239746,
      "learning_rate": 0.00015113131796548883,
      "loss": 0.3397,
      "step": 13370
    },
    {
      "epoch": 0.9605168700646087,
      "grad_norm": 2.09824275970459,
      "learning_rate": 0.0001510781420329159,
      "loss": 0.4172,
      "step": 13380
    },
    {
      "epoch": 0.9612347451543432,
      "grad_norm": 0.29338911175727844,
      "learning_rate": 0.000151024966100343,
      "loss": 0.2663,
      "step": 13390
    },
    {
      "epoch": 0.9619526202440776,
      "grad_norm": 14.681293487548828,
      "learning_rate": 0.00015097179016777007,
      "loss": 0.0958,
      "step": 13400
    },
    {
      "epoch": 0.9626704953338119,
      "grad_norm": 1.014146327972412,
      "learning_rate": 0.00015091861423519715,
      "loss": 0.1604,
      "step": 13410
    },
    {
      "epoch": 0.9633883704235463,
      "grad_norm": 15.669000625610352,
      "learning_rate": 0.00015086543830262423,
      "loss": 0.1256,
      "step": 13420
    },
    {
      "epoch": 0.9641062455132807,
      "grad_norm": 13.046699523925781,
      "learning_rate": 0.00015081226237005134,
      "loss": 0.0801,
      "step": 13430
    },
    {
      "epoch": 0.964824120603015,
      "grad_norm": 0.2385810762643814,
      "learning_rate": 0.00015075908643747842,
      "loss": 0.3289,
      "step": 13440
    },
    {
      "epoch": 0.9655419956927495,
      "grad_norm": 12.898436546325684,
      "learning_rate": 0.00015070591050490547,
      "loss": 0.2898,
      "step": 13450
    },
    {
      "epoch": 0.9662598707824839,
      "grad_norm": 3.2042651176452637,
      "learning_rate": 0.00015065273457233257,
      "loss": 0.2105,
      "step": 13460
    },
    {
      "epoch": 0.9669777458722182,
      "grad_norm": 0.06560003012418747,
      "learning_rate": 0.00015059955863975965,
      "loss": 0.2724,
      "step": 13470
    },
    {
      "epoch": 0.9676956209619526,
      "grad_norm": 2.880194902420044,
      "learning_rate": 0.00015054638270718673,
      "loss": 0.3737,
      "step": 13480
    },
    {
      "epoch": 0.968413496051687,
      "grad_norm": 0.2954391539096832,
      "learning_rate": 0.0001504932067746138,
      "loss": 0.2464,
      "step": 13490
    },
    {
      "epoch": 0.9691313711414213,
      "grad_norm": 10.652420997619629,
      "learning_rate": 0.0001504400308420409,
      "loss": 0.2421,
      "step": 13500
    },
    {
      "epoch": 0.9698492462311558,
      "grad_norm": 2.6683199405670166,
      "learning_rate": 0.00015038685490946797,
      "loss": 0.2024,
      "step": 13510
    },
    {
      "epoch": 0.9705671213208902,
      "grad_norm": 24.8371524810791,
      "learning_rate": 0.00015033367897689505,
      "loss": 0.3289,
      "step": 13520
    },
    {
      "epoch": 0.9712849964106246,
      "grad_norm": 14.60846996307373,
      "learning_rate": 0.00015028050304432216,
      "loss": 0.3001,
      "step": 13530
    },
    {
      "epoch": 0.9720028715003589,
      "grad_norm": 3.500464677810669,
      "learning_rate": 0.00015022732711174924,
      "loss": 0.6851,
      "step": 13540
    },
    {
      "epoch": 0.9727207465900933,
      "grad_norm": 5.790135383605957,
      "learning_rate": 0.0001501741511791763,
      "loss": 0.2592,
      "step": 13550
    },
    {
      "epoch": 0.9734386216798278,
      "grad_norm": 7.436849117279053,
      "learning_rate": 0.0001501209752466034,
      "loss": 0.1465,
      "step": 13560
    },
    {
      "epoch": 0.9741564967695621,
      "grad_norm": 11.616500854492188,
      "learning_rate": 0.00015006779931403048,
      "loss": 0.1637,
      "step": 13570
    },
    {
      "epoch": 0.9748743718592965,
      "grad_norm": 0.11369828134775162,
      "learning_rate": 0.00015001462338145756,
      "loss": 0.0442,
      "step": 13580
    },
    {
      "epoch": 0.9755922469490309,
      "grad_norm": 3.370940685272217,
      "learning_rate": 0.00014996144744888466,
      "loss": 0.2073,
      "step": 13590
    },
    {
      "epoch": 0.9763101220387652,
      "grad_norm": 8.556706428527832,
      "learning_rate": 0.00014990827151631171,
      "loss": 0.12,
      "step": 13600
    },
    {
      "epoch": 0.9770279971284996,
      "grad_norm": 0.3264889717102051,
      "learning_rate": 0.0001498550955837388,
      "loss": 0.2487,
      "step": 13610
    },
    {
      "epoch": 0.9777458722182341,
      "grad_norm": 2.4446659088134766,
      "learning_rate": 0.00014980191965116587,
      "loss": 0.2184,
      "step": 13620
    },
    {
      "epoch": 0.9784637473079684,
      "grad_norm": 4.6823835372924805,
      "learning_rate": 0.00014974874371859298,
      "loss": 0.2293,
      "step": 13630
    },
    {
      "epoch": 0.9791816223977028,
      "grad_norm": 10.568124771118164,
      "learning_rate": 0.00014969556778602006,
      "loss": 0.2502,
      "step": 13640
    },
    {
      "epoch": 0.9798994974874372,
      "grad_norm": 4.744994163513184,
      "learning_rate": 0.00014964239185344714,
      "loss": 0.1942,
      "step": 13650
    },
    {
      "epoch": 0.9806173725771715,
      "grad_norm": 0.11141906678676605,
      "learning_rate": 0.00014958921592087422,
      "loss": 0.1854,
      "step": 13660
    },
    {
      "epoch": 0.9813352476669059,
      "grad_norm": 0.008504096418619156,
      "learning_rate": 0.0001495360399883013,
      "loss": 0.2225,
      "step": 13670
    },
    {
      "epoch": 0.9820531227566404,
      "grad_norm": 0.23196864128112793,
      "learning_rate": 0.00014948286405572838,
      "loss": 0.1607,
      "step": 13680
    },
    {
      "epoch": 0.9827709978463748,
      "grad_norm": 8.974105834960938,
      "learning_rate": 0.00014942968812315548,
      "loss": 0.1914,
      "step": 13690
    },
    {
      "epoch": 0.9834888729361091,
      "grad_norm": 0.014874087646603584,
      "learning_rate": 0.00014937651219058256,
      "loss": 0.2723,
      "step": 13700
    },
    {
      "epoch": 0.9842067480258435,
      "grad_norm": 9.376877784729004,
      "learning_rate": 0.00014932333625800962,
      "loss": 0.243,
      "step": 13710
    },
    {
      "epoch": 0.9849246231155779,
      "grad_norm": 2.4265124797821045,
      "learning_rate": 0.00014927016032543672,
      "loss": 0.1848,
      "step": 13720
    },
    {
      "epoch": 0.9856424982053122,
      "grad_norm": 1.0372475385665894,
      "learning_rate": 0.0001492169843928638,
      "loss": 0.3579,
      "step": 13730
    },
    {
      "epoch": 0.9863603732950467,
      "grad_norm": 0.27790752053260803,
      "learning_rate": 0.00014916380846029088,
      "loss": 0.1596,
      "step": 13740
    },
    {
      "epoch": 0.9870782483847811,
      "grad_norm": 6.133638381958008,
      "learning_rate": 0.00014911063252771796,
      "loss": 0.2342,
      "step": 13750
    },
    {
      "epoch": 0.9877961234745154,
      "grad_norm": 12.65764045715332,
      "learning_rate": 0.00014905745659514504,
      "loss": 0.1188,
      "step": 13760
    },
    {
      "epoch": 0.9885139985642498,
      "grad_norm": 11.169763565063477,
      "learning_rate": 0.00014900428066257212,
      "loss": 0.2156,
      "step": 13770
    },
    {
      "epoch": 0.9892318736539842,
      "grad_norm": 1.823293685913086,
      "learning_rate": 0.0001489511047299992,
      "loss": 0.2947,
      "step": 13780
    },
    {
      "epoch": 0.9899497487437185,
      "grad_norm": 7.151967525482178,
      "learning_rate": 0.0001488979287974263,
      "loss": 0.3663,
      "step": 13790
    },
    {
      "epoch": 0.990667623833453,
      "grad_norm": 15.785416603088379,
      "learning_rate": 0.00014884475286485339,
      "loss": 0.2759,
      "step": 13800
    },
    {
      "epoch": 0.9913854989231874,
      "grad_norm": 0.9865481853485107,
      "learning_rate": 0.00014879157693228047,
      "loss": 0.1975,
      "step": 13810
    },
    {
      "epoch": 0.9921033740129217,
      "grad_norm": 15.943313598632812,
      "learning_rate": 0.00014873840099970754,
      "loss": 0.1603,
      "step": 13820
    },
    {
      "epoch": 0.9928212491026561,
      "grad_norm": 1.106642484664917,
      "learning_rate": 0.00014868522506713462,
      "loss": 0.054,
      "step": 13830
    },
    {
      "epoch": 0.9935391241923905,
      "grad_norm": 0.09535820782184601,
      "learning_rate": 0.0001486320491345617,
      "loss": 0.2935,
      "step": 13840
    },
    {
      "epoch": 0.994256999282125,
      "grad_norm": 0.17130906879901886,
      "learning_rate": 0.00014857887320198878,
      "loss": 0.1392,
      "step": 13850
    },
    {
      "epoch": 0.9949748743718593,
      "grad_norm": 5.365914344787598,
      "learning_rate": 0.00014852569726941586,
      "loss": 0.0858,
      "step": 13860
    },
    {
      "epoch": 0.9956927494615937,
      "grad_norm": 11.845368385314941,
      "learning_rate": 0.00014847252133684294,
      "loss": 0.2124,
      "step": 13870
    },
    {
      "epoch": 0.9964106245513281,
      "grad_norm": 0.6609846353530884,
      "learning_rate": 0.00014841934540427002,
      "loss": 0.1966,
      "step": 13880
    },
    {
      "epoch": 0.9971284996410624,
      "grad_norm": 0.5441268086433411,
      "learning_rate": 0.00014836616947169713,
      "loss": 0.5906,
      "step": 13890
    },
    {
      "epoch": 0.9978463747307968,
      "grad_norm": 0.024298978969454765,
      "learning_rate": 0.0001483129935391242,
      "loss": 0.1454,
      "step": 13900
    },
    {
      "epoch": 0.9985642498205313,
      "grad_norm": 0.2875455617904663,
      "learning_rate": 0.0001482598176065513,
      "loss": 0.1299,
      "step": 13910
    },
    {
      "epoch": 0.9992821249102656,
      "grad_norm": 1.5657727718353271,
      "learning_rate": 0.00014820664167397837,
      "loss": 0.2246,
      "step": 13920
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.08295547217130661,
      "learning_rate": 0.00014815346574140545,
      "loss": 0.4096,
      "step": 13930
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9417898571967,
      "eval_f1_class_0_Oxidored": 0.9498901098901099,
      "eval_f1_class_1_Transfer": 0.9404856013551666,
      "eval_f1_class_2_Hydrolas": 0.9246370623398804,
      "eval_f1_class_3_Lyases": 0.9132749441868453,
      "eval_f1_class_4_Isomeras": 0.9224067519738633,
      "eval_f1_class_5_Ligases": 0.9786644499861458,
      "eval_f1_class_6_Transloc": 0.9604944907282988,
      "eval_f1_macro": 0.9414076300657586,
      "eval_f1_micro": 0.9417898571967,
      "eval_loss": 0.23410142958164215,
      "eval_precision_macro": 0.9464988717512034,
      "eval_precision_micro": 0.9417898571967,
      "eval_recall_macro": 0.9381444223541068,
      "eval_recall_micro": 0.9417898571967,
      "eval_runtime": 207.036,
      "eval_samples_per_second": 115.337,
      "eval_steps_per_second": 14.418,
      "step": 13930
    },
    {
      "epoch": 1.0007178750897343,
      "grad_norm": 7.683931827545166,
      "learning_rate": 0.00014810028980883253,
      "loss": 0.2248,
      "step": 13940
    },
    {
      "epoch": 1.0014357501794688,
      "grad_norm": 17.524221420288086,
      "learning_rate": 0.0001480471138762596,
      "loss": 0.3022,
      "step": 13950
    },
    {
      "epoch": 1.0021536252692032,
      "grad_norm": 1.139318823814392,
      "learning_rate": 0.0001479939379436867,
      "loss": 0.0595,
      "step": 13960
    },
    {
      "epoch": 1.0028715003589375,
      "grad_norm": 14.711714744567871,
      "learning_rate": 0.00014794076201111376,
      "loss": 0.3506,
      "step": 13970
    },
    {
      "epoch": 1.003589375448672,
      "grad_norm": 0.9802612662315369,
      "learning_rate": 0.00014788758607854084,
      "loss": 0.1228,
      "step": 13980
    },
    {
      "epoch": 1.0043072505384063,
      "grad_norm": 6.939553260803223,
      "learning_rate": 0.00014783441014596795,
      "loss": 0.1356,
      "step": 13990
    },
    {
      "epoch": 1.0050251256281406,
      "grad_norm": 2.3899898529052734,
      "learning_rate": 0.00014778123421339503,
      "loss": 0.2285,
      "step": 14000
    },
    {
      "epoch": 1.0057430007178751,
      "grad_norm": 4.494326114654541,
      "learning_rate": 0.0001477280582808221,
      "loss": 0.129,
      "step": 14010
    },
    {
      "epoch": 1.0064608758076095,
      "grad_norm": 0.08572094142436981,
      "learning_rate": 0.0001476748823482492,
      "loss": 0.1506,
      "step": 14020
    },
    {
      "epoch": 1.0071787508973438,
      "grad_norm": 0.41162192821502686,
      "learning_rate": 0.00014762170641567627,
      "loss": 0.0157,
      "step": 14030
    },
    {
      "epoch": 1.0078966259870783,
      "grad_norm": 0.097951240837574,
      "learning_rate": 0.00014756853048310335,
      "loss": 0.1653,
      "step": 14040
    },
    {
      "epoch": 1.0086145010768126,
      "grad_norm": 0.24892832338809967,
      "learning_rate": 0.00014751535455053043,
      "loss": 0.1171,
      "step": 14050
    },
    {
      "epoch": 1.009332376166547,
      "grad_norm": 5.026662349700928,
      "learning_rate": 0.00014746217861795753,
      "loss": 0.3689,
      "step": 14060
    },
    {
      "epoch": 1.0100502512562815,
      "grad_norm": 1.3786367177963257,
      "learning_rate": 0.00014740900268538461,
      "loss": 0.2969,
      "step": 14070
    },
    {
      "epoch": 1.0107681263460158,
      "grad_norm": 6.488454818725586,
      "learning_rate": 0.00014735582675281167,
      "loss": 0.0761,
      "step": 14080
    },
    {
      "epoch": 1.01148600143575,
      "grad_norm": 0.24402858316898346,
      "learning_rate": 0.00014730265082023877,
      "loss": 0.3229,
      "step": 14090
    },
    {
      "epoch": 1.0122038765254846,
      "grad_norm": 0.6121760010719299,
      "learning_rate": 0.00014724947488766585,
      "loss": 0.2734,
      "step": 14100
    },
    {
      "epoch": 1.012921751615219,
      "grad_norm": 14.808692932128906,
      "learning_rate": 0.00014719629895509293,
      "loss": 0.1179,
      "step": 14110
    },
    {
      "epoch": 1.0136396267049534,
      "grad_norm": 0.07201755046844482,
      "learning_rate": 0.00014714312302252,
      "loss": 0.1172,
      "step": 14120
    },
    {
      "epoch": 1.0143575017946878,
      "grad_norm": 17.425256729125977,
      "learning_rate": 0.0001470899470899471,
      "loss": 0.132,
      "step": 14130
    },
    {
      "epoch": 1.015075376884422,
      "grad_norm": 0.16343875229358673,
      "learning_rate": 0.00014703677115737417,
      "loss": 0.2818,
      "step": 14140
    },
    {
      "epoch": 1.0157932519741566,
      "grad_norm": 0.653221070766449,
      "learning_rate": 0.00014698359522480125,
      "loss": 0.2188,
      "step": 14150
    },
    {
      "epoch": 1.016511127063891,
      "grad_norm": 0.1339060366153717,
      "learning_rate": 0.00014693041929222836,
      "loss": 0.6044,
      "step": 14160
    },
    {
      "epoch": 1.0172290021536252,
      "grad_norm": 0.46169525384902954,
      "learning_rate": 0.00014687724335965544,
      "loss": 0.172,
      "step": 14170
    },
    {
      "epoch": 1.0179468772433597,
      "grad_norm": 7.590945243835449,
      "learning_rate": 0.0001468240674270825,
      "loss": 0.1985,
      "step": 14180
    },
    {
      "epoch": 1.018664752333094,
      "grad_norm": 28.32359504699707,
      "learning_rate": 0.0001467708914945096,
      "loss": 0.3303,
      "step": 14190
    },
    {
      "epoch": 1.0193826274228284,
      "grad_norm": 0.2867048680782318,
      "learning_rate": 0.00014671771556193667,
      "loss": 0.0996,
      "step": 14200
    },
    {
      "epoch": 1.020100502512563,
      "grad_norm": 0.3940131962299347,
      "learning_rate": 0.00014666453962936375,
      "loss": 0.0642,
      "step": 14210
    },
    {
      "epoch": 1.0208183776022972,
      "grad_norm": 0.4095127582550049,
      "learning_rate": 0.00014661136369679086,
      "loss": 0.011,
      "step": 14220
    },
    {
      "epoch": 1.0215362526920315,
      "grad_norm": 6.803194522857666,
      "learning_rate": 0.0001465581877642179,
      "loss": 0.1226,
      "step": 14230
    },
    {
      "epoch": 1.022254127781766,
      "grad_norm": 7.198747634887695,
      "learning_rate": 0.000146505011831645,
      "loss": 0.2626,
      "step": 14240
    },
    {
      "epoch": 1.0229720028715004,
      "grad_norm": 11.711627960205078,
      "learning_rate": 0.00014645183589907207,
      "loss": 0.2469,
      "step": 14250
    },
    {
      "epoch": 1.0236898779612347,
      "grad_norm": 8.926891326904297,
      "learning_rate": 0.00014639865996649918,
      "loss": 0.2723,
      "step": 14260
    },
    {
      "epoch": 1.0244077530509692,
      "grad_norm": 0.0680442526936531,
      "learning_rate": 0.00014634548403392626,
      "loss": 0.1625,
      "step": 14270
    },
    {
      "epoch": 1.0251256281407035,
      "grad_norm": 0.33641552925109863,
      "learning_rate": 0.00014629230810135334,
      "loss": 0.1268,
      "step": 14280
    },
    {
      "epoch": 1.0258435032304378,
      "grad_norm": 2.3456997871398926,
      "learning_rate": 0.00014623913216878042,
      "loss": 0.2578,
      "step": 14290
    },
    {
      "epoch": 1.0265613783201724,
      "grad_norm": 0.3825930953025818,
      "learning_rate": 0.0001461859562362075,
      "loss": 0.1107,
      "step": 14300
    },
    {
      "epoch": 1.0272792534099067,
      "grad_norm": 0.0170411616563797,
      "learning_rate": 0.00014613278030363458,
      "loss": 0.6649,
      "step": 14310
    },
    {
      "epoch": 1.027997128499641,
      "grad_norm": 3.010622262954712,
      "learning_rate": 0.00014607960437106168,
      "loss": 0.0914,
      "step": 14320
    },
    {
      "epoch": 1.0287150035893755,
      "grad_norm": 6.913626670837402,
      "learning_rate": 0.00014602642843848876,
      "loss": 0.1341,
      "step": 14330
    },
    {
      "epoch": 1.0294328786791098,
      "grad_norm": 0.018498210236430168,
      "learning_rate": 0.00014597325250591581,
      "loss": 0.0739,
      "step": 14340
    },
    {
      "epoch": 1.0301507537688441,
      "grad_norm": 15.935408592224121,
      "learning_rate": 0.0001459200765733429,
      "loss": 0.288,
      "step": 14350
    },
    {
      "epoch": 1.0308686288585787,
      "grad_norm": 0.48200082778930664,
      "learning_rate": 0.00014586690064077,
      "loss": 0.2568,
      "step": 14360
    },
    {
      "epoch": 1.031586503948313,
      "grad_norm": 0.016049522906541824,
      "learning_rate": 0.00014581372470819708,
      "loss": 0.2287,
      "step": 14370
    },
    {
      "epoch": 1.0323043790380473,
      "grad_norm": 0.015358328819274902,
      "learning_rate": 0.00014576054877562416,
      "loss": 0.1509,
      "step": 14380
    },
    {
      "epoch": 1.0330222541277818,
      "grad_norm": 9.849780082702637,
      "learning_rate": 0.00014570737284305124,
      "loss": 0.1664,
      "step": 14390
    },
    {
      "epoch": 1.0337401292175161,
      "grad_norm": 15.435285568237305,
      "learning_rate": 0.00014565419691047832,
      "loss": 0.0625,
      "step": 14400
    },
    {
      "epoch": 1.0344580043072504,
      "grad_norm": 6.459534168243408,
      "learning_rate": 0.0001456010209779054,
      "loss": 0.2413,
      "step": 14410
    },
    {
      "epoch": 1.035175879396985,
      "grad_norm": 13.217079162597656,
      "learning_rate": 0.0001455478450453325,
      "loss": 0.0735,
      "step": 14420
    },
    {
      "epoch": 1.0358937544867193,
      "grad_norm": 9.192770004272461,
      "learning_rate": 0.00014549466911275958,
      "loss": 0.18,
      "step": 14430
    },
    {
      "epoch": 1.0366116295764538,
      "grad_norm": 16.705434799194336,
      "learning_rate": 0.00014544149318018666,
      "loss": 0.2306,
      "step": 14440
    },
    {
      "epoch": 1.0373295046661881,
      "grad_norm": 0.5514551997184753,
      "learning_rate": 0.00014538831724761374,
      "loss": 0.2157,
      "step": 14450
    },
    {
      "epoch": 1.0380473797559224,
      "grad_norm": 0.16438791155815125,
      "learning_rate": 0.00014533514131504082,
      "loss": 0.156,
      "step": 14460
    },
    {
      "epoch": 1.038765254845657,
      "grad_norm": 11.995630264282227,
      "learning_rate": 0.0001452819653824679,
      "loss": 0.3586,
      "step": 14470
    },
    {
      "epoch": 1.0394831299353913,
      "grad_norm": 11.322447776794434,
      "learning_rate": 0.00014522878944989498,
      "loss": 0.1107,
      "step": 14480
    },
    {
      "epoch": 1.0402010050251256,
      "grad_norm": 0.037796735763549805,
      "learning_rate": 0.00014517561351732206,
      "loss": 0.3091,
      "step": 14490
    },
    {
      "epoch": 1.0409188801148601,
      "grad_norm": 8.236418724060059,
      "learning_rate": 0.00014512243758474914,
      "loss": 0.1486,
      "step": 14500
    },
    {
      "epoch": 1.0416367552045944,
      "grad_norm": 9.960639953613281,
      "learning_rate": 0.00014506926165217622,
      "loss": 0.3113,
      "step": 14510
    },
    {
      "epoch": 1.0423546302943287,
      "grad_norm": 0.015658525750041008,
      "learning_rate": 0.00014501608571960333,
      "loss": 0.1986,
      "step": 14520
    },
    {
      "epoch": 1.0430725053840633,
      "grad_norm": 3.470421075820923,
      "learning_rate": 0.0001449629097870304,
      "loss": 0.3304,
      "step": 14530
    },
    {
      "epoch": 1.0437903804737976,
      "grad_norm": 0.03987062722444534,
      "learning_rate": 0.00014490973385445749,
      "loss": 0.1053,
      "step": 14540
    },
    {
      "epoch": 1.0445082555635319,
      "grad_norm": 15.951703071594238,
      "learning_rate": 0.00014485655792188456,
      "loss": 0.4756,
      "step": 14550
    },
    {
      "epoch": 1.0452261306532664,
      "grad_norm": 5.0780110359191895,
      "learning_rate": 0.00014480338198931164,
      "loss": 0.1657,
      "step": 14560
    },
    {
      "epoch": 1.0459440057430007,
      "grad_norm": 0.6923946738243103,
      "learning_rate": 0.00014475020605673872,
      "loss": 0.0691,
      "step": 14570
    },
    {
      "epoch": 1.046661880832735,
      "grad_norm": 1.0099290609359741,
      "learning_rate": 0.0001446970301241658,
      "loss": 0.2173,
      "step": 14580
    },
    {
      "epoch": 1.0473797559224696,
      "grad_norm": 0.31798988580703735,
      "learning_rate": 0.0001446438541915929,
      "loss": 0.1222,
      "step": 14590
    },
    {
      "epoch": 1.0480976310122039,
      "grad_norm": 6.92903995513916,
      "learning_rate": 0.00014459067825901996,
      "loss": 0.1231,
      "step": 14600
    },
    {
      "epoch": 1.0488155061019382,
      "grad_norm": 0.2795828878879547,
      "learning_rate": 0.00014453750232644704,
      "loss": 0.0795,
      "step": 14610
    },
    {
      "epoch": 1.0495333811916727,
      "grad_norm": 0.03413629159331322,
      "learning_rate": 0.00014448432639387415,
      "loss": 0.1235,
      "step": 14620
    },
    {
      "epoch": 1.050251256281407,
      "grad_norm": 0.0180372204631567,
      "learning_rate": 0.00014443115046130123,
      "loss": 0.1807,
      "step": 14630
    },
    {
      "epoch": 1.0509691313711413,
      "grad_norm": 0.9480995535850525,
      "learning_rate": 0.0001443779745287283,
      "loss": 0.0124,
      "step": 14640
    },
    {
      "epoch": 1.0516870064608759,
      "grad_norm": 11.678857803344727,
      "learning_rate": 0.0001443247985961554,
      "loss": 0.2594,
      "step": 14650
    },
    {
      "epoch": 1.0524048815506102,
      "grad_norm": 0.28234562277793884,
      "learning_rate": 0.00014427162266358247,
      "loss": 0.3997,
      "step": 14660
    },
    {
      "epoch": 1.0531227566403445,
      "grad_norm": 0.9987117648124695,
      "learning_rate": 0.00014421844673100955,
      "loss": 0.1648,
      "step": 14670
    },
    {
      "epoch": 1.053840631730079,
      "grad_norm": 15.211991310119629,
      "learning_rate": 0.00014416527079843663,
      "loss": 0.5312,
      "step": 14680
    },
    {
      "epoch": 1.0545585068198133,
      "grad_norm": 2.8438873291015625,
      "learning_rate": 0.00014411209486586373,
      "loss": 0.1512,
      "step": 14690
    },
    {
      "epoch": 1.0552763819095476,
      "grad_norm": 16.822181701660156,
      "learning_rate": 0.0001440589189332908,
      "loss": 0.1979,
      "step": 14700
    },
    {
      "epoch": 1.0559942569992822,
      "grad_norm": 7.24062967300415,
      "learning_rate": 0.00014400574300071786,
      "loss": 0.2014,
      "step": 14710
    },
    {
      "epoch": 1.0567121320890165,
      "grad_norm": 7.019815444946289,
      "learning_rate": 0.00014395256706814497,
      "loss": 0.3671,
      "step": 14720
    },
    {
      "epoch": 1.0574300071787508,
      "grad_norm": 11.070502281188965,
      "learning_rate": 0.00014389939113557205,
      "loss": 0.5729,
      "step": 14730
    },
    {
      "epoch": 1.0581478822684853,
      "grad_norm": 15.459554672241211,
      "learning_rate": 0.00014384621520299913,
      "loss": 0.1093,
      "step": 14740
    },
    {
      "epoch": 1.0588657573582196,
      "grad_norm": 18.096946716308594,
      "learning_rate": 0.0001437930392704262,
      "loss": 0.1946,
      "step": 14750
    },
    {
      "epoch": 1.059583632447954,
      "grad_norm": 37.199790954589844,
      "learning_rate": 0.0001437398633378533,
      "loss": 0.3249,
      "step": 14760
    },
    {
      "epoch": 1.0603015075376885,
      "grad_norm": 19.245311737060547,
      "learning_rate": 0.00014368668740528037,
      "loss": 0.2234,
      "step": 14770
    },
    {
      "epoch": 1.0610193826274228,
      "grad_norm": 7.060549259185791,
      "learning_rate": 0.00014363351147270745,
      "loss": 0.4845,
      "step": 14780
    },
    {
      "epoch": 1.0617372577171573,
      "grad_norm": 12.699909210205078,
      "learning_rate": 0.00014358033554013455,
      "loss": 0.4012,
      "step": 14790
    },
    {
      "epoch": 1.0624551328068916,
      "grad_norm": 17.28961944580078,
      "learning_rate": 0.00014352715960756163,
      "loss": 0.4526,
      "step": 14800
    },
    {
      "epoch": 1.063173007896626,
      "grad_norm": 0.34390518069267273,
      "learning_rate": 0.00014347398367498869,
      "loss": 0.2185,
      "step": 14810
    },
    {
      "epoch": 1.0638908829863605,
      "grad_norm": 0.025077205151319504,
      "learning_rate": 0.0001434208077424158,
      "loss": 0.0523,
      "step": 14820
    },
    {
      "epoch": 1.0646087580760948,
      "grad_norm": 7.470213890075684,
      "learning_rate": 0.00014336763180984287,
      "loss": 0.16,
      "step": 14830
    },
    {
      "epoch": 1.065326633165829,
      "grad_norm": 0.4061422348022461,
      "learning_rate": 0.00014331445587726995,
      "loss": 0.0374,
      "step": 14840
    },
    {
      "epoch": 1.0660445082555636,
      "grad_norm": 1.2715134620666504,
      "learning_rate": 0.00014326127994469706,
      "loss": 0.1804,
      "step": 14850
    },
    {
      "epoch": 1.066762383345298,
      "grad_norm": 2.073646306991577,
      "learning_rate": 0.0001432081040121241,
      "loss": 0.5991,
      "step": 14860
    },
    {
      "epoch": 1.0674802584350322,
      "grad_norm": 0.06607133150100708,
      "learning_rate": 0.0001431549280795512,
      "loss": 0.2873,
      "step": 14870
    },
    {
      "epoch": 1.0681981335247668,
      "grad_norm": 1.4567309617996216,
      "learning_rate": 0.00014310175214697827,
      "loss": 0.0835,
      "step": 14880
    },
    {
      "epoch": 1.068916008614501,
      "grad_norm": 19.574886322021484,
      "learning_rate": 0.00014304857621440538,
      "loss": 0.1636,
      "step": 14890
    },
    {
      "epoch": 1.0696338837042354,
      "grad_norm": 11.810165405273438,
      "learning_rate": 0.00014299540028183246,
      "loss": 0.1526,
      "step": 14900
    },
    {
      "epoch": 1.07035175879397,
      "grad_norm": 6.970269203186035,
      "learning_rate": 0.00014294222434925954,
      "loss": 0.3654,
      "step": 14910
    },
    {
      "epoch": 1.0710696338837042,
      "grad_norm": 19.474353790283203,
      "learning_rate": 0.00014288904841668661,
      "loss": 0.1263,
      "step": 14920
    },
    {
      "epoch": 1.0717875089734386,
      "grad_norm": 0.014469465240836143,
      "learning_rate": 0.0001428358724841137,
      "loss": 0.197,
      "step": 14930
    },
    {
      "epoch": 1.072505384063173,
      "grad_norm": 0.01339380256831646,
      "learning_rate": 0.00014278269655154077,
      "loss": 0.0604,
      "step": 14940
    },
    {
      "epoch": 1.0732232591529074,
      "grad_norm": 0.0263641569763422,
      "learning_rate": 0.00014272952061896788,
      "loss": 0.2899,
      "step": 14950
    },
    {
      "epoch": 1.0739411342426417,
      "grad_norm": 20.355113983154297,
      "learning_rate": 0.00014267634468639496,
      "loss": 0.1375,
      "step": 14960
    },
    {
      "epoch": 1.0746590093323762,
      "grad_norm": 14.143233299255371,
      "learning_rate": 0.000142623168753822,
      "loss": 0.2118,
      "step": 14970
    },
    {
      "epoch": 1.0753768844221105,
      "grad_norm": 21.790660858154297,
      "learning_rate": 0.0001425699928212491,
      "loss": 0.1778,
      "step": 14980
    },
    {
      "epoch": 1.0760947595118449,
      "grad_norm": 4.642856121063232,
      "learning_rate": 0.0001425168168886762,
      "loss": 0.2174,
      "step": 14990
    },
    {
      "epoch": 1.0768126346015794,
      "grad_norm": 0.01669071801006794,
      "learning_rate": 0.00014246364095610328,
      "loss": 0.0475,
      "step": 15000
    },
    {
      "epoch": 1.0775305096913137,
      "grad_norm": 0.3047932982444763,
      "learning_rate": 0.00014241046502353036,
      "loss": 0.3364,
      "step": 15010
    },
    {
      "epoch": 1.078248384781048,
      "grad_norm": 2.493211269378662,
      "learning_rate": 0.00014235728909095744,
      "loss": 0.3318,
      "step": 15020
    },
    {
      "epoch": 1.0789662598707825,
      "grad_norm": 3.549751043319702,
      "learning_rate": 0.00014230411315838452,
      "loss": 0.178,
      "step": 15030
    },
    {
      "epoch": 1.0796841349605169,
      "grad_norm": 0.04449732229113579,
      "learning_rate": 0.0001422509372258116,
      "loss": 0.095,
      "step": 15040
    },
    {
      "epoch": 1.0804020100502512,
      "grad_norm": 14.867303848266602,
      "learning_rate": 0.0001421977612932387,
      "loss": 0.2657,
      "step": 15050
    },
    {
      "epoch": 1.0811198851399857,
      "grad_norm": 0.2026975452899933,
      "learning_rate": 0.00014214458536066578,
      "loss": 0.3453,
      "step": 15060
    },
    {
      "epoch": 1.08183776022972,
      "grad_norm": 0.8269562721252441,
      "learning_rate": 0.00014209140942809283,
      "loss": 0.2851,
      "step": 15070
    },
    {
      "epoch": 1.0825556353194543,
      "grad_norm": 0.305074006319046,
      "learning_rate": 0.00014203823349551991,
      "loss": 0.0925,
      "step": 15080
    },
    {
      "epoch": 1.0832735104091888,
      "grad_norm": 21.908720016479492,
      "learning_rate": 0.00014198505756294702,
      "loss": 0.1243,
      "step": 15090
    },
    {
      "epoch": 1.0839913854989232,
      "grad_norm": 2.8115053176879883,
      "learning_rate": 0.0001419318816303741,
      "loss": 0.2554,
      "step": 15100
    },
    {
      "epoch": 1.0847092605886575,
      "grad_norm": 0.004292844329029322,
      "learning_rate": 0.00014187870569780118,
      "loss": 0.0925,
      "step": 15110
    },
    {
      "epoch": 1.085427135678392,
      "grad_norm": 0.10765793919563293,
      "learning_rate": 0.00014182552976522826,
      "loss": 0.3984,
      "step": 15120
    },
    {
      "epoch": 1.0861450107681263,
      "grad_norm": 17.14583396911621,
      "learning_rate": 0.00014177235383265534,
      "loss": 0.1435,
      "step": 15130
    },
    {
      "epoch": 1.0868628858578608,
      "grad_norm": 0.5383179783821106,
      "learning_rate": 0.00014171917790008242,
      "loss": 0.2153,
      "step": 15140
    },
    {
      "epoch": 1.0875807609475951,
      "grad_norm": 23.83989715576172,
      "learning_rate": 0.00014166600196750952,
      "loss": 0.1253,
      "step": 15150
    },
    {
      "epoch": 1.0882986360373295,
      "grad_norm": 0.2376023232936859,
      "learning_rate": 0.0001416128260349366,
      "loss": 0.1516,
      "step": 15160
    },
    {
      "epoch": 1.089016511127064,
      "grad_norm": 0.2940368056297302,
      "learning_rate": 0.00014155965010236368,
      "loss": 0.3375,
      "step": 15170
    },
    {
      "epoch": 1.0897343862167983,
      "grad_norm": 12.554272651672363,
      "learning_rate": 0.00014150647416979076,
      "loss": 0.1193,
      "step": 15180
    },
    {
      "epoch": 1.0904522613065326,
      "grad_norm": 21.927139282226562,
      "learning_rate": 0.00014145329823721784,
      "loss": 0.1958,
      "step": 15190
    },
    {
      "epoch": 1.0911701363962671,
      "grad_norm": 13.397717475891113,
      "learning_rate": 0.00014140012230464492,
      "loss": 0.1526,
      "step": 15200
    },
    {
      "epoch": 1.0918880114860015,
      "grad_norm": 23.91118049621582,
      "learning_rate": 0.000141346946372072,
      "loss": 0.1322,
      "step": 15210
    },
    {
      "epoch": 1.0926058865757358,
      "grad_norm": 11.02538776397705,
      "learning_rate": 0.0001412937704394991,
      "loss": 0.4318,
      "step": 15220
    },
    {
      "epoch": 1.0933237616654703,
      "grad_norm": 1.0581672191619873,
      "learning_rate": 0.00014124059450692616,
      "loss": 0.2443,
      "step": 15230
    },
    {
      "epoch": 1.0940416367552046,
      "grad_norm": 2.0317273139953613,
      "learning_rate": 0.00014118741857435324,
      "loss": 0.0285,
      "step": 15240
    },
    {
      "epoch": 1.094759511844939,
      "grad_norm": 14.73431396484375,
      "learning_rate": 0.00014113424264178035,
      "loss": 0.2715,
      "step": 15250
    },
    {
      "epoch": 1.0954773869346734,
      "grad_norm": 11.149141311645508,
      "learning_rate": 0.00014108106670920743,
      "loss": 0.6072,
      "step": 15260
    },
    {
      "epoch": 1.0961952620244078,
      "grad_norm": 0.636984646320343,
      "learning_rate": 0.0001410278907766345,
      "loss": 0.1899,
      "step": 15270
    },
    {
      "epoch": 1.096913137114142,
      "grad_norm": 6.207257270812988,
      "learning_rate": 0.00014097471484406159,
      "loss": 0.1223,
      "step": 15280
    },
    {
      "epoch": 1.0976310122038766,
      "grad_norm": 0.7151857614517212,
      "learning_rate": 0.00014092153891148866,
      "loss": 0.214,
      "step": 15290
    },
    {
      "epoch": 1.098348887293611,
      "grad_norm": 0.3529118299484253,
      "learning_rate": 0.00014086836297891574,
      "loss": 0.1364,
      "step": 15300
    },
    {
      "epoch": 1.0990667623833452,
      "grad_norm": 0.07744552940130234,
      "learning_rate": 0.00014081518704634282,
      "loss": 0.1749,
      "step": 15310
    },
    {
      "epoch": 1.0997846374730798,
      "grad_norm": 16.705949783325195,
      "learning_rate": 0.00014076201111376993,
      "loss": 0.483,
      "step": 15320
    },
    {
      "epoch": 1.100502512562814,
      "grad_norm": 1.206075668334961,
      "learning_rate": 0.000140708835181197,
      "loss": 0.2709,
      "step": 15330
    },
    {
      "epoch": 1.1012203876525484,
      "grad_norm": 0.47922757267951965,
      "learning_rate": 0.00014065565924862406,
      "loss": 0.0987,
      "step": 15340
    },
    {
      "epoch": 1.101938262742283,
      "grad_norm": 0.06869063526391983,
      "learning_rate": 0.00014060248331605117,
      "loss": 0.0999,
      "step": 15350
    },
    {
      "epoch": 1.1026561378320172,
      "grad_norm": 2.5727336406707764,
      "learning_rate": 0.00014054930738347825,
      "loss": 0.0918,
      "step": 15360
    },
    {
      "epoch": 1.1033740129217515,
      "grad_norm": 0.5039606094360352,
      "learning_rate": 0.00014049613145090533,
      "loss": 0.2448,
      "step": 15370
    },
    {
      "epoch": 1.104091888011486,
      "grad_norm": 10.538653373718262,
      "learning_rate": 0.0001404429555183324,
      "loss": 0.1809,
      "step": 15380
    },
    {
      "epoch": 1.1048097631012204,
      "grad_norm": 7.879743576049805,
      "learning_rate": 0.0001403897795857595,
      "loss": 0.0832,
      "step": 15390
    },
    {
      "epoch": 1.1055276381909547,
      "grad_norm": 0.07662173360586166,
      "learning_rate": 0.00014033660365318657,
      "loss": 0.0724,
      "step": 15400
    },
    {
      "epoch": 1.1062455132806892,
      "grad_norm": 22.07644271850586,
      "learning_rate": 0.00014028342772061365,
      "loss": 0.5152,
      "step": 15410
    },
    {
      "epoch": 1.1069633883704235,
      "grad_norm": 27.03565216064453,
      "learning_rate": 0.00014023025178804075,
      "loss": 0.4135,
      "step": 15420
    },
    {
      "epoch": 1.107681263460158,
      "grad_norm": 19.582534790039062,
      "learning_rate": 0.00014017707585546783,
      "loss": 0.0982,
      "step": 15430
    },
    {
      "epoch": 1.1083991385498924,
      "grad_norm": 0.009196881204843521,
      "learning_rate": 0.00014012389992289488,
      "loss": 0.1362,
      "step": 15440
    },
    {
      "epoch": 1.1091170136396267,
      "grad_norm": 14.121232986450195,
      "learning_rate": 0.000140070723990322,
      "loss": 0.2187,
      "step": 15450
    },
    {
      "epoch": 1.109834888729361,
      "grad_norm": 0.2367660254240036,
      "learning_rate": 0.00014001754805774907,
      "loss": 0.0507,
      "step": 15460
    },
    {
      "epoch": 1.1105527638190955,
      "grad_norm": 0.46114543080329895,
      "learning_rate": 0.00013996437212517615,
      "loss": 0.1999,
      "step": 15470
    },
    {
      "epoch": 1.1112706389088298,
      "grad_norm": 11.104732513427734,
      "learning_rate": 0.00013991119619260326,
      "loss": 0.2669,
      "step": 15480
    },
    {
      "epoch": 1.1119885139985644,
      "grad_norm": 19.98661231994629,
      "learning_rate": 0.0001398580202600303,
      "loss": 0.3022,
      "step": 15490
    },
    {
      "epoch": 1.1127063890882987,
      "grad_norm": 17.866710662841797,
      "learning_rate": 0.0001398048443274574,
      "loss": 0.2936,
      "step": 15500
    },
    {
      "epoch": 1.113424264178033,
      "grad_norm": 9.811758041381836,
      "learning_rate": 0.00013975166839488447,
      "loss": 0.2439,
      "step": 15510
    },
    {
      "epoch": 1.1141421392677675,
      "grad_norm": 0.008552310988307,
      "learning_rate": 0.00013969849246231157,
      "loss": 0.0746,
      "step": 15520
    },
    {
      "epoch": 1.1148600143575018,
      "grad_norm": 0.13235396146774292,
      "learning_rate": 0.00013964531652973865,
      "loss": 0.1858,
      "step": 15530
    },
    {
      "epoch": 1.1155778894472361,
      "grad_norm": 0.14850760996341705,
      "learning_rate": 0.00013959214059716573,
      "loss": 0.3312,
      "step": 15540
    },
    {
      "epoch": 1.1162957645369707,
      "grad_norm": 0.07272806018590927,
      "learning_rate": 0.0001395389646645928,
      "loss": 0.1742,
      "step": 15550
    },
    {
      "epoch": 1.117013639626705,
      "grad_norm": 12.906286239624023,
      "learning_rate": 0.0001394857887320199,
      "loss": 0.1467,
      "step": 15560
    },
    {
      "epoch": 1.1177315147164393,
      "grad_norm": 0.0029359047766774893,
      "learning_rate": 0.00013943261279944697,
      "loss": 0.1111,
      "step": 15570
    },
    {
      "epoch": 1.1184493898061738,
      "grad_norm": 19.003250122070312,
      "learning_rate": 0.00013937943686687408,
      "loss": 0.2439,
      "step": 15580
    },
    {
      "epoch": 1.1191672648959081,
      "grad_norm": 8.687451362609863,
      "learning_rate": 0.00013932626093430116,
      "loss": 0.3713,
      "step": 15590
    },
    {
      "epoch": 1.1198851399856424,
      "grad_norm": 13.733816146850586,
      "learning_rate": 0.0001392730850017282,
      "loss": 0.2379,
      "step": 15600
    },
    {
      "epoch": 1.120603015075377,
      "grad_norm": 1.58037531375885,
      "learning_rate": 0.0001392199090691553,
      "loss": 0.2121,
      "step": 15610
    },
    {
      "epoch": 1.1213208901651113,
      "grad_norm": 0.354863703250885,
      "learning_rate": 0.0001391667331365824,
      "loss": 0.1422,
      "step": 15620
    },
    {
      "epoch": 1.1220387652548456,
      "grad_norm": 0.037901535630226135,
      "learning_rate": 0.00013911355720400948,
      "loss": 0.4118,
      "step": 15630
    },
    {
      "epoch": 1.1227566403445801,
      "grad_norm": 0.12573009729385376,
      "learning_rate": 0.00013906038127143656,
      "loss": 0.2533,
      "step": 15640
    },
    {
      "epoch": 1.1234745154343144,
      "grad_norm": 25.883304595947266,
      "learning_rate": 0.00013900720533886363,
      "loss": 0.1631,
      "step": 15650
    },
    {
      "epoch": 1.1241923905240487,
      "grad_norm": 18.514753341674805,
      "learning_rate": 0.00013895402940629071,
      "loss": 0.1198,
      "step": 15660
    },
    {
      "epoch": 1.1249102656137833,
      "grad_norm": 0.10263758152723312,
      "learning_rate": 0.0001389008534737178,
      "loss": 0.1212,
      "step": 15670
    },
    {
      "epoch": 1.1256281407035176,
      "grad_norm": 8.131513595581055,
      "learning_rate": 0.0001388476775411449,
      "loss": 0.1235,
      "step": 15680
    },
    {
      "epoch": 1.1263460157932519,
      "grad_norm": 19.08176612854004,
      "learning_rate": 0.00013879450160857198,
      "loss": 0.4071,
      "step": 15690
    },
    {
      "epoch": 1.1270638908829864,
      "grad_norm": 4.440095901489258,
      "learning_rate": 0.00013874132567599903,
      "loss": 0.0941,
      "step": 15700
    },
    {
      "epoch": 1.1277817659727207,
      "grad_norm": 2.232675075531006,
      "learning_rate": 0.0001386881497434261,
      "loss": 0.2763,
      "step": 15710
    },
    {
      "epoch": 1.128499641062455,
      "grad_norm": 12.351683616638184,
      "learning_rate": 0.00013863497381085322,
      "loss": 0.2583,
      "step": 15720
    },
    {
      "epoch": 1.1292175161521896,
      "grad_norm": 0.015077784657478333,
      "learning_rate": 0.0001385817978782803,
      "loss": 0.2753,
      "step": 15730
    },
    {
      "epoch": 1.1299353912419239,
      "grad_norm": 25.774070739746094,
      "learning_rate": 0.00013852862194570738,
      "loss": 0.1263,
      "step": 15740
    },
    {
      "epoch": 1.1306532663316582,
      "grad_norm": 0.31401845812797546,
      "learning_rate": 0.00013847544601313446,
      "loss": 0.1925,
      "step": 15750
    },
    {
      "epoch": 1.1313711414213927,
      "grad_norm": 0.03316757455468178,
      "learning_rate": 0.00013842227008056154,
      "loss": 0.3101,
      "step": 15760
    },
    {
      "epoch": 1.132089016511127,
      "grad_norm": 0.3924485146999359,
      "learning_rate": 0.00013836909414798862,
      "loss": 0.3402,
      "step": 15770
    },
    {
      "epoch": 1.1328068916008616,
      "grad_norm": 0.1417451649904251,
      "learning_rate": 0.00013831591821541572,
      "loss": 0.2594,
      "step": 15780
    },
    {
      "epoch": 1.1335247666905959,
      "grad_norm": 0.06431678682565689,
      "learning_rate": 0.0001382627422828428,
      "loss": 0.0208,
      "step": 15790
    },
    {
      "epoch": 1.1342426417803302,
      "grad_norm": 14.888352394104004,
      "learning_rate": 0.00013820956635026988,
      "loss": 0.3681,
      "step": 15800
    },
    {
      "epoch": 1.1349605168700645,
      "grad_norm": 12.23147964477539,
      "learning_rate": 0.00013815639041769693,
      "loss": 0.3348,
      "step": 15810
    },
    {
      "epoch": 1.135678391959799,
      "grad_norm": 0.18745489418506622,
      "learning_rate": 0.00013810321448512404,
      "loss": 0.2332,
      "step": 15820
    },
    {
      "epoch": 1.1363962670495333,
      "grad_norm": 13.905192375183105,
      "learning_rate": 0.00013805003855255112,
      "loss": 0.134,
      "step": 15830
    },
    {
      "epoch": 1.1371141421392679,
      "grad_norm": 13.180026054382324,
      "learning_rate": 0.0001379968626199782,
      "loss": 0.2174,
      "step": 15840
    },
    {
      "epoch": 1.1378320172290022,
      "grad_norm": 10.645207405090332,
      "learning_rate": 0.0001379436866874053,
      "loss": 0.2113,
      "step": 15850
    },
    {
      "epoch": 1.1385498923187365,
      "grad_norm": 1.8368796110153198,
      "learning_rate": 0.00013789051075483236,
      "loss": 0.0734,
      "step": 15860
    },
    {
      "epoch": 1.139267767408471,
      "grad_norm": 8.702089309692383,
      "learning_rate": 0.00013783733482225944,
      "loss": 0.2315,
      "step": 15870
    },
    {
      "epoch": 1.1399856424982053,
      "grad_norm": 0.02187713049352169,
      "learning_rate": 0.00013778415888968654,
      "loss": 0.128,
      "step": 15880
    },
    {
      "epoch": 1.1407035175879396,
      "grad_norm": 13.664501190185547,
      "learning_rate": 0.00013773098295711362,
      "loss": 0.1516,
      "step": 15890
    },
    {
      "epoch": 1.1414213926776742,
      "grad_norm": 18.083580017089844,
      "learning_rate": 0.0001376778070245407,
      "loss": 0.3367,
      "step": 15900
    },
    {
      "epoch": 1.1421392677674085,
      "grad_norm": 9.557247161865234,
      "learning_rate": 0.00013762463109196778,
      "loss": 0.1078,
      "step": 15910
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 1.8960621356964111,
      "learning_rate": 0.00013757145515939486,
      "loss": 0.2187,
      "step": 15920
    },
    {
      "epoch": 1.1435750179468773,
      "grad_norm": 10.515687942504883,
      "learning_rate": 0.00013751827922682194,
      "loss": 0.203,
      "step": 15930
    },
    {
      "epoch": 1.1442928930366116,
      "grad_norm": 0.11124250292778015,
      "learning_rate": 0.00013746510329424902,
      "loss": 0.1177,
      "step": 15940
    },
    {
      "epoch": 1.145010768126346,
      "grad_norm": 10.167496681213379,
      "learning_rate": 0.00013741192736167613,
      "loss": 0.2307,
      "step": 15950
    },
    {
      "epoch": 1.1457286432160805,
      "grad_norm": 35.23290252685547,
      "learning_rate": 0.00013735875142910318,
      "loss": 0.1989,
      "step": 15960
    },
    {
      "epoch": 1.1464465183058148,
      "grad_norm": 13.15490436553955,
      "learning_rate": 0.00013730557549653026,
      "loss": 0.1254,
      "step": 15970
    },
    {
      "epoch": 1.147164393395549,
      "grad_norm": 16.546030044555664,
      "learning_rate": 0.00013725239956395737,
      "loss": 0.2409,
      "step": 15980
    },
    {
      "epoch": 1.1478822684852836,
      "grad_norm": 6.298976898193359,
      "learning_rate": 0.00013719922363138445,
      "loss": 0.2213,
      "step": 15990
    },
    {
      "epoch": 1.148600143575018,
      "grad_norm": 0.021128900349140167,
      "learning_rate": 0.00013714604769881153,
      "loss": 0.0316,
      "step": 16000
    },
    {
      "epoch": 1.1493180186647523,
      "grad_norm": 17.01259994506836,
      "learning_rate": 0.0001370928717662386,
      "loss": 0.1162,
      "step": 16010
    },
    {
      "epoch": 1.1500358937544868,
      "grad_norm": 0.04918816313147545,
      "learning_rate": 0.00013703969583366568,
      "loss": 0.2703,
      "step": 16020
    },
    {
      "epoch": 1.150753768844221,
      "grad_norm": 8.61031436920166,
      "learning_rate": 0.00013698651990109276,
      "loss": 0.2391,
      "step": 16030
    },
    {
      "epoch": 1.1514716439339554,
      "grad_norm": 0.05435086041688919,
      "learning_rate": 0.00013693334396851984,
      "loss": 0.1846,
      "step": 16040
    },
    {
      "epoch": 1.15218951902369,
      "grad_norm": 30.1651668548584,
      "learning_rate": 0.00013688016803594695,
      "loss": 0.2339,
      "step": 16050
    },
    {
      "epoch": 1.1529073941134242,
      "grad_norm": 12.058472633361816,
      "learning_rate": 0.00013682699210337403,
      "loss": 0.2416,
      "step": 16060
    },
    {
      "epoch": 1.1536252692031586,
      "grad_norm": 4.793134689331055,
      "learning_rate": 0.00013677381617080108,
      "loss": 0.1978,
      "step": 16070
    },
    {
      "epoch": 1.154343144292893,
      "grad_norm": 20.374279022216797,
      "learning_rate": 0.0001367206402382282,
      "loss": 0.3567,
      "step": 16080
    },
    {
      "epoch": 1.1550610193826274,
      "grad_norm": 0.04059424251317978,
      "learning_rate": 0.00013666746430565527,
      "loss": 0.0709,
      "step": 16090
    },
    {
      "epoch": 1.1557788944723617,
      "grad_norm": 15.185287475585938,
      "learning_rate": 0.00013661428837308235,
      "loss": 0.2563,
      "step": 16100
    },
    {
      "epoch": 1.1564967695620962,
      "grad_norm": 10.429854393005371,
      "learning_rate": 0.00013656111244050945,
      "loss": 0.3898,
      "step": 16110
    },
    {
      "epoch": 1.1572146446518305,
      "grad_norm": 1.3743363618850708,
      "learning_rate": 0.0001365079365079365,
      "loss": 0.0753,
      "step": 16120
    },
    {
      "epoch": 1.157932519741565,
      "grad_norm": 0.014426293782889843,
      "learning_rate": 0.00013645476057536359,
      "loss": 0.1248,
      "step": 16130
    },
    {
      "epoch": 1.1586503948312994,
      "grad_norm": 28.282806396484375,
      "learning_rate": 0.00013640158464279067,
      "loss": 0.2397,
      "step": 16140
    },
    {
      "epoch": 1.1593682699210337,
      "grad_norm": 12.705659866333008,
      "learning_rate": 0.00013634840871021777,
      "loss": 0.1483,
      "step": 16150
    },
    {
      "epoch": 1.160086145010768,
      "grad_norm": 0.030781222507357597,
      "learning_rate": 0.00013629523277764485,
      "loss": 0.0316,
      "step": 16160
    },
    {
      "epoch": 1.1608040201005025,
      "grad_norm": 5.168625354766846,
      "learning_rate": 0.00013624205684507193,
      "loss": 0.2901,
      "step": 16170
    },
    {
      "epoch": 1.1615218951902369,
      "grad_norm": 23.67512321472168,
      "learning_rate": 0.000136188880912499,
      "loss": 0.2022,
      "step": 16180
    },
    {
      "epoch": 1.1622397702799714,
      "grad_norm": 16.6304988861084,
      "learning_rate": 0.0001361357049799261,
      "loss": 0.5805,
      "step": 16190
    },
    {
      "epoch": 1.1629576453697057,
      "grad_norm": 19.934520721435547,
      "learning_rate": 0.00013608252904735317,
      "loss": 0.0836,
      "step": 16200
    },
    {
      "epoch": 1.16367552045944,
      "grad_norm": 0.5888452529907227,
      "learning_rate": 0.00013602935311478028,
      "loss": 0.0868,
      "step": 16210
    },
    {
      "epoch": 1.1643933955491745,
      "grad_norm": 0.02040601707994938,
      "learning_rate": 0.00013597617718220736,
      "loss": 0.3091,
      "step": 16220
    },
    {
      "epoch": 1.1651112706389088,
      "grad_norm": 14.918143272399902,
      "learning_rate": 0.0001359230012496344,
      "loss": 0.1913,
      "step": 16230
    },
    {
      "epoch": 1.1658291457286432,
      "grad_norm": 1.4956685304641724,
      "learning_rate": 0.0001358698253170615,
      "loss": 0.0665,
      "step": 16240
    },
    {
      "epoch": 1.1665470208183777,
      "grad_norm": 0.006680395919829607,
      "learning_rate": 0.0001358166493844886,
      "loss": 0.2586,
      "step": 16250
    },
    {
      "epoch": 1.167264895908112,
      "grad_norm": 6.302558422088623,
      "learning_rate": 0.00013576347345191567,
      "loss": 0.2518,
      "step": 16260
    },
    {
      "epoch": 1.1679827709978463,
      "grad_norm": 6.3879523277282715,
      "learning_rate": 0.00013571029751934275,
      "loss": 0.0761,
      "step": 16270
    },
    {
      "epoch": 1.1687006460875808,
      "grad_norm": 8.83171272277832,
      "learning_rate": 0.00013565712158676983,
      "loss": 0.5241,
      "step": 16280
    },
    {
      "epoch": 1.1694185211773152,
      "grad_norm": 1.0510566234588623,
      "learning_rate": 0.0001356039456541969,
      "loss": 0.1021,
      "step": 16290
    },
    {
      "epoch": 1.1701363962670495,
      "grad_norm": 1.8134337663650513,
      "learning_rate": 0.000135550769721624,
      "loss": 0.1409,
      "step": 16300
    },
    {
      "epoch": 1.170854271356784,
      "grad_norm": 0.026139680296182632,
      "learning_rate": 0.0001354975937890511,
      "loss": 0.3605,
      "step": 16310
    },
    {
      "epoch": 1.1715721464465183,
      "grad_norm": 0.1980777084827423,
      "learning_rate": 0.00013544441785647818,
      "loss": 0.0105,
      "step": 16320
    },
    {
      "epoch": 1.1722900215362526,
      "grad_norm": 15.297237396240234,
      "learning_rate": 0.00013539124192390523,
      "loss": 0.328,
      "step": 16330
    },
    {
      "epoch": 1.1730078966259871,
      "grad_norm": 0.12556003034114838,
      "learning_rate": 0.0001353380659913323,
      "loss": 0.0049,
      "step": 16340
    },
    {
      "epoch": 1.1737257717157215,
      "grad_norm": 0.08876825124025345,
      "learning_rate": 0.00013528489005875942,
      "loss": 0.1749,
      "step": 16350
    },
    {
      "epoch": 1.1744436468054558,
      "grad_norm": 2.219794273376465,
      "learning_rate": 0.0001352317141261865,
      "loss": 0.23,
      "step": 16360
    },
    {
      "epoch": 1.1751615218951903,
      "grad_norm": 16.675220489501953,
      "learning_rate": 0.00013517853819361358,
      "loss": 0.1149,
      "step": 16370
    },
    {
      "epoch": 1.1758793969849246,
      "grad_norm": 4.795766830444336,
      "learning_rate": 0.00013512536226104065,
      "loss": 0.2503,
      "step": 16380
    },
    {
      "epoch": 1.176597272074659,
      "grad_norm": 10.032280921936035,
      "learning_rate": 0.00013507218632846773,
      "loss": 0.0928,
      "step": 16390
    },
    {
      "epoch": 1.1773151471643934,
      "grad_norm": 0.08090827614068985,
      "learning_rate": 0.00013501901039589481,
      "loss": 0.1081,
      "step": 16400
    },
    {
      "epoch": 1.1780330222541278,
      "grad_norm": 21.391067504882812,
      "learning_rate": 0.00013496583446332192,
      "loss": 0.186,
      "step": 16410
    },
    {
      "epoch": 1.1787508973438623,
      "grad_norm": 2.293121814727783,
      "learning_rate": 0.000134912658530749,
      "loss": 0.2732,
      "step": 16420
    },
    {
      "epoch": 1.1794687724335966,
      "grad_norm": 9.564663887023926,
      "learning_rate": 0.00013485948259817608,
      "loss": 0.1313,
      "step": 16430
    },
    {
      "epoch": 1.180186647523331,
      "grad_norm": 0.18051275610923767,
      "learning_rate": 0.00013480630666560313,
      "loss": 0.2464,
      "step": 16440
    },
    {
      "epoch": 1.1809045226130652,
      "grad_norm": 17.024240493774414,
      "learning_rate": 0.00013475313073303024,
      "loss": 0.0591,
      "step": 16450
    },
    {
      "epoch": 1.1816223977027998,
      "grad_norm": 0.02692841924726963,
      "learning_rate": 0.00013469995480045732,
      "loss": 0.2292,
      "step": 16460
    },
    {
      "epoch": 1.182340272792534,
      "grad_norm": 0.23818480968475342,
      "learning_rate": 0.0001346467788678844,
      "loss": 0.3013,
      "step": 16470
    },
    {
      "epoch": 1.1830581478822686,
      "grad_norm": 2.2302820682525635,
      "learning_rate": 0.0001345936029353115,
      "loss": 0.1788,
      "step": 16480
    },
    {
      "epoch": 1.183776022972003,
      "grad_norm": 19.90308380126953,
      "learning_rate": 0.00013454042700273856,
      "loss": 0.1275,
      "step": 16490
    },
    {
      "epoch": 1.1844938980617372,
      "grad_norm": 7.56805419921875,
      "learning_rate": 0.00013448725107016564,
      "loss": 0.529,
      "step": 16500
    },
    {
      "epoch": 1.1852117731514715,
      "grad_norm": 11.035993576049805,
      "learning_rate": 0.00013443407513759274,
      "loss": 0.1607,
      "step": 16510
    },
    {
      "epoch": 1.185929648241206,
      "grad_norm": 7.690922260284424,
      "learning_rate": 0.00013438089920501982,
      "loss": 0.1236,
      "step": 16520
    },
    {
      "epoch": 1.1866475233309404,
      "grad_norm": 4.758249282836914,
      "learning_rate": 0.0001343277232724469,
      "loss": 0.0739,
      "step": 16530
    },
    {
      "epoch": 1.187365398420675,
      "grad_norm": 3.0118274688720703,
      "learning_rate": 0.00013427454733987398,
      "loss": 0.2218,
      "step": 16540
    },
    {
      "epoch": 1.1880832735104092,
      "grad_norm": 3.86118483543396,
      "learning_rate": 0.00013422137140730106,
      "loss": 0.1777,
      "step": 16550
    },
    {
      "epoch": 1.1888011486001435,
      "grad_norm": 2.073786735534668,
      "learning_rate": 0.00013416819547472814,
      "loss": 0.3582,
      "step": 16560
    },
    {
      "epoch": 1.189519023689878,
      "grad_norm": 21.935945510864258,
      "learning_rate": 0.00013411501954215522,
      "loss": 0.1714,
      "step": 16570
    },
    {
      "epoch": 1.1902368987796124,
      "grad_norm": 0.0681031122803688,
      "learning_rate": 0.00013406184360958233,
      "loss": 0.3385,
      "step": 16580
    },
    {
      "epoch": 1.1909547738693467,
      "grad_norm": 0.22263135015964508,
      "learning_rate": 0.00013400866767700938,
      "loss": 0.1402,
      "step": 16590
    },
    {
      "epoch": 1.1916726489590812,
      "grad_norm": 12.506664276123047,
      "learning_rate": 0.00013395549174443646,
      "loss": 0.1654,
      "step": 16600
    },
    {
      "epoch": 1.1923905240488155,
      "grad_norm": 37.168296813964844,
      "learning_rate": 0.00013390231581186356,
      "loss": 0.1444,
      "step": 16610
    },
    {
      "epoch": 1.1931083991385498,
      "grad_norm": 21.40007781982422,
      "learning_rate": 0.00013384913987929064,
      "loss": 0.2862,
      "step": 16620
    },
    {
      "epoch": 1.1938262742282844,
      "grad_norm": 0.05932985246181488,
      "learning_rate": 0.00013379596394671772,
      "loss": 0.2854,
      "step": 16630
    },
    {
      "epoch": 1.1945441493180187,
      "grad_norm": 1.7159792184829712,
      "learning_rate": 0.0001337427880141448,
      "loss": 0.2923,
      "step": 16640
    },
    {
      "epoch": 1.195262024407753,
      "grad_norm": 11.845141410827637,
      "learning_rate": 0.00013368961208157188,
      "loss": 0.1132,
      "step": 16650
    },
    {
      "epoch": 1.1959798994974875,
      "grad_norm": 0.007004807237535715,
      "learning_rate": 0.00013363643614899896,
      "loss": 0.3724,
      "step": 16660
    },
    {
      "epoch": 1.1966977745872218,
      "grad_norm": 0.025249162688851357,
      "learning_rate": 0.00013358326021642604,
      "loss": 0.015,
      "step": 16670
    },
    {
      "epoch": 1.1974156496769561,
      "grad_norm": 17.625967025756836,
      "learning_rate": 0.00013353008428385315,
      "loss": 0.1979,
      "step": 16680
    },
    {
      "epoch": 1.1981335247666907,
      "grad_norm": 12.659892082214355,
      "learning_rate": 0.00013347690835128023,
      "loss": 0.1199,
      "step": 16690
    },
    {
      "epoch": 1.198851399856425,
      "grad_norm": 8.514445304870605,
      "learning_rate": 0.00013342373241870728,
      "loss": 0.4625,
      "step": 16700
    },
    {
      "epoch": 1.1995692749461593,
      "grad_norm": 30.479467391967773,
      "learning_rate": 0.0001333705564861344,
      "loss": 0.1216,
      "step": 16710
    },
    {
      "epoch": 1.2002871500358938,
      "grad_norm": 0.09988240152597427,
      "learning_rate": 0.00013331738055356147,
      "loss": 0.0736,
      "step": 16720
    },
    {
      "epoch": 1.2010050251256281,
      "grad_norm": 17.92105484008789,
      "learning_rate": 0.00013326420462098855,
      "loss": 0.0992,
      "step": 16730
    },
    {
      "epoch": 1.2017229002153624,
      "grad_norm": 0.7138730883598328,
      "learning_rate": 0.00013321102868841565,
      "loss": 0.2319,
      "step": 16740
    },
    {
      "epoch": 1.202440775305097,
      "grad_norm": 8.368956565856934,
      "learning_rate": 0.0001331578527558427,
      "loss": 0.0677,
      "step": 16750
    },
    {
      "epoch": 1.2031586503948313,
      "grad_norm": 12.482327461242676,
      "learning_rate": 0.00013310467682326978,
      "loss": 0.0658,
      "step": 16760
    },
    {
      "epoch": 1.2038765254845658,
      "grad_norm": 8.334861755371094,
      "learning_rate": 0.00013305150089069686,
      "loss": 0.1565,
      "step": 16770
    },
    {
      "epoch": 1.2045944005743001,
      "grad_norm": 3.5992021560668945,
      "learning_rate": 0.00013299832495812397,
      "loss": 0.0519,
      "step": 16780
    },
    {
      "epoch": 1.2053122756640344,
      "grad_norm": 0.09867293387651443,
      "learning_rate": 0.00013294514902555105,
      "loss": 0.3039,
      "step": 16790
    },
    {
      "epoch": 1.2060301507537687,
      "grad_norm": 17.87203025817871,
      "learning_rate": 0.00013289197309297813,
      "loss": 0.1193,
      "step": 16800
    },
    {
      "epoch": 1.2067480258435033,
      "grad_norm": 12.550498962402344,
      "learning_rate": 0.0001328387971604052,
      "loss": 0.3177,
      "step": 16810
    },
    {
      "epoch": 1.2074659009332376,
      "grad_norm": 14.265488624572754,
      "learning_rate": 0.0001327856212278323,
      "loss": 0.1362,
      "step": 16820
    },
    {
      "epoch": 1.2081837760229721,
      "grad_norm": 0.009731597267091274,
      "learning_rate": 0.00013273244529525937,
      "loss": 0.2896,
      "step": 16830
    },
    {
      "epoch": 1.2089016511127064,
      "grad_norm": 0.1316811740398407,
      "learning_rate": 0.00013267926936268647,
      "loss": 0.076,
      "step": 16840
    },
    {
      "epoch": 1.2096195262024407,
      "grad_norm": 0.9911450147628784,
      "learning_rate": 0.00013262609343011355,
      "loss": 0.1524,
      "step": 16850
    },
    {
      "epoch": 1.210337401292175,
      "grad_norm": 0.145605206489563,
      "learning_rate": 0.0001325729174975406,
      "loss": 0.2147,
      "step": 16860
    },
    {
      "epoch": 1.2110552763819096,
      "grad_norm": 0.11255817860364914,
      "learning_rate": 0.00013251974156496769,
      "loss": 0.0289,
      "step": 16870
    },
    {
      "epoch": 1.2117731514716439,
      "grad_norm": 0.026971304789185524,
      "learning_rate": 0.0001324665656323948,
      "loss": 0.5286,
      "step": 16880
    },
    {
      "epoch": 1.2124910265613784,
      "grad_norm": 8.074446678161621,
      "learning_rate": 0.00013241338969982187,
      "loss": 0.0569,
      "step": 16890
    },
    {
      "epoch": 1.2132089016511127,
      "grad_norm": 8.913790702819824,
      "learning_rate": 0.00013236021376724895,
      "loss": 0.1953,
      "step": 16900
    },
    {
      "epoch": 1.213926776740847,
      "grad_norm": 0.015628810971975327,
      "learning_rate": 0.00013230703783467603,
      "loss": 0.0828,
      "step": 16910
    },
    {
      "epoch": 1.2146446518305816,
      "grad_norm": 0.0805097296833992,
      "learning_rate": 0.0001322538619021031,
      "loss": 0.0392,
      "step": 16920
    },
    {
      "epoch": 1.2153625269203159,
      "grad_norm": 15.094881057739258,
      "learning_rate": 0.0001322006859695302,
      "loss": 0.2156,
      "step": 16930
    },
    {
      "epoch": 1.2160804020100502,
      "grad_norm": 3.6418399810791016,
      "learning_rate": 0.0001321475100369573,
      "loss": 0.2177,
      "step": 16940
    },
    {
      "epoch": 1.2167982770997847,
      "grad_norm": 0.09291093051433563,
      "learning_rate": 0.00013209433410438438,
      "loss": 0.2517,
      "step": 16950
    },
    {
      "epoch": 1.217516152189519,
      "grad_norm": 49.59718704223633,
      "learning_rate": 0.00013204115817181143,
      "loss": 0.3705,
      "step": 16960
    },
    {
      "epoch": 1.2182340272792533,
      "grad_norm": 2.2898366451263428,
      "learning_rate": 0.0001319879822392385,
      "loss": 0.0636,
      "step": 16970
    },
    {
      "epoch": 1.2189519023689879,
      "grad_norm": 0.018802344799041748,
      "learning_rate": 0.00013193480630666561,
      "loss": 0.2241,
      "step": 16980
    },
    {
      "epoch": 1.2196697774587222,
      "grad_norm": 4.606019496917725,
      "learning_rate": 0.0001318816303740927,
      "loss": 0.1293,
      "step": 16990
    },
    {
      "epoch": 1.2203876525484565,
      "grad_norm": 0.02874157391488552,
      "learning_rate": 0.00013182845444151977,
      "loss": 0.0743,
      "step": 17000
    },
    {
      "epoch": 1.221105527638191,
      "grad_norm": 7.1903791427612305,
      "learning_rate": 0.00013177527850894685,
      "loss": 0.0993,
      "step": 17010
    },
    {
      "epoch": 1.2218234027279253,
      "grad_norm": 0.47494277358055115,
      "learning_rate": 0.00013172210257637393,
      "loss": 0.2107,
      "step": 17020
    },
    {
      "epoch": 1.2225412778176596,
      "grad_norm": 0.22717507183551788,
      "learning_rate": 0.000131668926643801,
      "loss": 0.198,
      "step": 17030
    },
    {
      "epoch": 1.2232591529073942,
      "grad_norm": 29.060571670532227,
      "learning_rate": 0.00013161575071122812,
      "loss": 0.1568,
      "step": 17040
    },
    {
      "epoch": 1.2239770279971285,
      "grad_norm": 0.024072736501693726,
      "learning_rate": 0.0001315625747786552,
      "loss": 0.1523,
      "step": 17050
    },
    {
      "epoch": 1.2246949030868628,
      "grad_norm": 2.0022850036621094,
      "learning_rate": 0.00013150939884608228,
      "loss": 0.0799,
      "step": 17060
    },
    {
      "epoch": 1.2254127781765973,
      "grad_norm": 10.325780868530273,
      "learning_rate": 0.00013145622291350933,
      "loss": 0.1125,
      "step": 17070
    },
    {
      "epoch": 1.2261306532663316,
      "grad_norm": 0.00807139091193676,
      "learning_rate": 0.00013140304698093644,
      "loss": 0.1032,
      "step": 17080
    },
    {
      "epoch": 1.226848528356066,
      "grad_norm": 0.007592456415295601,
      "learning_rate": 0.00013134987104836352,
      "loss": 0.2049,
      "step": 17090
    },
    {
      "epoch": 1.2275664034458005,
      "grad_norm": 0.25307872891426086,
      "learning_rate": 0.0001312966951157906,
      "loss": 0.2239,
      "step": 17100
    },
    {
      "epoch": 1.2282842785355348,
      "grad_norm": 16.262861251831055,
      "learning_rate": 0.0001312435191832177,
      "loss": 0.1382,
      "step": 17110
    },
    {
      "epoch": 1.2290021536252693,
      "grad_norm": 0.12204795330762863,
      "learning_rate": 0.00013119034325064475,
      "loss": 0.1815,
      "step": 17120
    },
    {
      "epoch": 1.2297200287150036,
      "grad_norm": 2.9720659255981445,
      "learning_rate": 0.00013113716731807183,
      "loss": 0.1216,
      "step": 17130
    },
    {
      "epoch": 1.230437903804738,
      "grad_norm": 25.758197784423828,
      "learning_rate": 0.00013108399138549894,
      "loss": 0.3293,
      "step": 17140
    },
    {
      "epoch": 1.2311557788944723,
      "grad_norm": 12.509957313537598,
      "learning_rate": 0.00013103081545292602,
      "loss": 0.0904,
      "step": 17150
    },
    {
      "epoch": 1.2318736539842068,
      "grad_norm": 0.5380130410194397,
      "learning_rate": 0.0001309776395203531,
      "loss": 0.1522,
      "step": 17160
    },
    {
      "epoch": 1.232591529073941,
      "grad_norm": 19.785968780517578,
      "learning_rate": 0.00013092446358778018,
      "loss": 0.1213,
      "step": 17170
    },
    {
      "epoch": 1.2333094041636756,
      "grad_norm": 0.05918622761964798,
      "learning_rate": 0.00013087128765520726,
      "loss": 0.0666,
      "step": 17180
    },
    {
      "epoch": 1.23402727925341,
      "grad_norm": 9.123563766479492,
      "learning_rate": 0.00013081811172263434,
      "loss": 0.1649,
      "step": 17190
    },
    {
      "epoch": 1.2347451543431442,
      "grad_norm": 0.06102483347058296,
      "learning_rate": 0.00013076493579006142,
      "loss": 0.349,
      "step": 17200
    },
    {
      "epoch": 1.2354630294328786,
      "grad_norm": 2.533071756362915,
      "learning_rate": 0.00013071175985748852,
      "loss": 0.0559,
      "step": 17210
    },
    {
      "epoch": 1.236180904522613,
      "grad_norm": 28.062707901000977,
      "learning_rate": 0.00013065858392491558,
      "loss": 0.4909,
      "step": 17220
    },
    {
      "epoch": 1.2368987796123474,
      "grad_norm": 4.744726181030273,
      "learning_rate": 0.00013060540799234266,
      "loss": 0.1558,
      "step": 17230
    },
    {
      "epoch": 1.237616654702082,
      "grad_norm": 0.06848500669002533,
      "learning_rate": 0.00013055223205976976,
      "loss": 0.1539,
      "step": 17240
    },
    {
      "epoch": 1.2383345297918162,
      "grad_norm": 1.6669812202453613,
      "learning_rate": 0.00013049905612719684,
      "loss": 0.2528,
      "step": 17250
    },
    {
      "epoch": 1.2390524048815506,
      "grad_norm": 0.21413283050060272,
      "learning_rate": 0.00013044588019462392,
      "loss": 0.1648,
      "step": 17260
    },
    {
      "epoch": 1.239770279971285,
      "grad_norm": 0.028526276350021362,
      "learning_rate": 0.000130392704262051,
      "loss": 0.2066,
      "step": 17270
    },
    {
      "epoch": 1.2404881550610194,
      "grad_norm": 0.030755598098039627,
      "learning_rate": 0.00013033952832947808,
      "loss": 0.2976,
      "step": 17280
    },
    {
      "epoch": 1.2412060301507537,
      "grad_norm": 14.634062767028809,
      "learning_rate": 0.00013028635239690516,
      "loss": 0.3248,
      "step": 17290
    },
    {
      "epoch": 1.2419239052404882,
      "grad_norm": 10.857305526733398,
      "learning_rate": 0.00013023317646433224,
      "loss": 0.4501,
      "step": 17300
    },
    {
      "epoch": 1.2426417803302225,
      "grad_norm": 10.576329231262207,
      "learning_rate": 0.00013018000053175935,
      "loss": 0.1235,
      "step": 17310
    },
    {
      "epoch": 1.2433596554199569,
      "grad_norm": 3.115358591079712,
      "learning_rate": 0.00013012682459918643,
      "loss": 0.1079,
      "step": 17320
    },
    {
      "epoch": 1.2440775305096914,
      "grad_norm": 0.36753472685813904,
      "learning_rate": 0.00013007364866661348,
      "loss": 0.0338,
      "step": 17330
    },
    {
      "epoch": 1.2447954055994257,
      "grad_norm": 0.0029768329113721848,
      "learning_rate": 0.00013002047273404058,
      "loss": 0.155,
      "step": 17340
    },
    {
      "epoch": 1.24551328068916,
      "grad_norm": 1.4515752792358398,
      "learning_rate": 0.00012996729680146766,
      "loss": 0.2016,
      "step": 17350
    },
    {
      "epoch": 1.2462311557788945,
      "grad_norm": 15.895071029663086,
      "learning_rate": 0.00012991412086889474,
      "loss": 0.1136,
      "step": 17360
    },
    {
      "epoch": 1.2469490308686288,
      "grad_norm": 15.591389656066895,
      "learning_rate": 0.00012986094493632185,
      "loss": 0.3773,
      "step": 17370
    },
    {
      "epoch": 1.2476669059583632,
      "grad_norm": 0.03071129508316517,
      "learning_rate": 0.0001298077690037489,
      "loss": 0.0726,
      "step": 17380
    },
    {
      "epoch": 1.2483847810480977,
      "grad_norm": 7.3628129959106445,
      "learning_rate": 0.00012975459307117598,
      "loss": 0.2547,
      "step": 17390
    },
    {
      "epoch": 1.249102656137832,
      "grad_norm": 0.23702310025691986,
      "learning_rate": 0.00012970141713860306,
      "loss": 0.3781,
      "step": 17400
    },
    {
      "epoch": 1.2498205312275663,
      "grad_norm": 0.04337414354085922,
      "learning_rate": 0.00012964824120603017,
      "loss": 0.1366,
      "step": 17410
    },
    {
      "epoch": 1.2505384063173008,
      "grad_norm": 7.26453971862793,
      "learning_rate": 0.00012959506527345725,
      "loss": 0.1431,
      "step": 17420
    },
    {
      "epoch": 1.2512562814070352,
      "grad_norm": 12.141316413879395,
      "learning_rate": 0.00012954188934088433,
      "loss": 0.3522,
      "step": 17430
    },
    {
      "epoch": 1.2519741564967695,
      "grad_norm": 0.04336532950401306,
      "learning_rate": 0.0001294887134083114,
      "loss": 0.0877,
      "step": 17440
    },
    {
      "epoch": 1.252692031586504,
      "grad_norm": 2.865699291229248,
      "learning_rate": 0.00012943553747573849,
      "loss": 0.1529,
      "step": 17450
    },
    {
      "epoch": 1.2534099066762383,
      "grad_norm": 30.941547393798828,
      "learning_rate": 0.00012938236154316557,
      "loss": 0.1783,
      "step": 17460
    },
    {
      "epoch": 1.2541277817659728,
      "grad_norm": 7.441716194152832,
      "learning_rate": 0.00012932918561059267,
      "loss": 0.026,
      "step": 17470
    },
    {
      "epoch": 1.2548456568557071,
      "grad_norm": 6.913032531738281,
      "learning_rate": 0.00012927600967801972,
      "loss": 0.2336,
      "step": 17480
    },
    {
      "epoch": 1.2555635319454415,
      "grad_norm": 2.6526360511779785,
      "learning_rate": 0.0001292228337454468,
      "loss": 0.2978,
      "step": 17490
    },
    {
      "epoch": 1.2562814070351758,
      "grad_norm": 0.9236118793487549,
      "learning_rate": 0.00012916965781287388,
      "loss": 0.1246,
      "step": 17500
    },
    {
      "epoch": 1.2569992821249103,
      "grad_norm": 0.39473479986190796,
      "learning_rate": 0.000129116481880301,
      "loss": 0.1425,
      "step": 17510
    },
    {
      "epoch": 1.2577171572146446,
      "grad_norm": 12.222085952758789,
      "learning_rate": 0.00012906330594772807,
      "loss": 0.1157,
      "step": 17520
    },
    {
      "epoch": 1.2584350323043791,
      "grad_norm": 5.608402729034424,
      "learning_rate": 0.00012901013001515515,
      "loss": 0.3571,
      "step": 17530
    },
    {
      "epoch": 1.2591529073941135,
      "grad_norm": 1.160805106163025,
      "learning_rate": 0.00012895695408258223,
      "loss": 0.2653,
      "step": 17540
    },
    {
      "epoch": 1.2598707824838478,
      "grad_norm": 0.0230579674243927,
      "learning_rate": 0.0001289037781500093,
      "loss": 0.2397,
      "step": 17550
    },
    {
      "epoch": 1.260588657573582,
      "grad_norm": 27.180273056030273,
      "learning_rate": 0.0001288506022174364,
      "loss": 0.3002,
      "step": 17560
    },
    {
      "epoch": 1.2613065326633166,
      "grad_norm": 8.611339569091797,
      "learning_rate": 0.0001287974262848635,
      "loss": 0.1515,
      "step": 17570
    },
    {
      "epoch": 1.262024407753051,
      "grad_norm": 0.015222049318253994,
      "learning_rate": 0.00012874425035229057,
      "loss": 0.0656,
      "step": 17580
    },
    {
      "epoch": 1.2627422828427854,
      "grad_norm": 3.0105996131896973,
      "learning_rate": 0.00012869107441971763,
      "loss": 0.0601,
      "step": 17590
    },
    {
      "epoch": 1.2634601579325198,
      "grad_norm": 0.07212084531784058,
      "learning_rate": 0.0001286378984871447,
      "loss": 0.0667,
      "step": 17600
    },
    {
      "epoch": 1.264178033022254,
      "grad_norm": 0.037943676114082336,
      "learning_rate": 0.0001285847225545718,
      "loss": 0.0886,
      "step": 17610
    },
    {
      "epoch": 1.2648959081119884,
      "grad_norm": 0.2730564773082733,
      "learning_rate": 0.0001285315466219989,
      "loss": 0.1563,
      "step": 17620
    },
    {
      "epoch": 1.265613783201723,
      "grad_norm": 17.126928329467773,
      "learning_rate": 0.00012847837068942597,
      "loss": 0.1291,
      "step": 17630
    },
    {
      "epoch": 1.2663316582914572,
      "grad_norm": 7.622262954711914,
      "learning_rate": 0.00012842519475685305,
      "loss": 0.0497,
      "step": 17640
    },
    {
      "epoch": 1.2670495333811918,
      "grad_norm": 0.18258683383464813,
      "learning_rate": 0.00012837201882428013,
      "loss": 0.0582,
      "step": 17650
    },
    {
      "epoch": 1.267767408470926,
      "grad_norm": 0.21307803690433502,
      "learning_rate": 0.0001283188428917072,
      "loss": 0.3296,
      "step": 17660
    },
    {
      "epoch": 1.2684852835606604,
      "grad_norm": 31.264524459838867,
      "learning_rate": 0.00012826566695913432,
      "loss": 0.3886,
      "step": 17670
    },
    {
      "epoch": 1.269203158650395,
      "grad_norm": 1.2359238862991333,
      "learning_rate": 0.0001282124910265614,
      "loss": 0.1254,
      "step": 17680
    },
    {
      "epoch": 1.2699210337401292,
      "grad_norm": 9.834798812866211,
      "learning_rate": 0.00012815931509398848,
      "loss": 0.3592,
      "step": 17690
    },
    {
      "epoch": 1.2706389088298635,
      "grad_norm": 0.057643041014671326,
      "learning_rate": 0.00012810613916141553,
      "loss": 0.1569,
      "step": 17700
    },
    {
      "epoch": 1.271356783919598,
      "grad_norm": 3.168907880783081,
      "learning_rate": 0.00012805296322884263,
      "loss": 0.1693,
      "step": 17710
    },
    {
      "epoch": 1.2720746590093324,
      "grad_norm": 0.0137795927003026,
      "learning_rate": 0.00012799978729626971,
      "loss": 0.2535,
      "step": 17720
    },
    {
      "epoch": 1.2727925340990667,
      "grad_norm": 0.08410152792930603,
      "learning_rate": 0.0001279466113636968,
      "loss": 0.1671,
      "step": 17730
    },
    {
      "epoch": 1.2735104091888012,
      "grad_norm": 5.024313926696777,
      "learning_rate": 0.0001278934354311239,
      "loss": 0.1946,
      "step": 17740
    },
    {
      "epoch": 1.2742282842785355,
      "grad_norm": 16.48325538635254,
      "learning_rate": 0.00012784025949855095,
      "loss": 0.4151,
      "step": 17750
    },
    {
      "epoch": 1.27494615936827,
      "grad_norm": 0.365260511636734,
      "learning_rate": 0.00012778708356597803,
      "loss": 0.0262,
      "step": 17760
    },
    {
      "epoch": 1.2756640344580044,
      "grad_norm": 16.109323501586914,
      "learning_rate": 0.00012773390763340514,
      "loss": 0.4389,
      "step": 17770
    },
    {
      "epoch": 1.2763819095477387,
      "grad_norm": 25.837909698486328,
      "learning_rate": 0.00012768073170083222,
      "loss": 0.1157,
      "step": 17780
    },
    {
      "epoch": 1.277099784637473,
      "grad_norm": 0.48817163705825806,
      "learning_rate": 0.0001276275557682593,
      "loss": 0.0819,
      "step": 17790
    },
    {
      "epoch": 1.2778176597272075,
      "grad_norm": 0.021467192098498344,
      "learning_rate": 0.00012757437983568638,
      "loss": 0.0774,
      "step": 17800
    },
    {
      "epoch": 1.2785355348169418,
      "grad_norm": 0.00406024930998683,
      "learning_rate": 0.00012752120390311346,
      "loss": 0.1911,
      "step": 17810
    },
    {
      "epoch": 1.2792534099066764,
      "grad_norm": 0.031278789043426514,
      "learning_rate": 0.00012746802797054054,
      "loss": 0.1659,
      "step": 17820
    },
    {
      "epoch": 1.2799712849964107,
      "grad_norm": 0.7053841948509216,
      "learning_rate": 0.00012741485203796762,
      "loss": 0.1135,
      "step": 17830
    },
    {
      "epoch": 1.280689160086145,
      "grad_norm": 11.035269737243652,
      "learning_rate": 0.00012736167610539472,
      "loss": 0.1494,
      "step": 17840
    },
    {
      "epoch": 1.2814070351758793,
      "grad_norm": 5.141314506530762,
      "learning_rate": 0.00012730850017282177,
      "loss": 0.1631,
      "step": 17850
    },
    {
      "epoch": 1.2821249102656138,
      "grad_norm": 0.21997928619384766,
      "learning_rate": 0.00012725532424024885,
      "loss": 0.0856,
      "step": 17860
    },
    {
      "epoch": 1.2828427853553481,
      "grad_norm": 21.91972541809082,
      "learning_rate": 0.00012720214830767596,
      "loss": 0.281,
      "step": 17870
    },
    {
      "epoch": 1.2835606604450827,
      "grad_norm": 16.694406509399414,
      "learning_rate": 0.00012714897237510304,
      "loss": 0.1113,
      "step": 17880
    },
    {
      "epoch": 1.284278535534817,
      "grad_norm": 0.014825722202658653,
      "learning_rate": 0.00012709579644253012,
      "loss": 0.2766,
      "step": 17890
    },
    {
      "epoch": 1.2849964106245513,
      "grad_norm": 4.441236972808838,
      "learning_rate": 0.0001270426205099572,
      "loss": 0.0796,
      "step": 17900
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 5.755829334259033,
      "learning_rate": 0.00012698944457738428,
      "loss": 0.2166,
      "step": 17910
    },
    {
      "epoch": 1.2864321608040201,
      "grad_norm": 8.172782897949219,
      "learning_rate": 0.00012693626864481136,
      "loss": 0.2097,
      "step": 17920
    },
    {
      "epoch": 1.2871500358937544,
      "grad_norm": 0.21991752088069916,
      "learning_rate": 0.00012688309271223844,
      "loss": 0.1975,
      "step": 17930
    },
    {
      "epoch": 1.287867910983489,
      "grad_norm": 0.247788205742836,
      "learning_rate": 0.00012682991677966554,
      "loss": 0.096,
      "step": 17940
    },
    {
      "epoch": 1.2885857860732233,
      "grad_norm": 0.05818350985646248,
      "learning_rate": 0.00012677674084709262,
      "loss": 0.2561,
      "step": 17950
    },
    {
      "epoch": 1.2893036611629576,
      "grad_norm": 26.948720932006836,
      "learning_rate": 0.00012672356491451968,
      "loss": 0.2387,
      "step": 17960
    },
    {
      "epoch": 1.2900215362526921,
      "grad_norm": 0.4291504919528961,
      "learning_rate": 0.00012667038898194678,
      "loss": 0.0797,
      "step": 17970
    },
    {
      "epoch": 1.2907394113424264,
      "grad_norm": 0.37314748764038086,
      "learning_rate": 0.00012661721304937386,
      "loss": 0.0489,
      "step": 17980
    },
    {
      "epoch": 1.2914572864321607,
      "grad_norm": 27.68294906616211,
      "learning_rate": 0.00012656403711680094,
      "loss": 0.3017,
      "step": 17990
    },
    {
      "epoch": 1.2921751615218953,
      "grad_norm": 1.6668332815170288,
      "learning_rate": 0.00012651086118422802,
      "loss": 0.1888,
      "step": 18000
    },
    {
      "epoch": 1.2928930366116296,
      "grad_norm": 9.017914772033691,
      "learning_rate": 0.0001264576852516551,
      "loss": 0.1548,
      "step": 18010
    },
    {
      "epoch": 1.2936109117013639,
      "grad_norm": 17.274633407592773,
      "learning_rate": 0.00012640450931908218,
      "loss": 0.2177,
      "step": 18020
    },
    {
      "epoch": 1.2943287867910984,
      "grad_norm": 0.07330209016799927,
      "learning_rate": 0.00012635133338650926,
      "loss": 0.1263,
      "step": 18030
    },
    {
      "epoch": 1.2950466618808327,
      "grad_norm": 0.09424925595521927,
      "learning_rate": 0.00012629815745393637,
      "loss": 0.1022,
      "step": 18040
    },
    {
      "epoch": 1.2957645369705673,
      "grad_norm": 11.061662673950195,
      "learning_rate": 0.00012624498152136345,
      "loss": 0.312,
      "step": 18050
    },
    {
      "epoch": 1.2964824120603016,
      "grad_norm": 11.86251163482666,
      "learning_rate": 0.00012619180558879053,
      "loss": 0.3255,
      "step": 18060
    },
    {
      "epoch": 1.2972002871500359,
      "grad_norm": 16.257617950439453,
      "learning_rate": 0.0001261386296562176,
      "loss": 0.3593,
      "step": 18070
    },
    {
      "epoch": 1.2979181622397702,
      "grad_norm": 10.106926918029785,
      "learning_rate": 0.00012608545372364468,
      "loss": 0.1508,
      "step": 18080
    },
    {
      "epoch": 1.2986360373295047,
      "grad_norm": 1.1019412279129028,
      "learning_rate": 0.00012603227779107176,
      "loss": 0.1192,
      "step": 18090
    },
    {
      "epoch": 1.299353912419239,
      "grad_norm": 0.0035913472529500723,
      "learning_rate": 0.00012597910185849887,
      "loss": 0.1139,
      "step": 18100
    },
    {
      "epoch": 1.3000717875089736,
      "grad_norm": 0.04637708514928818,
      "learning_rate": 0.00012592592592592592,
      "loss": 0.0845,
      "step": 18110
    },
    {
      "epoch": 1.3007896625987079,
      "grad_norm": 8.082785606384277,
      "learning_rate": 0.000125872749993353,
      "loss": 0.1387,
      "step": 18120
    },
    {
      "epoch": 1.3015075376884422,
      "grad_norm": 1.4608798027038574,
      "learning_rate": 0.00012581957406078008,
      "loss": 0.2382,
      "step": 18130
    },
    {
      "epoch": 1.3022254127781765,
      "grad_norm": 15.470115661621094,
      "learning_rate": 0.0001257663981282072,
      "loss": 0.0532,
      "step": 18140
    },
    {
      "epoch": 1.302943287867911,
      "grad_norm": 1.57004714012146,
      "learning_rate": 0.00012571322219563427,
      "loss": 0.1967,
      "step": 18150
    },
    {
      "epoch": 1.3036611629576453,
      "grad_norm": 0.9379510879516602,
      "learning_rate": 0.00012566004626306135,
      "loss": 0.1164,
      "step": 18160
    },
    {
      "epoch": 1.3043790380473799,
      "grad_norm": 0.026501929387450218,
      "learning_rate": 0.00012560687033048843,
      "loss": 0.1059,
      "step": 18170
    },
    {
      "epoch": 1.3050969131371142,
      "grad_norm": 18.84992790222168,
      "learning_rate": 0.0001255536943979155,
      "loss": 0.2306,
      "step": 18180
    },
    {
      "epoch": 1.3058147882268485,
      "grad_norm": 0.010143906809389591,
      "learning_rate": 0.00012550051846534259,
      "loss": 0.099,
      "step": 18190
    },
    {
      "epoch": 1.3065326633165828,
      "grad_norm": 0.737114667892456,
      "learning_rate": 0.0001254473425327697,
      "loss": 0.1751,
      "step": 18200
    },
    {
      "epoch": 1.3072505384063173,
      "grad_norm": 0.018062040209770203,
      "learning_rate": 0.00012539416660019677,
      "loss": 0.0724,
      "step": 18210
    },
    {
      "epoch": 1.3079684134960516,
      "grad_norm": 0.1774401217699051,
      "learning_rate": 0.00012534099066762382,
      "loss": 0.4045,
      "step": 18220
    },
    {
      "epoch": 1.3086862885857862,
      "grad_norm": 13.694185256958008,
      "learning_rate": 0.0001252878147350509,
      "loss": 0.2682,
      "step": 18230
    },
    {
      "epoch": 1.3094041636755205,
      "grad_norm": 2.1807408332824707,
      "learning_rate": 0.000125234638802478,
      "loss": 0.236,
      "step": 18240
    },
    {
      "epoch": 1.3101220387652548,
      "grad_norm": 12.14421272277832,
      "learning_rate": 0.0001251814628699051,
      "loss": 0.0852,
      "step": 18250
    },
    {
      "epoch": 1.310839913854989,
      "grad_norm": 5.37691593170166,
      "learning_rate": 0.00012512828693733217,
      "loss": 0.1989,
      "step": 18260
    },
    {
      "epoch": 1.3115577889447236,
      "grad_norm": 8.630528450012207,
      "learning_rate": 0.00012507511100475925,
      "loss": 0.0851,
      "step": 18270
    },
    {
      "epoch": 1.312275664034458,
      "grad_norm": 0.05007617920637131,
      "learning_rate": 0.00012502193507218633,
      "loss": 0.0704,
      "step": 18280
    },
    {
      "epoch": 1.3129935391241925,
      "grad_norm": 0.2844602167606354,
      "learning_rate": 0.0001249687591396134,
      "loss": 0.4791,
      "step": 18290
    },
    {
      "epoch": 1.3137114142139268,
      "grad_norm": 0.005280026234686375,
      "learning_rate": 0.00012491558320704051,
      "loss": 0.1124,
      "step": 18300
    },
    {
      "epoch": 1.314429289303661,
      "grad_norm": 0.0904691144824028,
      "learning_rate": 0.0001248624072744676,
      "loss": 0.478,
      "step": 18310
    },
    {
      "epoch": 1.3151471643933956,
      "grad_norm": 0.9599155187606812,
      "learning_rate": 0.00012480923134189467,
      "loss": 0.0382,
      "step": 18320
    },
    {
      "epoch": 1.31586503948313,
      "grad_norm": 2.5168216228485107,
      "learning_rate": 0.00012475605540932173,
      "loss": 0.0102,
      "step": 18330
    },
    {
      "epoch": 1.3165829145728642,
      "grad_norm": 0.20732644200325012,
      "learning_rate": 0.00012470287947674883,
      "loss": 0.0749,
      "step": 18340
    },
    {
      "epoch": 1.3173007896625988,
      "grad_norm": 1.0000457763671875,
      "learning_rate": 0.0001246497035441759,
      "loss": 0.1456,
      "step": 18350
    },
    {
      "epoch": 1.318018664752333,
      "grad_norm": 0.009975004941225052,
      "learning_rate": 0.000124596527611603,
      "loss": 0.0243,
      "step": 18360
    },
    {
      "epoch": 1.3187365398420674,
      "grad_norm": 1.4709640741348267,
      "learning_rate": 0.0001245433516790301,
      "loss": 0.1255,
      "step": 18370
    },
    {
      "epoch": 1.319454414931802,
      "grad_norm": 12.17270278930664,
      "learning_rate": 0.00012449017574645715,
      "loss": 0.2335,
      "step": 18380
    },
    {
      "epoch": 1.3201722900215362,
      "grad_norm": 19.318910598754883,
      "learning_rate": 0.00012443699981388423,
      "loss": 0.1949,
      "step": 18390
    },
    {
      "epoch": 1.3208901651112708,
      "grad_norm": 16.488264083862305,
      "learning_rate": 0.00012438382388131134,
      "loss": 0.287,
      "step": 18400
    },
    {
      "epoch": 1.321608040201005,
      "grad_norm": 2.7746787071228027,
      "learning_rate": 0.00012433064794873842,
      "loss": 0.1497,
      "step": 18410
    },
    {
      "epoch": 1.3223259152907394,
      "grad_norm": 31.60505485534668,
      "learning_rate": 0.0001242774720161655,
      "loss": 0.3424,
      "step": 18420
    },
    {
      "epoch": 1.3230437903804737,
      "grad_norm": 1.2111295461654663,
      "learning_rate": 0.00012422429608359258,
      "loss": 0.077,
      "step": 18430
    },
    {
      "epoch": 1.3237616654702082,
      "grad_norm": 0.1292610615491867,
      "learning_rate": 0.00012417112015101965,
      "loss": 0.044,
      "step": 18440
    },
    {
      "epoch": 1.3244795405599425,
      "grad_norm": 15.505657196044922,
      "learning_rate": 0.00012411794421844673,
      "loss": 0.2161,
      "step": 18450
    },
    {
      "epoch": 1.325197415649677,
      "grad_norm": 0.042271316051483154,
      "learning_rate": 0.00012406476828587381,
      "loss": 0.1494,
      "step": 18460
    },
    {
      "epoch": 1.3259152907394114,
      "grad_norm": 17.90487289428711,
      "learning_rate": 0.00012401159235330092,
      "loss": 0.1225,
      "step": 18470
    },
    {
      "epoch": 1.3266331658291457,
      "grad_norm": 23.97334098815918,
      "learning_rate": 0.00012395841642072797,
      "loss": 0.1761,
      "step": 18480
    },
    {
      "epoch": 1.32735104091888,
      "grad_norm": 0.2950870990753174,
      "learning_rate": 0.00012390524048815505,
      "loss": 0.0442,
      "step": 18490
    },
    {
      "epoch": 1.3280689160086145,
      "grad_norm": 0.1342005431652069,
      "learning_rate": 0.00012385206455558216,
      "loss": 0.1128,
      "step": 18500
    },
    {
      "epoch": 1.3287867910983489,
      "grad_norm": 18.711275100708008,
      "learning_rate": 0.00012379888862300924,
      "loss": 0.0714,
      "step": 18510
    },
    {
      "epoch": 1.3295046661880834,
      "grad_norm": 0.08402568101882935,
      "learning_rate": 0.00012374571269043632,
      "loss": 0.1589,
      "step": 18520
    },
    {
      "epoch": 1.3302225412778177,
      "grad_norm": 15.749418258666992,
      "learning_rate": 0.0001236925367578634,
      "loss": 0.175,
      "step": 18530
    },
    {
      "epoch": 1.330940416367552,
      "grad_norm": 24.70121955871582,
      "learning_rate": 0.00012363936082529048,
      "loss": 0.3734,
      "step": 18540
    },
    {
      "epoch": 1.3316582914572863,
      "grad_norm": 0.017232432961463928,
      "learning_rate": 0.00012358618489271756,
      "loss": 0.1528,
      "step": 18550
    },
    {
      "epoch": 1.3323761665470208,
      "grad_norm": 5.694661617279053,
      "learning_rate": 0.00012353300896014464,
      "loss": 0.2031,
      "step": 18560
    },
    {
      "epoch": 1.3330940416367552,
      "grad_norm": 1.3918519020080566,
      "learning_rate": 0.00012347983302757174,
      "loss": 0.2381,
      "step": 18570
    },
    {
      "epoch": 1.3338119167264897,
      "grad_norm": 0.03315357863903046,
      "learning_rate": 0.00012342665709499882,
      "loss": 0.0516,
      "step": 18580
    },
    {
      "epoch": 1.334529791816224,
      "grad_norm": 5.830288887023926,
      "learning_rate": 0.00012337348116242587,
      "loss": 0.1964,
      "step": 18590
    },
    {
      "epoch": 1.3352476669059583,
      "grad_norm": 10.060792922973633,
      "learning_rate": 0.00012332030522985298,
      "loss": 0.1538,
      "step": 18600
    },
    {
      "epoch": 1.3359655419956926,
      "grad_norm": 13.813641548156738,
      "learning_rate": 0.00012326712929728006,
      "loss": 0.2858,
      "step": 18610
    },
    {
      "epoch": 1.3366834170854272,
      "grad_norm": 10.46837043762207,
      "learning_rate": 0.00012321395336470714,
      "loss": 0.3953,
      "step": 18620
    },
    {
      "epoch": 1.3374012921751615,
      "grad_norm": 0.5560712814331055,
      "learning_rate": 0.00012316077743213422,
      "loss": 0.1083,
      "step": 18630
    },
    {
      "epoch": 1.338119167264896,
      "grad_norm": 0.47778576612472534,
      "learning_rate": 0.0001231076014995613,
      "loss": 0.2148,
      "step": 18640
    },
    {
      "epoch": 1.3388370423546303,
      "grad_norm": 0.7217497825622559,
      "learning_rate": 0.00012305442556698838,
      "loss": 0.3321,
      "step": 18650
    },
    {
      "epoch": 1.3395549174443646,
      "grad_norm": 25.940906524658203,
      "learning_rate": 0.00012300124963441546,
      "loss": 0.3268,
      "step": 18660
    },
    {
      "epoch": 1.3402727925340991,
      "grad_norm": 1.6996650695800781,
      "learning_rate": 0.00012294807370184256,
      "loss": 0.1495,
      "step": 18670
    },
    {
      "epoch": 1.3409906676238335,
      "grad_norm": 0.00513342022895813,
      "learning_rate": 0.00012289489776926964,
      "loss": 0.181,
      "step": 18680
    },
    {
      "epoch": 1.3417085427135678,
      "grad_norm": 0.1135614886879921,
      "learning_rate": 0.00012284172183669672,
      "loss": 0.1996,
      "step": 18690
    },
    {
      "epoch": 1.3424264178033023,
      "grad_norm": 2.1189348697662354,
      "learning_rate": 0.0001227885459041238,
      "loss": 0.1679,
      "step": 18700
    },
    {
      "epoch": 1.3431442928930366,
      "grad_norm": 13.394584655761719,
      "learning_rate": 0.00012273536997155088,
      "loss": 0.1077,
      "step": 18710
    },
    {
      "epoch": 1.343862167982771,
      "grad_norm": 2.996478319168091,
      "learning_rate": 0.00012268219403897796,
      "loss": 0.2507,
      "step": 18720
    },
    {
      "epoch": 1.3445800430725054,
      "grad_norm": 10.133096694946289,
      "learning_rate": 0.00012262901810640507,
      "loss": 0.2033,
      "step": 18730
    },
    {
      "epoch": 1.3452979181622398,
      "grad_norm": 31.34398078918457,
      "learning_rate": 0.00012257584217383212,
      "loss": 0.3127,
      "step": 18740
    },
    {
      "epoch": 1.3460157932519743,
      "grad_norm": 0.6935137510299683,
      "learning_rate": 0.0001225226662412592,
      "loss": 0.3061,
      "step": 18750
    },
    {
      "epoch": 1.3467336683417086,
      "grad_norm": 1.1108559370040894,
      "learning_rate": 0.00012246949030868628,
      "loss": 0.0919,
      "step": 18760
    },
    {
      "epoch": 1.347451543431443,
      "grad_norm": 5.95393180847168,
      "learning_rate": 0.0001224163143761134,
      "loss": 0.0944,
      "step": 18770
    },
    {
      "epoch": 1.3481694185211772,
      "grad_norm": 19.491701126098633,
      "learning_rate": 0.00012236313844354047,
      "loss": 0.1411,
      "step": 18780
    },
    {
      "epoch": 1.3488872936109118,
      "grad_norm": 0.0024668762926012278,
      "learning_rate": 0.00012230996251096755,
      "loss": 0.087,
      "step": 18790
    },
    {
      "epoch": 1.349605168700646,
      "grad_norm": 17.679412841796875,
      "learning_rate": 0.00012225678657839463,
      "loss": 0.2149,
      "step": 18800
    },
    {
      "epoch": 1.3503230437903806,
      "grad_norm": 3.0808064937591553,
      "learning_rate": 0.0001222036106458217,
      "loss": 0.059,
      "step": 18810
    },
    {
      "epoch": 1.351040918880115,
      "grad_norm": 0.020040543749928474,
      "learning_rate": 0.00012215043471324878,
      "loss": 0.3995,
      "step": 18820
    },
    {
      "epoch": 1.3517587939698492,
      "grad_norm": 0.013394520618021488,
      "learning_rate": 0.0001220972587806759,
      "loss": 0.0774,
      "step": 18830
    },
    {
      "epoch": 1.3524766690595835,
      "grad_norm": 0.2399880290031433,
      "learning_rate": 0.00012204408284810296,
      "loss": 0.3388,
      "step": 18840
    },
    {
      "epoch": 1.353194544149318,
      "grad_norm": 0.15576618909835815,
      "learning_rate": 0.00012199090691553004,
      "loss": 0.0974,
      "step": 18850
    },
    {
      "epoch": 1.3539124192390524,
      "grad_norm": 14.866196632385254,
      "learning_rate": 0.00012193773098295712,
      "loss": 0.1611,
      "step": 18860
    },
    {
      "epoch": 1.354630294328787,
      "grad_norm": 0.8413169980049133,
      "learning_rate": 0.00012188455505038421,
      "loss": 0.0929,
      "step": 18870
    },
    {
      "epoch": 1.3553481694185212,
      "grad_norm": 8.89681625366211,
      "learning_rate": 0.00012183137911781129,
      "loss": 0.1167,
      "step": 18880
    },
    {
      "epoch": 1.3560660445082555,
      "grad_norm": 0.0638456791639328,
      "learning_rate": 0.00012177820318523835,
      "loss": 0.4894,
      "step": 18890
    },
    {
      "epoch": 1.3567839195979898,
      "grad_norm": 7.743799209594727,
      "learning_rate": 0.00012172502725266546,
      "loss": 0.1198,
      "step": 18900
    },
    {
      "epoch": 1.3575017946877244,
      "grad_norm": 3.6257901191711426,
      "learning_rate": 0.00012167185132009254,
      "loss": 0.2624,
      "step": 18910
    },
    {
      "epoch": 1.3582196697774587,
      "grad_norm": 0.02367318421602249,
      "learning_rate": 0.0001216186753875196,
      "loss": 0.3537,
      "step": 18920
    },
    {
      "epoch": 1.3589375448671932,
      "grad_norm": 0.5878052711486816,
      "learning_rate": 0.00012156549945494671,
      "loss": 0.1106,
      "step": 18930
    },
    {
      "epoch": 1.3596554199569275,
      "grad_norm": 1.2981797456741333,
      "learning_rate": 0.00012151232352237378,
      "loss": 0.1753,
      "step": 18940
    },
    {
      "epoch": 1.3603732950466618,
      "grad_norm": 0.001529415138065815,
      "learning_rate": 0.00012145914758980086,
      "loss": 0.092,
      "step": 18950
    },
    {
      "epoch": 1.3610911701363961,
      "grad_norm": 0.009146220050752163,
      "learning_rate": 0.00012140597165722794,
      "loss": 0.1376,
      "step": 18960
    },
    {
      "epoch": 1.3618090452261307,
      "grad_norm": 8.921772956848145,
      "learning_rate": 0.00012135279572465503,
      "loss": 0.4617,
      "step": 18970
    },
    {
      "epoch": 1.362526920315865,
      "grad_norm": 0.002036159625276923,
      "learning_rate": 0.00012129961979208211,
      "loss": 0.0629,
      "step": 18980
    },
    {
      "epoch": 1.3632447954055995,
      "grad_norm": 0.30463263392448425,
      "learning_rate": 0.00012124644385950919,
      "loss": 0.0717,
      "step": 18990
    },
    {
      "epoch": 1.3639626704953338,
      "grad_norm": 0.2508198022842407,
      "learning_rate": 0.00012119326792693628,
      "loss": 0.0494,
      "step": 19000
    },
    {
      "epoch": 1.3646805455850681,
      "grad_norm": 0.21458867192268372,
      "learning_rate": 0.00012114009199436336,
      "loss": 0.1991,
      "step": 19010
    },
    {
      "epoch": 1.3653984206748027,
      "grad_norm": 7.931280612945557,
      "learning_rate": 0.00012108691606179043,
      "loss": 0.3453,
      "step": 19020
    },
    {
      "epoch": 1.366116295764537,
      "grad_norm": 14.061219215393066,
      "learning_rate": 0.00012103374012921753,
      "loss": 0.1176,
      "step": 19030
    },
    {
      "epoch": 1.3668341708542713,
      "grad_norm": 42.8404541015625,
      "learning_rate": 0.00012098056419664461,
      "loss": 0.0915,
      "step": 19040
    },
    {
      "epoch": 1.3675520459440058,
      "grad_norm": 9.174962043762207,
      "learning_rate": 0.00012092738826407168,
      "loss": 0.1987,
      "step": 19050
    },
    {
      "epoch": 1.3682699210337401,
      "grad_norm": 0.011793933808803558,
      "learning_rate": 0.00012087421233149876,
      "loss": 0.0652,
      "step": 19060
    },
    {
      "epoch": 1.3689877961234744,
      "grad_norm": 18.29448127746582,
      "learning_rate": 0.00012082103639892585,
      "loss": 0.3143,
      "step": 19070
    },
    {
      "epoch": 1.369705671213209,
      "grad_norm": 0.2822072505950928,
      "learning_rate": 0.00012076786046635293,
      "loss": 0.1627,
      "step": 19080
    },
    {
      "epoch": 1.3704235463029433,
      "grad_norm": 16.586803436279297,
      "learning_rate": 0.00012071468453378001,
      "loss": 0.3419,
      "step": 19090
    },
    {
      "epoch": 1.3711414213926778,
      "grad_norm": 0.0017295698635280132,
      "learning_rate": 0.0001206615086012071,
      "loss": 0.1105,
      "step": 19100
    },
    {
      "epoch": 1.3718592964824121,
      "grad_norm": 0.008932271040976048,
      "learning_rate": 0.00012060833266863418,
      "loss": 0.0575,
      "step": 19110
    },
    {
      "epoch": 1.3725771715721464,
      "grad_norm": 3.98223876953125,
      "learning_rate": 0.00012055515673606126,
      "loss": 0.2205,
      "step": 19120
    },
    {
      "epoch": 1.3732950466618807,
      "grad_norm": 0.0067774392664432526,
      "learning_rate": 0.00012050198080348836,
      "loss": 0.251,
      "step": 19130
    },
    {
      "epoch": 1.3740129217516153,
      "grad_norm": 11.01003646850586,
      "learning_rate": 0.00012044880487091544,
      "loss": 0.1229,
      "step": 19140
    },
    {
      "epoch": 1.3747307968413496,
      "grad_norm": 7.691339015960693,
      "learning_rate": 0.0001203956289383425,
      "loss": 0.3188,
      "step": 19150
    },
    {
      "epoch": 1.375448671931084,
      "grad_norm": 0.06532978266477585,
      "learning_rate": 0.00012034245300576958,
      "loss": 0.2005,
      "step": 19160
    },
    {
      "epoch": 1.3761665470208184,
      "grad_norm": 10.320684432983398,
      "learning_rate": 0.00012028927707319669,
      "loss": 0.1826,
      "step": 19170
    },
    {
      "epoch": 1.3768844221105527,
      "grad_norm": 15.958524703979492,
      "learning_rate": 0.00012023610114062375,
      "loss": 0.1944,
      "step": 19180
    },
    {
      "epoch": 1.377602297200287,
      "grad_norm": 0.2868305444717407,
      "learning_rate": 0.00012018292520805083,
      "loss": 0.1438,
      "step": 19190
    },
    {
      "epoch": 1.3783201722900216,
      "grad_norm": 0.01143055222928524,
      "learning_rate": 0.00012012974927547793,
      "loss": 0.1389,
      "step": 19200
    },
    {
      "epoch": 1.3790380473797559,
      "grad_norm": 0.29399287700653076,
      "learning_rate": 0.000120076573342905,
      "loss": 0.0856,
      "step": 19210
    },
    {
      "epoch": 1.3797559224694904,
      "grad_norm": 4.860620975494385,
      "learning_rate": 0.00012002339741033209,
      "loss": 0.147,
      "step": 19220
    },
    {
      "epoch": 1.3804737975592247,
      "grad_norm": 32.780757904052734,
      "learning_rate": 0.00011997022147775918,
      "loss": 0.2993,
      "step": 19230
    },
    {
      "epoch": 1.381191672648959,
      "grad_norm": 11.637004852294922,
      "learning_rate": 0.00011991704554518626,
      "loss": 0.2497,
      "step": 19240
    },
    {
      "epoch": 1.3819095477386933,
      "grad_norm": 3.203195571899414,
      "learning_rate": 0.00011986386961261334,
      "loss": 0.3345,
      "step": 19250
    },
    {
      "epoch": 1.3826274228284279,
      "grad_norm": 2.1249077320098877,
      "learning_rate": 0.0001198106936800404,
      "loss": 0.2716,
      "step": 19260
    },
    {
      "epoch": 1.3833452979181622,
      "grad_norm": 9.019655227661133,
      "learning_rate": 0.00011975751774746751,
      "loss": 0.1857,
      "step": 19270
    },
    {
      "epoch": 1.3840631730078967,
      "grad_norm": 1.0735527276992798,
      "learning_rate": 0.00011970434181489458,
      "loss": 0.1698,
      "step": 19280
    },
    {
      "epoch": 1.384781048097631,
      "grad_norm": 3.1833503246307373,
      "learning_rate": 0.00011965116588232166,
      "loss": 0.1018,
      "step": 19290
    },
    {
      "epoch": 1.3854989231873653,
      "grad_norm": 0.056789711117744446,
      "learning_rate": 0.00011959798994974876,
      "loss": 0.072,
      "step": 19300
    },
    {
      "epoch": 1.3862167982770996,
      "grad_norm": 0.006964389234781265,
      "learning_rate": 0.00011954481401717583,
      "loss": 0.2615,
      "step": 19310
    },
    {
      "epoch": 1.3869346733668342,
      "grad_norm": 9.701007843017578,
      "learning_rate": 0.00011949163808460291,
      "loss": 0.2038,
      "step": 19320
    },
    {
      "epoch": 1.3876525484565685,
      "grad_norm": 0.008404976688325405,
      "learning_rate": 0.00011943846215203,
      "loss": 0.1237,
      "step": 19330
    },
    {
      "epoch": 1.388370423546303,
      "grad_norm": 0.2638360261917114,
      "learning_rate": 0.00011938528621945708,
      "loss": 0.1322,
      "step": 19340
    },
    {
      "epoch": 1.3890882986360373,
      "grad_norm": 0.09750594198703766,
      "learning_rate": 0.00011933211028688416,
      "loss": 0.1129,
      "step": 19350
    },
    {
      "epoch": 1.3898061737257716,
      "grad_norm": 9.89799976348877,
      "learning_rate": 0.00011927893435431124,
      "loss": 0.1775,
      "step": 19360
    },
    {
      "epoch": 1.3905240488155062,
      "grad_norm": 0.05173586308956146,
      "learning_rate": 0.00011922575842173833,
      "loss": 0.1976,
      "step": 19370
    },
    {
      "epoch": 1.3912419239052405,
      "grad_norm": 0.0059770322404801846,
      "learning_rate": 0.00011917258248916541,
      "loss": 0.0657,
      "step": 19380
    },
    {
      "epoch": 1.3919597989949748,
      "grad_norm": 18.097326278686523,
      "learning_rate": 0.00011911940655659248,
      "loss": 0.2704,
      "step": 19390
    },
    {
      "epoch": 1.3926776740847093,
      "grad_norm": 3.200465202331543,
      "learning_rate": 0.00011906623062401958,
      "loss": 0.4137,
      "step": 19400
    },
    {
      "epoch": 1.3933955491744436,
      "grad_norm": 15.750873565673828,
      "learning_rate": 0.00011901305469144665,
      "loss": 0.0501,
      "step": 19410
    },
    {
      "epoch": 1.394113424264178,
      "grad_norm": 21.40621566772461,
      "learning_rate": 0.00011895987875887373,
      "loss": 0.626,
      "step": 19420
    },
    {
      "epoch": 1.3948312993539125,
      "grad_norm": 0.30843430757522583,
      "learning_rate": 0.00011890670282630084,
      "loss": 0.1022,
      "step": 19430
    },
    {
      "epoch": 1.3955491744436468,
      "grad_norm": 2.852978467941284,
      "learning_rate": 0.0001188535268937279,
      "loss": 0.0113,
      "step": 19440
    },
    {
      "epoch": 1.3962670495333813,
      "grad_norm": 7.398921012878418,
      "learning_rate": 0.00011880035096115498,
      "loss": 0.3071,
      "step": 19450
    },
    {
      "epoch": 1.3969849246231156,
      "grad_norm": 8.355484962463379,
      "learning_rate": 0.00011874717502858208,
      "loss": 0.3411,
      "step": 19460
    },
    {
      "epoch": 1.39770279971285,
      "grad_norm": 0.47303563356399536,
      "learning_rate": 0.00011869399909600915,
      "loss": 0.062,
      "step": 19470
    },
    {
      "epoch": 1.3984206748025843,
      "grad_norm": 6.294236183166504,
      "learning_rate": 0.00011864082316343623,
      "loss": 0.0685,
      "step": 19480
    },
    {
      "epoch": 1.3991385498923188,
      "grad_norm": 0.08611486852169037,
      "learning_rate": 0.00011858764723086331,
      "loss": 0.393,
      "step": 19490
    },
    {
      "epoch": 1.399856424982053,
      "grad_norm": 0.9173271059989929,
      "learning_rate": 0.0001185344712982904,
      "loss": 0.1493,
      "step": 19500
    },
    {
      "epoch": 1.4005743000717876,
      "grad_norm": 0.017554596066474915,
      "learning_rate": 0.00011848129536571749,
      "loss": 0.2514,
      "step": 19510
    },
    {
      "epoch": 1.401292175161522,
      "grad_norm": 0.029978716745972633,
      "learning_rate": 0.00011842811943314455,
      "loss": 0.1149,
      "step": 19520
    },
    {
      "epoch": 1.4020100502512562,
      "grad_norm": 3.4532041549682617,
      "learning_rate": 0.00011837494350057166,
      "loss": 0.2822,
      "step": 19530
    },
    {
      "epoch": 1.4027279253409906,
      "grad_norm": 12.336570739746094,
      "learning_rate": 0.00011832176756799872,
      "loss": 0.3052,
      "step": 19540
    },
    {
      "epoch": 1.403445800430725,
      "grad_norm": 9.794063568115234,
      "learning_rate": 0.0001182685916354258,
      "loss": 0.1153,
      "step": 19550
    },
    {
      "epoch": 1.4041636755204594,
      "grad_norm": 0.01855374313890934,
      "learning_rate": 0.00011821541570285291,
      "loss": 0.1064,
      "step": 19560
    },
    {
      "epoch": 1.404881550610194,
      "grad_norm": 0.03511466458439827,
      "learning_rate": 0.00011816223977027998,
      "loss": 0.0943,
      "step": 19570
    },
    {
      "epoch": 1.4055994256999282,
      "grad_norm": 0.02710958756506443,
      "learning_rate": 0.00011810906383770706,
      "loss": 0.0433,
      "step": 19580
    },
    {
      "epoch": 1.4063173007896625,
      "grad_norm": 0.6048016548156738,
      "learning_rate": 0.00011805588790513414,
      "loss": 0.1481,
      "step": 19590
    },
    {
      "epoch": 1.4070351758793969,
      "grad_norm": 3.0691754817962646,
      "learning_rate": 0.00011800271197256123,
      "loss": 0.1791,
      "step": 19600
    },
    {
      "epoch": 1.4077530509691314,
      "grad_norm": 0.025399915874004364,
      "learning_rate": 0.00011794953603998831,
      "loss": 0.1891,
      "step": 19610
    },
    {
      "epoch": 1.4084709260588657,
      "grad_norm": 4.228495121002197,
      "learning_rate": 0.00011789636010741539,
      "loss": 0.3379,
      "step": 19620
    },
    {
      "epoch": 1.4091888011486002,
      "grad_norm": 6.523067951202393,
      "learning_rate": 0.00011784318417484248,
      "loss": 0.3282,
      "step": 19630
    },
    {
      "epoch": 1.4099066762383345,
      "grad_norm": 0.0361136794090271,
      "learning_rate": 0.00011779000824226956,
      "loss": 0.1279,
      "step": 19640
    },
    {
      "epoch": 1.4106245513280689,
      "grad_norm": 16.20221519470215,
      "learning_rate": 0.00011773683230969663,
      "loss": 0.258,
      "step": 19650
    },
    {
      "epoch": 1.4113424264178032,
      "grad_norm": 11.20158576965332,
      "learning_rate": 0.00011768365637712373,
      "loss": 0.0871,
      "step": 19660
    },
    {
      "epoch": 1.4120603015075377,
      "grad_norm": 0.5235979557037354,
      "learning_rate": 0.00011763048044455081,
      "loss": 0.1175,
      "step": 19670
    },
    {
      "epoch": 1.412778176597272,
      "grad_norm": 0.02387411519885063,
      "learning_rate": 0.00011757730451197788,
      "loss": 0.2069,
      "step": 19680
    },
    {
      "epoch": 1.4134960516870065,
      "grad_norm": 0.039175938814878464,
      "learning_rate": 0.00011752412857940496,
      "loss": 0.0796,
      "step": 19690
    },
    {
      "epoch": 1.4142139267767408,
      "grad_norm": 18.351146697998047,
      "learning_rate": 0.00011747095264683205,
      "loss": 0.1225,
      "step": 19700
    },
    {
      "epoch": 1.4149318018664752,
      "grad_norm": 8.765024185180664,
      "learning_rate": 0.00011741777671425913,
      "loss": 0.2827,
      "step": 19710
    },
    {
      "epoch": 1.4156496769562097,
      "grad_norm": 20.280420303344727,
      "learning_rate": 0.00011736460078168621,
      "loss": 0.1603,
      "step": 19720
    },
    {
      "epoch": 1.416367552045944,
      "grad_norm": 0.12051282823085785,
      "learning_rate": 0.0001173114248491133,
      "loss": 0.0224,
      "step": 19730
    },
    {
      "epoch": 1.4170854271356783,
      "grad_norm": 0.09440640360116959,
      "learning_rate": 0.00011725824891654038,
      "loss": 0.1039,
      "step": 19740
    },
    {
      "epoch": 1.4178033022254128,
      "grad_norm": 0.007297256495803595,
      "learning_rate": 0.00011720507298396746,
      "loss": 0.2224,
      "step": 19750
    },
    {
      "epoch": 1.4185211773151472,
      "grad_norm": 17.587919235229492,
      "learning_rate": 0.00011715189705139455,
      "loss": 0.0782,
      "step": 19760
    },
    {
      "epoch": 1.4192390524048815,
      "grad_norm": 18.16722297668457,
      "learning_rate": 0.00011709872111882163,
      "loss": 0.4061,
      "step": 19770
    },
    {
      "epoch": 1.419956927494616,
      "grad_norm": 0.03584643453359604,
      "learning_rate": 0.0001170455451862487,
      "loss": 0.0354,
      "step": 19780
    },
    {
      "epoch": 1.4206748025843503,
      "grad_norm": 1.2080848217010498,
      "learning_rate": 0.00011699236925367578,
      "loss": 0.2928,
      "step": 19790
    },
    {
      "epoch": 1.4213926776740848,
      "grad_norm": 0.01834787242114544,
      "learning_rate": 0.00011693919332110289,
      "loss": 0.1666,
      "step": 19800
    },
    {
      "epoch": 1.4221105527638191,
      "grad_norm": 16.615562438964844,
      "learning_rate": 0.00011688601738852995,
      "loss": 0.3514,
      "step": 19810
    },
    {
      "epoch": 1.4228284278535535,
      "grad_norm": 1.5082839727401733,
      "learning_rate": 0.00011683284145595703,
      "loss": 0.3385,
      "step": 19820
    },
    {
      "epoch": 1.4235463029432878,
      "grad_norm": 23.97476577758789,
      "learning_rate": 0.00011677966552338412,
      "loss": 0.3538,
      "step": 19830
    },
    {
      "epoch": 1.4242641780330223,
      "grad_norm": 14.647143363952637,
      "learning_rate": 0.0001167264895908112,
      "loss": 0.2359,
      "step": 19840
    },
    {
      "epoch": 1.4249820531227566,
      "grad_norm": 10.958089828491211,
      "learning_rate": 0.00011667331365823828,
      "loss": 0.1669,
      "step": 19850
    },
    {
      "epoch": 1.4256999282124911,
      "grad_norm": 0.011131496168673038,
      "learning_rate": 0.00011662013772566538,
      "loss": 0.0248,
      "step": 19860
    },
    {
      "epoch": 1.4264178033022255,
      "grad_norm": 3.49760365486145,
      "learning_rate": 0.00011656696179309246,
      "loss": 0.2237,
      "step": 19870
    },
    {
      "epoch": 1.4271356783919598,
      "grad_norm": 1.938018798828125,
      "learning_rate": 0.00011651378586051954,
      "loss": 0.0208,
      "step": 19880
    },
    {
      "epoch": 1.427853553481694,
      "grad_norm": 0.1821342259645462,
      "learning_rate": 0.0001164606099279466,
      "loss": 0.1781,
      "step": 19890
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.10732289403676987,
      "learning_rate": 0.00011640743399537371,
      "loss": 0.2904,
      "step": 19900
    },
    {
      "epoch": 1.429289303661163,
      "grad_norm": 0.10070313513278961,
      "learning_rate": 0.00011635425806280077,
      "loss": 0.0614,
      "step": 19910
    },
    {
      "epoch": 1.4300071787508974,
      "grad_norm": 0.016837703064084053,
      "learning_rate": 0.00011630108213022785,
      "loss": 0.0021,
      "step": 19920
    },
    {
      "epoch": 1.4307250538406318,
      "grad_norm": 27.996070861816406,
      "learning_rate": 0.00011624790619765496,
      "loss": 0.1614,
      "step": 19930
    },
    {
      "epoch": 1.431442928930366,
      "grad_norm": 8.065011978149414,
      "learning_rate": 0.00011619473026508203,
      "loss": 0.1638,
      "step": 19940
    },
    {
      "epoch": 1.4321608040201004,
      "grad_norm": 26.321985244750977,
      "learning_rate": 0.0001161415543325091,
      "loss": 0.3367,
      "step": 19950
    },
    {
      "epoch": 1.432878679109835,
      "grad_norm": 1.9576518535614014,
      "learning_rate": 0.0001160883783999362,
      "loss": 0.1349,
      "step": 19960
    },
    {
      "epoch": 1.4335965541995692,
      "grad_norm": 17.673038482666016,
      "learning_rate": 0.00011603520246736328,
      "loss": 0.321,
      "step": 19970
    },
    {
      "epoch": 1.4343144292893037,
      "grad_norm": 11.715115547180176,
      "learning_rate": 0.00011598202653479036,
      "loss": 0.2916,
      "step": 19980
    },
    {
      "epoch": 1.435032304379038,
      "grad_norm": 0.008044591173529625,
      "learning_rate": 0.00011592885060221744,
      "loss": 0.0829,
      "step": 19990
    },
    {
      "epoch": 1.4357501794687724,
      "grad_norm": 25.326576232910156,
      "learning_rate": 0.00011587567466964453,
      "loss": 0.2623,
      "step": 20000
    },
    {
      "epoch": 1.436468054558507,
      "grad_norm": 0.2890024483203888,
      "learning_rate": 0.00011582249873707161,
      "loss": 0.1085,
      "step": 20010
    },
    {
      "epoch": 1.4371859296482412,
      "grad_norm": 0.27447983622550964,
      "learning_rate": 0.00011576932280449868,
      "loss": 0.1346,
      "step": 20020
    },
    {
      "epoch": 1.4379038047379755,
      "grad_norm": 0.003988848067820072,
      "learning_rate": 0.00011571614687192578,
      "loss": 0.2307,
      "step": 20030
    },
    {
      "epoch": 1.43862167982771,
      "grad_norm": 0.07586514204740524,
      "learning_rate": 0.00011566297093935285,
      "loss": 0.1351,
      "step": 20040
    },
    {
      "epoch": 1.4393395549174444,
      "grad_norm": 0.671788215637207,
      "learning_rate": 0.00011560979500677993,
      "loss": 0.082,
      "step": 20050
    },
    {
      "epoch": 1.4400574300071787,
      "grad_norm": 8.2781982421875,
      "learning_rate": 0.00011555661907420703,
      "loss": 0.2498,
      "step": 20060
    },
    {
      "epoch": 1.4407753050969132,
      "grad_norm": 4.942678928375244,
      "learning_rate": 0.0001155034431416341,
      "loss": 0.1317,
      "step": 20070
    },
    {
      "epoch": 1.4414931801866475,
      "grad_norm": 26.67867088317871,
      "learning_rate": 0.00011545026720906118,
      "loss": 0.2681,
      "step": 20080
    },
    {
      "epoch": 1.442211055276382,
      "grad_norm": 1.3569419384002686,
      "learning_rate": 0.00011539709127648826,
      "loss": 0.1725,
      "step": 20090
    },
    {
      "epoch": 1.4429289303661164,
      "grad_norm": 16.502670288085938,
      "learning_rate": 0.00011534391534391535,
      "loss": 0.0729,
      "step": 20100
    },
    {
      "epoch": 1.4436468054558507,
      "grad_norm": 0.04768310859799385,
      "learning_rate": 0.00011529073941134243,
      "loss": 0.2704,
      "step": 20110
    },
    {
      "epoch": 1.444364680545585,
      "grad_norm": 14.093764305114746,
      "learning_rate": 0.00011523756347876951,
      "loss": 0.2395,
      "step": 20120
    },
    {
      "epoch": 1.4450825556353195,
      "grad_norm": 17.966835021972656,
      "learning_rate": 0.0001151843875461966,
      "loss": 0.4213,
      "step": 20130
    },
    {
      "epoch": 1.4458004307250538,
      "grad_norm": 11.953675270080566,
      "learning_rate": 0.00011513121161362368,
      "loss": 0.0982,
      "step": 20140
    },
    {
      "epoch": 1.4465183058147884,
      "grad_norm": 0.7809251546859741,
      "learning_rate": 0.00011507803568105075,
      "loss": 0.2278,
      "step": 20150
    },
    {
      "epoch": 1.4472361809045227,
      "grad_norm": 13.276141166687012,
      "learning_rate": 0.00011502485974847786,
      "loss": 0.1658,
      "step": 20160
    },
    {
      "epoch": 1.447954055994257,
      "grad_norm": 0.19211116433143616,
      "learning_rate": 0.00011497168381590492,
      "loss": 0.1404,
      "step": 20170
    },
    {
      "epoch": 1.4486719310839913,
      "grad_norm": 14.906972885131836,
      "learning_rate": 0.000114918507883332,
      "loss": 0.2121,
      "step": 20180
    },
    {
      "epoch": 1.4493898061737258,
      "grad_norm": 0.8246539831161499,
      "learning_rate": 0.00011486533195075911,
      "loss": 0.0292,
      "step": 20190
    },
    {
      "epoch": 1.4501076812634601,
      "grad_norm": 20.1459903717041,
      "learning_rate": 0.00011481215601818617,
      "loss": 0.1719,
      "step": 20200
    },
    {
      "epoch": 1.4508255563531947,
      "grad_norm": 7.774777412414551,
      "learning_rate": 0.00011475898008561325,
      "loss": 0.0869,
      "step": 20210
    },
    {
      "epoch": 1.451543431442929,
      "grad_norm": 0.03308495879173279,
      "learning_rate": 0.00011470580415304033,
      "loss": 0.207,
      "step": 20220
    },
    {
      "epoch": 1.4522613065326633,
      "grad_norm": 0.410931795835495,
      "learning_rate": 0.00011465262822046743,
      "loss": 0.1209,
      "step": 20230
    },
    {
      "epoch": 1.4529791816223976,
      "grad_norm": 0.060936134308576584,
      "learning_rate": 0.0001145994522878945,
      "loss": 0.009,
      "step": 20240
    },
    {
      "epoch": 1.4536970567121321,
      "grad_norm": 11.918434143066406,
      "learning_rate": 0.00011454627635532159,
      "loss": 0.3279,
      "step": 20250
    },
    {
      "epoch": 1.4544149318018664,
      "grad_norm": 30.239126205444336,
      "learning_rate": 0.00011449310042274868,
      "loss": 0.2191,
      "step": 20260
    },
    {
      "epoch": 1.455132806891601,
      "grad_norm": 1.1026540994644165,
      "learning_rate": 0.00011443992449017576,
      "loss": 0.03,
      "step": 20270
    },
    {
      "epoch": 1.4558506819813353,
      "grad_norm": 10.515050888061523,
      "learning_rate": 0.00011438674855760282,
      "loss": 0.0741,
      "step": 20280
    },
    {
      "epoch": 1.4565685570710696,
      "grad_norm": 0.42266175150871277,
      "learning_rate": 0.00011433357262502993,
      "loss": 0.2659,
      "step": 20290
    },
    {
      "epoch": 1.457286432160804,
      "grad_norm": 1.2379000186920166,
      "learning_rate": 0.000114280396692457,
      "loss": 0.2499,
      "step": 20300
    },
    {
      "epoch": 1.4580043072505384,
      "grad_norm": 3.4090890884399414,
      "learning_rate": 0.00011422722075988408,
      "loss": 0.0912,
      "step": 20310
    },
    {
      "epoch": 1.4587221823402727,
      "grad_norm": 0.3343246579170227,
      "learning_rate": 0.00011417404482731116,
      "loss": 0.1848,
      "step": 20320
    },
    {
      "epoch": 1.4594400574300073,
      "grad_norm": 0.22664302587509155,
      "learning_rate": 0.00011412086889473825,
      "loss": 0.4625,
      "step": 20330
    },
    {
      "epoch": 1.4601579325197416,
      "grad_norm": 4.589033603668213,
      "learning_rate": 0.00011406769296216533,
      "loss": 0.2782,
      "step": 20340
    },
    {
      "epoch": 1.4608758076094759,
      "grad_norm": 0.007784377317875624,
      "learning_rate": 0.00011401451702959241,
      "loss": 0.0218,
      "step": 20350
    },
    {
      "epoch": 1.4615936826992104,
      "grad_norm": 18.30685043334961,
      "learning_rate": 0.0001139613410970195,
      "loss": 0.3908,
      "step": 20360
    },
    {
      "epoch": 1.4623115577889447,
      "grad_norm": 10.510741233825684,
      "learning_rate": 0.00011390816516444658,
      "loss": 0.1638,
      "step": 20370
    },
    {
      "epoch": 1.463029432878679,
      "grad_norm": 0.29971814155578613,
      "learning_rate": 0.00011385498923187366,
      "loss": 0.2227,
      "step": 20380
    },
    {
      "epoch": 1.4637473079684136,
      "grad_norm": 3.9936254024505615,
      "learning_rate": 0.00011380181329930075,
      "loss": 0.1789,
      "step": 20390
    },
    {
      "epoch": 1.4644651830581479,
      "grad_norm": 4.778620719909668,
      "learning_rate": 0.00011374863736672783,
      "loss": 0.2565,
      "step": 20400
    },
    {
      "epoch": 1.4651830581478822,
      "grad_norm": 0.2015201598405838,
      "learning_rate": 0.0001136954614341549,
      "loss": 0.1198,
      "step": 20410
    },
    {
      "epoch": 1.4659009332376167,
      "grad_norm": 15.009505271911621,
      "learning_rate": 0.00011364228550158198,
      "loss": 0.1708,
      "step": 20420
    },
    {
      "epoch": 1.466618808327351,
      "grad_norm": 1.225887417793274,
      "learning_rate": 0.00011358910956900908,
      "loss": 0.3991,
      "step": 20430
    },
    {
      "epoch": 1.4673366834170856,
      "grad_norm": 1.4437932968139648,
      "learning_rate": 0.00011353593363643615,
      "loss": 0.0515,
      "step": 20440
    },
    {
      "epoch": 1.4680545585068199,
      "grad_norm": 9.482627868652344,
      "learning_rate": 0.00011348275770386323,
      "loss": 0.113,
      "step": 20450
    },
    {
      "epoch": 1.4687724335965542,
      "grad_norm": 1.2242109775543213,
      "learning_rate": 0.00011342958177129032,
      "loss": 0.1427,
      "step": 20460
    },
    {
      "epoch": 1.4694903086862885,
      "grad_norm": 18.502500534057617,
      "learning_rate": 0.0001133764058387174,
      "loss": 0.1092,
      "step": 20470
    },
    {
      "epoch": 1.470208183776023,
      "grad_norm": 0.006698329467326403,
      "learning_rate": 0.00011332322990614448,
      "loss": 0.2759,
      "step": 20480
    },
    {
      "epoch": 1.4709260588657573,
      "grad_norm": 0.0038191720377653837,
      "learning_rate": 0.00011327005397357158,
      "loss": 0.2174,
      "step": 20490
    },
    {
      "epoch": 1.4716439339554919,
      "grad_norm": 0.08901114016771317,
      "learning_rate": 0.00011321687804099865,
      "loss": 0.0393,
      "step": 20500
    },
    {
      "epoch": 1.4723618090452262,
      "grad_norm": 0.24049177765846252,
      "learning_rate": 0.00011316370210842573,
      "loss": 0.0907,
      "step": 20510
    },
    {
      "epoch": 1.4730796841349605,
      "grad_norm": 1.632543683052063,
      "learning_rate": 0.0001131105261758528,
      "loss": 0.0895,
      "step": 20520
    },
    {
      "epoch": 1.4737975592246948,
      "grad_norm": 0.8032276034355164,
      "learning_rate": 0.0001130573502432799,
      "loss": 0.2032,
      "step": 20530
    },
    {
      "epoch": 1.4745154343144293,
      "grad_norm": 1.7149145603179932,
      "learning_rate": 0.00011300417431070697,
      "loss": 0.0315,
      "step": 20540
    },
    {
      "epoch": 1.4752333094041636,
      "grad_norm": 0.8134072422981262,
      "learning_rate": 0.00011295099837813405,
      "loss": 0.1147,
      "step": 20550
    },
    {
      "epoch": 1.4759511844938982,
      "grad_norm": 1.950977087020874,
      "learning_rate": 0.00011289782244556116,
      "loss": 0.124,
      "step": 20560
    },
    {
      "epoch": 1.4766690595836325,
      "grad_norm": 10.558313369750977,
      "learning_rate": 0.00011284464651298822,
      "loss": 0.1089,
      "step": 20570
    },
    {
      "epoch": 1.4773869346733668,
      "grad_norm": 13.445982933044434,
      "learning_rate": 0.0001127914705804153,
      "loss": 0.2077,
      "step": 20580
    },
    {
      "epoch": 1.478104809763101,
      "grad_norm": 13.127480506896973,
      "learning_rate": 0.0001127382946478424,
      "loss": 0.3291,
      "step": 20590
    },
    {
      "epoch": 1.4788226848528356,
      "grad_norm": 0.20686520636081696,
      "learning_rate": 0.00011268511871526948,
      "loss": 0.0428,
      "step": 20600
    },
    {
      "epoch": 1.47954055994257,
      "grad_norm": 0.01384693942964077,
      "learning_rate": 0.00011263194278269656,
      "loss": 0.1748,
      "step": 20610
    },
    {
      "epoch": 1.4802584350323045,
      "grad_norm": 0.025494016706943512,
      "learning_rate": 0.00011257876685012364,
      "loss": 0.1877,
      "step": 20620
    },
    {
      "epoch": 1.4809763101220388,
      "grad_norm": 0.026914644986391068,
      "learning_rate": 0.00011252559091755073,
      "loss": 0.0664,
      "step": 20630
    },
    {
      "epoch": 1.481694185211773,
      "grad_norm": 5.181797981262207,
      "learning_rate": 0.00011247241498497781,
      "loss": 0.1106,
      "step": 20640
    },
    {
      "epoch": 1.4824120603015074,
      "grad_norm": 1.2387043237686157,
      "learning_rate": 0.00011241923905240487,
      "loss": 0.1814,
      "step": 20650
    },
    {
      "epoch": 1.483129935391242,
      "grad_norm": 4.014029502868652,
      "learning_rate": 0.00011236606311983198,
      "loss": 0.2491,
      "step": 20660
    },
    {
      "epoch": 1.4838478104809762,
      "grad_norm": 0.24333979189395905,
      "learning_rate": 0.00011231288718725905,
      "loss": 0.17,
      "step": 20670
    },
    {
      "epoch": 1.4845656855707108,
      "grad_norm": 0.10551206022500992,
      "learning_rate": 0.00011225971125468613,
      "loss": 0.1754,
      "step": 20680
    },
    {
      "epoch": 1.485283560660445,
      "grad_norm": 9.259521484375,
      "learning_rate": 0.00011220653532211323,
      "loss": 0.4534,
      "step": 20690
    },
    {
      "epoch": 1.4860014357501794,
      "grad_norm": 0.008775663562119007,
      "learning_rate": 0.0001121533593895403,
      "loss": 0.2779,
      "step": 20700
    },
    {
      "epoch": 1.486719310839914,
      "grad_norm": 0.064423568546772,
      "learning_rate": 0.00011210018345696738,
      "loss": 0.0847,
      "step": 20710
    },
    {
      "epoch": 1.4874371859296482,
      "grad_norm": 35.51310729980469,
      "learning_rate": 0.00011204700752439446,
      "loss": 0.3512,
      "step": 20720
    },
    {
      "epoch": 1.4881550610193826,
      "grad_norm": 0.21101155877113342,
      "learning_rate": 0.00011199383159182155,
      "loss": 0.1213,
      "step": 20730
    },
    {
      "epoch": 1.488872936109117,
      "grad_norm": 0.037760939449071884,
      "learning_rate": 0.00011194065565924863,
      "loss": 0.2077,
      "step": 20740
    },
    {
      "epoch": 1.4895908111988514,
      "grad_norm": 21.192113876342773,
      "learning_rate": 0.00011188747972667571,
      "loss": 0.4063,
      "step": 20750
    },
    {
      "epoch": 1.4903086862885857,
      "grad_norm": 11.363921165466309,
      "learning_rate": 0.0001118343037941028,
      "loss": 0.3432,
      "step": 20760
    },
    {
      "epoch": 1.4910265613783202,
      "grad_norm": 6.753996849060059,
      "learning_rate": 0.00011178112786152988,
      "loss": 0.0581,
      "step": 20770
    },
    {
      "epoch": 1.4917444364680545,
      "grad_norm": 10.110320091247559,
      "learning_rate": 0.00011172795192895695,
      "loss": 0.0227,
      "step": 20780
    },
    {
      "epoch": 1.492462311557789,
      "grad_norm": 5.3065266609191895,
      "learning_rate": 0.00011167477599638405,
      "loss": 0.1265,
      "step": 20790
    },
    {
      "epoch": 1.4931801866475234,
      "grad_norm": 13.488198280334473,
      "learning_rate": 0.00011162160006381112,
      "loss": 0.2212,
      "step": 20800
    },
    {
      "epoch": 1.4938980617372577,
      "grad_norm": 17.0015869140625,
      "learning_rate": 0.0001115684241312382,
      "loss": 0.16,
      "step": 20810
    },
    {
      "epoch": 1.494615936826992,
      "grad_norm": 1.0304932594299316,
      "learning_rate": 0.00011151524819866528,
      "loss": 0.0399,
      "step": 20820
    },
    {
      "epoch": 1.4953338119167265,
      "grad_norm": 2.6336891651153564,
      "learning_rate": 0.00011146207226609237,
      "loss": 0.3038,
      "step": 20830
    },
    {
      "epoch": 1.4960516870064609,
      "grad_norm": 8.43249225616455,
      "learning_rate": 0.00011140889633351945,
      "loss": 0.0927,
      "step": 20840
    },
    {
      "epoch": 1.4967695620961954,
      "grad_norm": 0.01032582949846983,
      "learning_rate": 0.00011135572040094653,
      "loss": 0.0496,
      "step": 20850
    },
    {
      "epoch": 1.4974874371859297,
      "grad_norm": 0.4131980240345001,
      "learning_rate": 0.00011130254446837362,
      "loss": 0.2162,
      "step": 20860
    },
    {
      "epoch": 1.498205312275664,
      "grad_norm": 10.488866806030273,
      "learning_rate": 0.0001112493685358007,
      "loss": 0.2535,
      "step": 20870
    },
    {
      "epoch": 1.4989231873653983,
      "grad_norm": 7.404097080230713,
      "learning_rate": 0.00011119619260322778,
      "loss": 0.0164,
      "step": 20880
    },
    {
      "epoch": 1.4996410624551328,
      "grad_norm": 2.498521566390991,
      "learning_rate": 0.00011114301667065488,
      "loss": 0.1094,
      "step": 20890
    },
    {
      "epoch": 1.5003589375448672,
      "grad_norm": 13.2909517288208,
      "learning_rate": 0.00011108984073808196,
      "loss": 0.2204,
      "step": 20900
    },
    {
      "epoch": 1.5010768126346017,
      "grad_norm": 0.6919401288032532,
      "learning_rate": 0.00011103666480550902,
      "loss": 0.1378,
      "step": 20910
    },
    {
      "epoch": 1.501794687724336,
      "grad_norm": 7.006785869598389,
      "learning_rate": 0.00011098348887293613,
      "loss": 0.1389,
      "step": 20920
    },
    {
      "epoch": 1.5025125628140703,
      "grad_norm": 6.706514835357666,
      "learning_rate": 0.0001109303129403632,
      "loss": 0.1933,
      "step": 20930
    },
    {
      "epoch": 1.5032304379038046,
      "grad_norm": 0.006490702275186777,
      "learning_rate": 0.00011087713700779027,
      "loss": 0.0987,
      "step": 20940
    },
    {
      "epoch": 1.5039483129935391,
      "grad_norm": 0.0022795109543949366,
      "learning_rate": 0.00011082396107521735,
      "loss": 0.0639,
      "step": 20950
    },
    {
      "epoch": 1.5046661880832735,
      "grad_norm": 8.801907539367676,
      "learning_rate": 0.00011077078514264445,
      "loss": 0.242,
      "step": 20960
    },
    {
      "epoch": 1.505384063173008,
      "grad_norm": 0.20262698829174042,
      "learning_rate": 0.00011071760921007153,
      "loss": 0.0288,
      "step": 20970
    },
    {
      "epoch": 1.5061019382627423,
      "grad_norm": 0.020241733640432358,
      "learning_rate": 0.0001106644332774986,
      "loss": 0.2106,
      "step": 20980
    },
    {
      "epoch": 1.5068198133524766,
      "grad_norm": 0.09000896662473679,
      "learning_rate": 0.0001106112573449257,
      "loss": 0.1478,
      "step": 20990
    },
    {
      "epoch": 1.507537688442211,
      "grad_norm": 0.1631166934967041,
      "learning_rate": 0.00011055808141235278,
      "loss": 0.1895,
      "step": 21000
    },
    {
      "epoch": 1.5082555635319455,
      "grad_norm": 24.766727447509766,
      "learning_rate": 0.00011050490547977986,
      "loss": 0.1712,
      "step": 21010
    },
    {
      "epoch": 1.50897343862168,
      "grad_norm": 0.015034303069114685,
      "learning_rate": 0.00011045172954720695,
      "loss": 0.0323,
      "step": 21020
    },
    {
      "epoch": 1.5096913137114143,
      "grad_norm": 16.529937744140625,
      "learning_rate": 0.00011039855361463403,
      "loss": 0.5472,
      "step": 21030
    },
    {
      "epoch": 1.5104091888011486,
      "grad_norm": 19.4173641204834,
      "learning_rate": 0.0001103453776820611,
      "loss": 0.1344,
      "step": 21040
    },
    {
      "epoch": 1.511127063890883,
      "grad_norm": 0.4926503002643585,
      "learning_rate": 0.00011029220174948818,
      "loss": 0.218,
      "step": 21050
    },
    {
      "epoch": 1.5118449389806172,
      "grad_norm": 6.705016136169434,
      "learning_rate": 0.00011023902581691527,
      "loss": 0.3141,
      "step": 21060
    },
    {
      "epoch": 1.5125628140703518,
      "grad_norm": 0.0032873102463781834,
      "learning_rate": 0.00011018584988434235,
      "loss": 0.1638,
      "step": 21070
    },
    {
      "epoch": 1.5132806891600863,
      "grad_norm": 18.166210174560547,
      "learning_rate": 0.00011013267395176943,
      "loss": 0.0865,
      "step": 21080
    },
    {
      "epoch": 1.5139985642498206,
      "grad_norm": 9.751028060913086,
      "learning_rate": 0.00011007949801919652,
      "loss": 0.2145,
      "step": 21090
    },
    {
      "epoch": 1.514716439339555,
      "grad_norm": 0.026890335604548454,
      "learning_rate": 0.0001100263220866236,
      "loss": 0.1628,
      "step": 21100
    },
    {
      "epoch": 1.5154343144292892,
      "grad_norm": 1.3523551225662231,
      "learning_rate": 0.00010997314615405068,
      "loss": 0.2572,
      "step": 21110
    },
    {
      "epoch": 1.5161521895190235,
      "grad_norm": 0.012471755035221577,
      "learning_rate": 0.00010991997022147777,
      "loss": 0.0134,
      "step": 21120
    },
    {
      "epoch": 1.516870064608758,
      "grad_norm": 8.069782257080078,
      "learning_rate": 0.00010986679428890485,
      "loss": 0.0447,
      "step": 21130
    },
    {
      "epoch": 1.5175879396984926,
      "grad_norm": 1.7746223211288452,
      "learning_rate": 0.00010981361835633193,
      "loss": 0.1324,
      "step": 21140
    },
    {
      "epoch": 1.518305814788227,
      "grad_norm": 22.764772415161133,
      "learning_rate": 0.000109760442423759,
      "loss": 0.1062,
      "step": 21150
    },
    {
      "epoch": 1.5190236898779612,
      "grad_norm": 0.010537657886743546,
      "learning_rate": 0.0001097072664911861,
      "loss": 0.0802,
      "step": 21160
    },
    {
      "epoch": 1.5197415649676955,
      "grad_norm": 18.57686996459961,
      "learning_rate": 0.00010965409055861317,
      "loss": 0.1854,
      "step": 21170
    },
    {
      "epoch": 1.52045944005743,
      "grad_norm": 11.384196281433105,
      "learning_rate": 0.00010960091462604025,
      "loss": 0.1641,
      "step": 21180
    },
    {
      "epoch": 1.5211773151471644,
      "grad_norm": 15.140609741210938,
      "learning_rate": 0.00010954773869346736,
      "loss": 0.051,
      "step": 21190
    },
    {
      "epoch": 1.521895190236899,
      "grad_norm": 14.707022666931152,
      "learning_rate": 0.00010949456276089442,
      "loss": 0.1791,
      "step": 21200
    },
    {
      "epoch": 1.5226130653266332,
      "grad_norm": 0.44758301973342896,
      "learning_rate": 0.0001094413868283215,
      "loss": 0.0103,
      "step": 21210
    },
    {
      "epoch": 1.5233309404163675,
      "grad_norm": 6.978225231170654,
      "learning_rate": 0.0001093882108957486,
      "loss": 0.3804,
      "step": 21220
    },
    {
      "epoch": 1.5240488155061018,
      "grad_norm": 0.42448052763938904,
      "learning_rate": 0.00010933503496317567,
      "loss": 0.2819,
      "step": 21230
    },
    {
      "epoch": 1.5247666905958364,
      "grad_norm": 0.02688680589199066,
      "learning_rate": 0.00010928185903060275,
      "loss": 0.2316,
      "step": 21240
    },
    {
      "epoch": 1.5254845656855707,
      "grad_norm": 0.03614664077758789,
      "learning_rate": 0.00010922868309802983,
      "loss": 0.1712,
      "step": 21250
    },
    {
      "epoch": 1.5262024407753052,
      "grad_norm": 3.339381456375122,
      "learning_rate": 0.00010917550716545693,
      "loss": 0.1112,
      "step": 21260
    },
    {
      "epoch": 1.5269203158650395,
      "grad_norm": 0.09725207090377808,
      "learning_rate": 0.000109122331232884,
      "loss": 0.038,
      "step": 21270
    },
    {
      "epoch": 1.5276381909547738,
      "grad_norm": 9.53788948059082,
      "learning_rate": 0.00010906915530031107,
      "loss": 0.1974,
      "step": 21280
    },
    {
      "epoch": 1.5283560660445081,
      "grad_norm": 0.014770734123885632,
      "learning_rate": 0.00010901597936773818,
      "loss": 0.3139,
      "step": 21290
    },
    {
      "epoch": 1.5290739411342427,
      "grad_norm": 0.16177822649478912,
      "learning_rate": 0.00010896280343516524,
      "loss": 0.2009,
      "step": 21300
    },
    {
      "epoch": 1.529791816223977,
      "grad_norm": 11.800522804260254,
      "learning_rate": 0.00010890962750259232,
      "loss": 0.4085,
      "step": 21310
    },
    {
      "epoch": 1.5305096913137115,
      "grad_norm": 1.0968924760818481,
      "learning_rate": 0.00010885645157001943,
      "loss": 0.0698,
      "step": 21320
    },
    {
      "epoch": 1.5312275664034458,
      "grad_norm": 0.14465203881263733,
      "learning_rate": 0.0001088032756374465,
      "loss": 0.0488,
      "step": 21330
    },
    {
      "epoch": 1.5319454414931801,
      "grad_norm": 0.14735865592956543,
      "learning_rate": 0.00010875009970487358,
      "loss": 0.1453,
      "step": 21340
    },
    {
      "epoch": 1.5326633165829144,
      "grad_norm": 0.3102021813392639,
      "learning_rate": 0.00010869692377230066,
      "loss": 0.1158,
      "step": 21350
    },
    {
      "epoch": 1.533381191672649,
      "grad_norm": 1.9504367113113403,
      "learning_rate": 0.00010864374783972775,
      "loss": 0.2345,
      "step": 21360
    },
    {
      "epoch": 1.5340990667623835,
      "grad_norm": 5.0334978103637695,
      "learning_rate": 0.00010859057190715483,
      "loss": 0.3765,
      "step": 21370
    },
    {
      "epoch": 1.5348169418521178,
      "grad_norm": 8.548107147216797,
      "learning_rate": 0.00010853739597458191,
      "loss": 0.175,
      "step": 21380
    },
    {
      "epoch": 1.5355348169418521,
      "grad_norm": 1.6846264600753784,
      "learning_rate": 0.000108484220042009,
      "loss": 0.0724,
      "step": 21390
    },
    {
      "epoch": 1.5362526920315864,
      "grad_norm": 13.165986061096191,
      "learning_rate": 0.00010843104410943608,
      "loss": 0.2726,
      "step": 21400
    },
    {
      "epoch": 1.5369705671213207,
      "grad_norm": 1.7429826259613037,
      "learning_rate": 0.00010837786817686315,
      "loss": 0.1642,
      "step": 21410
    },
    {
      "epoch": 1.5376884422110553,
      "grad_norm": 0.09316637367010117,
      "learning_rate": 0.00010832469224429025,
      "loss": 0.0906,
      "step": 21420
    },
    {
      "epoch": 1.5384063173007898,
      "grad_norm": 0.07597097754478455,
      "learning_rate": 0.00010827151631171732,
      "loss": 0.2955,
      "step": 21430
    },
    {
      "epoch": 1.5391241923905241,
      "grad_norm": 30.323673248291016,
      "learning_rate": 0.0001082183403791444,
      "loss": 0.1243,
      "step": 21440
    },
    {
      "epoch": 1.5398420674802584,
      "grad_norm": 9.569653511047363,
      "learning_rate": 0.00010816516444657148,
      "loss": 0.1065,
      "step": 21450
    },
    {
      "epoch": 1.5405599425699927,
      "grad_norm": 18.139915466308594,
      "learning_rate": 0.00010811198851399857,
      "loss": 0.2719,
      "step": 21460
    },
    {
      "epoch": 1.541277817659727,
      "grad_norm": 0.12252264469861984,
      "learning_rate": 0.00010805881258142565,
      "loss": 0.097,
      "step": 21470
    },
    {
      "epoch": 1.5419956927494616,
      "grad_norm": 0.11391738057136536,
      "learning_rate": 0.00010800563664885273,
      "loss": 0.317,
      "step": 21480
    },
    {
      "epoch": 1.542713567839196,
      "grad_norm": 8.044400215148926,
      "learning_rate": 0.00010795246071627982,
      "loss": 0.139,
      "step": 21490
    },
    {
      "epoch": 1.5434314429289304,
      "grad_norm": 0.05304792523384094,
      "learning_rate": 0.0001078992847837069,
      "loss": 0.2109,
      "step": 21500
    },
    {
      "epoch": 1.5441493180186647,
      "grad_norm": 2.2960691452026367,
      "learning_rate": 0.00010784610885113398,
      "loss": 0.0757,
      "step": 21510
    },
    {
      "epoch": 1.544867193108399,
      "grad_norm": 5.761720657348633,
      "learning_rate": 0.00010779293291856107,
      "loss": 0.0513,
      "step": 21520
    },
    {
      "epoch": 1.5455850681981336,
      "grad_norm": 0.04062656685709953,
      "learning_rate": 0.00010773975698598815,
      "loss": 0.4089,
      "step": 21530
    },
    {
      "epoch": 1.5463029432878679,
      "grad_norm": 0.013169686309993267,
      "learning_rate": 0.00010768658105341522,
      "loss": 0.1363,
      "step": 21540
    },
    {
      "epoch": 1.5470208183776024,
      "grad_norm": 0.4053550660610199,
      "learning_rate": 0.0001076334051208423,
      "loss": 0.1306,
      "step": 21550
    },
    {
      "epoch": 1.5477386934673367,
      "grad_norm": 25.10284423828125,
      "learning_rate": 0.00010758022918826939,
      "loss": 0.3677,
      "step": 21560
    },
    {
      "epoch": 1.548456568557071,
      "grad_norm": 1.2927236557006836,
      "learning_rate": 0.00010752705325569647,
      "loss": 0.1304,
      "step": 21570
    },
    {
      "epoch": 1.5491744436468053,
      "grad_norm": 5.9438252449035645,
      "learning_rate": 0.00010747387732312355,
      "loss": 0.2474,
      "step": 21580
    },
    {
      "epoch": 1.5498923187365399,
      "grad_norm": 14.513540267944336,
      "learning_rate": 0.00010742070139055064,
      "loss": 0.0595,
      "step": 21590
    },
    {
      "epoch": 1.5506101938262742,
      "grad_norm": 6.146848201751709,
      "learning_rate": 0.00010736752545797772,
      "loss": 0.0654,
      "step": 21600
    },
    {
      "epoch": 1.5513280689160087,
      "grad_norm": 0.011861296370625496,
      "learning_rate": 0.0001073143495254048,
      "loss": 0.094,
      "step": 21610
    },
    {
      "epoch": 1.552045944005743,
      "grad_norm": 6.364260196685791,
      "learning_rate": 0.0001072611735928319,
      "loss": 0.0233,
      "step": 21620
    },
    {
      "epoch": 1.5527638190954773,
      "grad_norm": 0.4720797538757324,
      "learning_rate": 0.00010720799766025898,
      "loss": 0.1017,
      "step": 21630
    },
    {
      "epoch": 1.5534816941852116,
      "grad_norm": 0.07471313327550888,
      "learning_rate": 0.00010715482172768606,
      "loss": 0.1208,
      "step": 21640
    },
    {
      "epoch": 1.5541995692749462,
      "grad_norm": 0.7294214963912964,
      "learning_rate": 0.00010710164579511315,
      "loss": 0.1744,
      "step": 21650
    },
    {
      "epoch": 1.5549174443646807,
      "grad_norm": 38.99738311767578,
      "learning_rate": 0.00010704846986254023,
      "loss": 0.1055,
      "step": 21660
    },
    {
      "epoch": 1.555635319454415,
      "grad_norm": 0.00427702022716403,
      "learning_rate": 0.0001069952939299673,
      "loss": 0.1616,
      "step": 21670
    },
    {
      "epoch": 1.5563531945441493,
      "grad_norm": 16.877260208129883,
      "learning_rate": 0.00010694211799739437,
      "loss": 0.3163,
      "step": 21680
    },
    {
      "epoch": 1.5570710696338836,
      "grad_norm": 15.939504623413086,
      "learning_rate": 0.00010688894206482147,
      "loss": 0.0992,
      "step": 21690
    },
    {
      "epoch": 1.557788944723618,
      "grad_norm": 22.76539421081543,
      "learning_rate": 0.00010683576613224855,
      "loss": 0.1808,
      "step": 21700
    },
    {
      "epoch": 1.5585068198133525,
      "grad_norm": 0.7103939056396484,
      "learning_rate": 0.00010678259019967563,
      "loss": 0.0251,
      "step": 21710
    },
    {
      "epoch": 1.559224694903087,
      "grad_norm": 3.5151681900024414,
      "learning_rate": 0.00010672941426710272,
      "loss": 0.1303,
      "step": 21720
    },
    {
      "epoch": 1.5599425699928213,
      "grad_norm": 0.08987791836261749,
      "learning_rate": 0.0001066762383345298,
      "loss": 0.1686,
      "step": 21730
    },
    {
      "epoch": 1.5606604450825556,
      "grad_norm": 6.201633930206299,
      "learning_rate": 0.00010662306240195688,
      "loss": 0.1082,
      "step": 21740
    },
    {
      "epoch": 1.56137832017229,
      "grad_norm": 17.532442092895508,
      "learning_rate": 0.00010656988646938397,
      "loss": 0.2198,
      "step": 21750
    },
    {
      "epoch": 1.5620961952620243,
      "grad_norm": 9.978069305419922,
      "learning_rate": 0.00010651671053681105,
      "loss": 0.244,
      "step": 21760
    },
    {
      "epoch": 1.5628140703517588,
      "grad_norm": 2.010669231414795,
      "learning_rate": 0.00010646353460423813,
      "loss": 0.0848,
      "step": 21770
    },
    {
      "epoch": 1.5635319454414933,
      "grad_norm": 15.742578506469727,
      "learning_rate": 0.0001064103586716652,
      "loss": 0.1744,
      "step": 21780
    },
    {
      "epoch": 1.5642498205312276,
      "grad_norm": 0.05866030976176262,
      "learning_rate": 0.0001063571827390923,
      "loss": 0.3155,
      "step": 21790
    },
    {
      "epoch": 1.564967695620962,
      "grad_norm": 0.2800873816013336,
      "learning_rate": 0.00010630400680651937,
      "loss": 0.1963,
      "step": 21800
    },
    {
      "epoch": 1.5656855707106963,
      "grad_norm": 0.09058208763599396,
      "learning_rate": 0.00010625083087394645,
      "loss": 0.1252,
      "step": 21810
    },
    {
      "epoch": 1.5664034458004306,
      "grad_norm": 0.011572733521461487,
      "learning_rate": 0.00010619765494137354,
      "loss": 0.0691,
      "step": 21820
    },
    {
      "epoch": 1.567121320890165,
      "grad_norm": 0.2087826281785965,
      "learning_rate": 0.00010614447900880062,
      "loss": 0.0824,
      "step": 21830
    },
    {
      "epoch": 1.5678391959798996,
      "grad_norm": 2.4846155643463135,
      "learning_rate": 0.0001060913030762277,
      "loss": 0.14,
      "step": 21840
    },
    {
      "epoch": 1.568557071069634,
      "grad_norm": 0.015545370057225227,
      "learning_rate": 0.00010603812714365479,
      "loss": 0.1837,
      "step": 21850
    },
    {
      "epoch": 1.5692749461593682,
      "grad_norm": 0.03677419573068619,
      "learning_rate": 0.00010598495121108187,
      "loss": 0.0415,
      "step": 21860
    },
    {
      "epoch": 1.5699928212491026,
      "grad_norm": 25.104732513427734,
      "learning_rate": 0.00010593177527850895,
      "loss": 0.2375,
      "step": 21870
    },
    {
      "epoch": 1.570710696338837,
      "grad_norm": 0.017256781458854675,
      "learning_rate": 0.00010587859934593602,
      "loss": 0.0858,
      "step": 21880
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.00241511850617826,
      "learning_rate": 0.00010582542341336312,
      "loss": 0.1404,
      "step": 21890
    },
    {
      "epoch": 1.572146446518306,
      "grad_norm": 0.05383528769016266,
      "learning_rate": 0.0001057722474807902,
      "loss": 0.0356,
      "step": 21900
    },
    {
      "epoch": 1.5728643216080402,
      "grad_norm": 25.109952926635742,
      "learning_rate": 0.00010571907154821727,
      "loss": 0.2033,
      "step": 21910
    },
    {
      "epoch": 1.5735821966977745,
      "grad_norm": 0.3241197466850281,
      "learning_rate": 0.00010566589561564438,
      "loss": 0.3601,
      "step": 21920
    },
    {
      "epoch": 1.5743000717875089,
      "grad_norm": 0.03647085279226303,
      "learning_rate": 0.00010561271968307144,
      "loss": 0.1329,
      "step": 21930
    },
    {
      "epoch": 1.5750179468772434,
      "grad_norm": 17.74012565612793,
      "learning_rate": 0.00010555954375049852,
      "loss": 0.1251,
      "step": 21940
    },
    {
      "epoch": 1.5757358219669777,
      "grad_norm": 6.085402011871338,
      "learning_rate": 0.00010550636781792563,
      "loss": 0.1085,
      "step": 21950
    },
    {
      "epoch": 1.5764536970567122,
      "grad_norm": 4.006132125854492,
      "learning_rate": 0.0001054531918853527,
      "loss": 0.0083,
      "step": 21960
    },
    {
      "epoch": 1.5771715721464465,
      "grad_norm": 0.011998699977993965,
      "learning_rate": 0.00010540001595277977,
      "loss": 0.2382,
      "step": 21970
    },
    {
      "epoch": 1.5778894472361809,
      "grad_norm": 0.43792444467544556,
      "learning_rate": 0.00010534684002020685,
      "loss": 0.3584,
      "step": 21980
    },
    {
      "epoch": 1.5786073223259152,
      "grad_norm": 16.90440559387207,
      "learning_rate": 0.00010529366408763395,
      "loss": 0.4744,
      "step": 21990
    },
    {
      "epoch": 1.5793251974156497,
      "grad_norm": 0.13718664646148682,
      "learning_rate": 0.00010524048815506103,
      "loss": 0.1047,
      "step": 22000
    },
    {
      "epoch": 1.5800430725053842,
      "grad_norm": 12.469085693359375,
      "learning_rate": 0.0001051873122224881,
      "loss": 0.0363,
      "step": 22010
    },
    {
      "epoch": 1.5807609475951185,
      "grad_norm": 15.991337776184082,
      "learning_rate": 0.0001051341362899152,
      "loss": 0.2811,
      "step": 22020
    },
    {
      "epoch": 1.5814788226848528,
      "grad_norm": 0.012987138703465462,
      "learning_rate": 0.00010508096035734228,
      "loss": 0.028,
      "step": 22030
    },
    {
      "epoch": 1.5821966977745872,
      "grad_norm": 10.726179122924805,
      "learning_rate": 0.00010502778442476934,
      "loss": 0.0805,
      "step": 22040
    },
    {
      "epoch": 1.5829145728643215,
      "grad_norm": 0.6076287627220154,
      "learning_rate": 0.00010497460849219645,
      "loss": 0.0641,
      "step": 22050
    },
    {
      "epoch": 1.583632447954056,
      "grad_norm": 10.294221878051758,
      "learning_rate": 0.00010492143255962352,
      "loss": 0.2951,
      "step": 22060
    },
    {
      "epoch": 1.5843503230437905,
      "grad_norm": 0.04232165962457657,
      "learning_rate": 0.0001048682566270506,
      "loss": 0.1426,
      "step": 22070
    },
    {
      "epoch": 1.5850681981335248,
      "grad_norm": 0.46006837487220764,
      "learning_rate": 0.00010481508069447768,
      "loss": 0.1285,
      "step": 22080
    },
    {
      "epoch": 1.5857860732232592,
      "grad_norm": 9.901103973388672,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.1656,
      "step": 22090
    },
    {
      "epoch": 1.5865039483129935,
      "grad_norm": 2.392866373062134,
      "learning_rate": 0.00010470872882933185,
      "loss": 0.0267,
      "step": 22100
    },
    {
      "epoch": 1.5872218234027278,
      "grad_norm": 19.86527442932129,
      "learning_rate": 0.00010465555289675893,
      "loss": 0.1595,
      "step": 22110
    },
    {
      "epoch": 1.5879396984924623,
      "grad_norm": 0.11684700101613998,
      "learning_rate": 0.00010460237696418602,
      "loss": 0.0678,
      "step": 22120
    },
    {
      "epoch": 1.5886575735821968,
      "grad_norm": 0.7407024502754211,
      "learning_rate": 0.0001045492010316131,
      "loss": 0.2273,
      "step": 22130
    },
    {
      "epoch": 1.5893754486719311,
      "grad_norm": 12.4498291015625,
      "learning_rate": 0.00010449602509904018,
      "loss": 0.1158,
      "step": 22140
    },
    {
      "epoch": 1.5900933237616655,
      "grad_norm": 0.050014592707157135,
      "learning_rate": 0.00010444284916646727,
      "loss": 0.0968,
      "step": 22150
    },
    {
      "epoch": 1.5908111988513998,
      "grad_norm": 18.84868621826172,
      "learning_rate": 0.00010438967323389435,
      "loss": 0.1827,
      "step": 22160
    },
    {
      "epoch": 1.5915290739411343,
      "grad_norm": 0.029618989676237106,
      "learning_rate": 0.00010433649730132142,
      "loss": 0.1503,
      "step": 22170
    },
    {
      "epoch": 1.5922469490308686,
      "grad_norm": 29.381704330444336,
      "learning_rate": 0.0001042833213687485,
      "loss": 0.162,
      "step": 22180
    },
    {
      "epoch": 1.5929648241206031,
      "grad_norm": 17.951705932617188,
      "learning_rate": 0.00010423014543617559,
      "loss": 0.1102,
      "step": 22190
    },
    {
      "epoch": 1.5936826992103375,
      "grad_norm": 9.098572731018066,
      "learning_rate": 0.00010417696950360267,
      "loss": 0.1212,
      "step": 22200
    },
    {
      "epoch": 1.5944005743000718,
      "grad_norm": 0.014826072379946709,
      "learning_rate": 0.00010412379357102975,
      "loss": 0.1323,
      "step": 22210
    },
    {
      "epoch": 1.595118449389806,
      "grad_norm": 4.05559778213501,
      "learning_rate": 0.00010407061763845684,
      "loss": 0.2632,
      "step": 22220
    },
    {
      "epoch": 1.5958363244795406,
      "grad_norm": 11.616257667541504,
      "learning_rate": 0.00010401744170588392,
      "loss": 0.2569,
      "step": 22230
    },
    {
      "epoch": 1.596554199569275,
      "grad_norm": 0.43780022859573364,
      "learning_rate": 0.000103964265773311,
      "loss": 0.1212,
      "step": 22240
    },
    {
      "epoch": 1.5972720746590094,
      "grad_norm": 4.295921802520752,
      "learning_rate": 0.0001039110898407381,
      "loss": 0.0295,
      "step": 22250
    },
    {
      "epoch": 1.5979899497487438,
      "grad_norm": 0.17633584141731262,
      "learning_rate": 0.00010385791390816517,
      "loss": 0.003,
      "step": 22260
    },
    {
      "epoch": 1.598707824838478,
      "grad_norm": 0.017519179731607437,
      "learning_rate": 0.00010380473797559225,
      "loss": 0.0655,
      "step": 22270
    },
    {
      "epoch": 1.5994256999282124,
      "grad_norm": 19.69925308227539,
      "learning_rate": 0.00010375156204301932,
      "loss": 0.1856,
      "step": 22280
    },
    {
      "epoch": 1.600143575017947,
      "grad_norm": 0.005922972224652767,
      "learning_rate": 0.00010369838611044643,
      "loss": 0.0556,
      "step": 22290
    },
    {
      "epoch": 1.6008614501076812,
      "grad_norm": 0.05597199872136116,
      "learning_rate": 0.00010364521017787349,
      "loss": 0.0531,
      "step": 22300
    },
    {
      "epoch": 1.6015793251974157,
      "grad_norm": 0.37128952145576477,
      "learning_rate": 0.00010359203424530057,
      "loss": 0.0422,
      "step": 22310
    },
    {
      "epoch": 1.60229720028715,
      "grad_norm": 12.518686294555664,
      "learning_rate": 0.00010353885831272767,
      "loss": 0.1455,
      "step": 22320
    },
    {
      "epoch": 1.6030150753768844,
      "grad_norm": 0.05933619663119316,
      "learning_rate": 0.00010348568238015474,
      "loss": 0.4558,
      "step": 22330
    },
    {
      "epoch": 1.6037329504666187,
      "grad_norm": 0.005697854794561863,
      "learning_rate": 0.00010343250644758182,
      "loss": 0.1188,
      "step": 22340
    },
    {
      "epoch": 1.6044508255563532,
      "grad_norm": 6.263818264007568,
      "learning_rate": 0.00010337933051500892,
      "loss": 0.1824,
      "step": 22350
    },
    {
      "epoch": 1.6051687006460877,
      "grad_norm": 12.31165885925293,
      "learning_rate": 0.000103326154582436,
      "loss": 0.0884,
      "step": 22360
    },
    {
      "epoch": 1.605886575735822,
      "grad_norm": 8.930685997009277,
      "learning_rate": 0.00010327297864986308,
      "loss": 0.1698,
      "step": 22370
    },
    {
      "epoch": 1.6066044508255564,
      "grad_norm": 12.23808765411377,
      "learning_rate": 0.00010321980271729017,
      "loss": 0.1489,
      "step": 22380
    },
    {
      "epoch": 1.6073223259152907,
      "grad_norm": 0.5121167302131653,
      "learning_rate": 0.00010316662678471725,
      "loss": 0.2589,
      "step": 22390
    },
    {
      "epoch": 1.608040201005025,
      "grad_norm": 11.120949745178223,
      "learning_rate": 0.00010311345085214433,
      "loss": 0.3434,
      "step": 22400
    },
    {
      "epoch": 1.6087580760947595,
      "grad_norm": 0.06684409826993942,
      "learning_rate": 0.0001030602749195714,
      "loss": 0.3104,
      "step": 22410
    },
    {
      "epoch": 1.609475951184494,
      "grad_norm": 0.12752562761306763,
      "learning_rate": 0.0001030070989869985,
      "loss": 0.0773,
      "step": 22420
    },
    {
      "epoch": 1.6101938262742284,
      "grad_norm": 0.019420228898525238,
      "learning_rate": 0.00010295392305442557,
      "loss": 0.3288,
      "step": 22430
    },
    {
      "epoch": 1.6109117013639627,
      "grad_norm": 3.3984878063201904,
      "learning_rate": 0.00010290074712185265,
      "loss": 0.1552,
      "step": 22440
    },
    {
      "epoch": 1.611629576453697,
      "grad_norm": 0.22871249914169312,
      "learning_rate": 0.00010284757118927974,
      "loss": 0.1181,
      "step": 22450
    },
    {
      "epoch": 1.6123474515434313,
      "grad_norm": 0.05868116766214371,
      "learning_rate": 0.00010279439525670682,
      "loss": 0.0113,
      "step": 22460
    },
    {
      "epoch": 1.6130653266331658,
      "grad_norm": 12.852622985839844,
      "learning_rate": 0.0001027412193241339,
      "loss": 0.0706,
      "step": 22470
    },
    {
      "epoch": 1.6137832017229004,
      "grad_norm": 17.411081314086914,
      "learning_rate": 0.00010268804339156099,
      "loss": 0.1797,
      "step": 22480
    },
    {
      "epoch": 1.6145010768126347,
      "grad_norm": 3.941762685775757,
      "learning_rate": 0.00010263486745898807,
      "loss": 0.2026,
      "step": 22490
    },
    {
      "epoch": 1.615218951902369,
      "grad_norm": 0.23169218003749847,
      "learning_rate": 0.00010258169152641515,
      "loss": 0.1057,
      "step": 22500
    },
    {
      "epoch": 1.6159368269921033,
      "grad_norm": 18.201602935791016,
      "learning_rate": 0.00010252851559384222,
      "loss": 0.22,
      "step": 22510
    },
    {
      "epoch": 1.6166547020818378,
      "grad_norm": 0.17433258891105652,
      "learning_rate": 0.00010247533966126932,
      "loss": 0.1086,
      "step": 22520
    },
    {
      "epoch": 1.6173725771715721,
      "grad_norm": 19.050207138061523,
      "learning_rate": 0.0001024221637286964,
      "loss": 0.0558,
      "step": 22530
    },
    {
      "epoch": 1.6180904522613067,
      "grad_norm": 22.546802520751953,
      "learning_rate": 0.00010236898779612347,
      "loss": 0.1564,
      "step": 22540
    },
    {
      "epoch": 1.618808327351041,
      "grad_norm": 0.5873876214027405,
      "learning_rate": 0.00010231581186355057,
      "loss": 0.1546,
      "step": 22550
    },
    {
      "epoch": 1.6195262024407753,
      "grad_norm": 0.019510159268975258,
      "learning_rate": 0.00010226263593097764,
      "loss": 0.5121,
      "step": 22560
    },
    {
      "epoch": 1.6202440775305096,
      "grad_norm": 16.255617141723633,
      "learning_rate": 0.00010220945999840472,
      "loss": 0.1976,
      "step": 22570
    },
    {
      "epoch": 1.6209619526202441,
      "grad_norm": 28.534475326538086,
      "learning_rate": 0.00010215628406583181,
      "loss": 0.1896,
      "step": 22580
    },
    {
      "epoch": 1.6216798277099784,
      "grad_norm": 0.008854544721543789,
      "learning_rate": 0.00010210310813325889,
      "loss": 0.098,
      "step": 22590
    },
    {
      "epoch": 1.622397702799713,
      "grad_norm": 0.18358184397220612,
      "learning_rate": 0.00010204993220068597,
      "loss": 0.5386,
      "step": 22600
    },
    {
      "epoch": 1.6231155778894473,
      "grad_norm": 2.887664318084717,
      "learning_rate": 0.00010199675626811305,
      "loss": 0.144,
      "step": 22610
    },
    {
      "epoch": 1.6238334529791816,
      "grad_norm": 23.68422508239746,
      "learning_rate": 0.00010194358033554014,
      "loss": 0.264,
      "step": 22620
    },
    {
      "epoch": 1.624551328068916,
      "grad_norm": 30.973236083984375,
      "learning_rate": 0.00010189040440296722,
      "loss": 0.1875,
      "step": 22630
    },
    {
      "epoch": 1.6252692031586504,
      "grad_norm": 18.22920036315918,
      "learning_rate": 0.00010183722847039429,
      "loss": 0.2944,
      "step": 22640
    },
    {
      "epoch": 1.6259870782483847,
      "grad_norm": 0.0765799805521965,
      "learning_rate": 0.0001017840525378214,
      "loss": 0.2216,
      "step": 22650
    },
    {
      "epoch": 1.6267049533381193,
      "grad_norm": 9.46467113494873,
      "learning_rate": 0.00010173087660524848,
      "loss": 0.2238,
      "step": 22660
    },
    {
      "epoch": 1.6274228284278536,
      "grad_norm": 1.5372289419174194,
      "learning_rate": 0.00010167770067267554,
      "loss": 0.1153,
      "step": 22670
    },
    {
      "epoch": 1.6281407035175879,
      "grad_norm": 0.16122397780418396,
      "learning_rate": 0.00010162452474010265,
      "loss": 0.0914,
      "step": 22680
    },
    {
      "epoch": 1.6288585786073222,
      "grad_norm": 17.125286102294922,
      "learning_rate": 0.00010157134880752971,
      "loss": 0.0576,
      "step": 22690
    },
    {
      "epoch": 1.6295764536970567,
      "grad_norm": 0.04602816328406334,
      "learning_rate": 0.0001015181728749568,
      "loss": 0.0596,
      "step": 22700
    },
    {
      "epoch": 1.6302943287867913,
      "grad_norm": 0.6743527054786682,
      "learning_rate": 0.00010146499694238387,
      "loss": 0.2172,
      "step": 22710
    },
    {
      "epoch": 1.6310122038765256,
      "grad_norm": 0.016819726675748825,
      "learning_rate": 0.00010141182100981097,
      "loss": 0.0215,
      "step": 22720
    },
    {
      "epoch": 1.6317300789662599,
      "grad_norm": 0.035047370940446854,
      "learning_rate": 0.00010135864507723805,
      "loss": 0.0882,
      "step": 22730
    },
    {
      "epoch": 1.6324479540559942,
      "grad_norm": 0.03146537393331528,
      "learning_rate": 0.00010130546914466513,
      "loss": 0.0662,
      "step": 22740
    },
    {
      "epoch": 1.6331658291457285,
      "grad_norm": 0.00970323383808136,
      "learning_rate": 0.00010125229321209222,
      "loss": 0.2772,
      "step": 22750
    },
    {
      "epoch": 1.633883704235463,
      "grad_norm": 0.10233530402183533,
      "learning_rate": 0.0001011991172795193,
      "loss": 0.2062,
      "step": 22760
    },
    {
      "epoch": 1.6346015793251976,
      "grad_norm": 0.07304677367210388,
      "learning_rate": 0.00010114594134694638,
      "loss": 0.0923,
      "step": 22770
    },
    {
      "epoch": 1.6353194544149319,
      "grad_norm": 14.05142879486084,
      "learning_rate": 0.00010109276541437347,
      "loss": 0.1853,
      "step": 22780
    },
    {
      "epoch": 1.6360373295046662,
      "grad_norm": 25.022119522094727,
      "learning_rate": 0.00010103958948180055,
      "loss": 0.2147,
      "step": 22790
    },
    {
      "epoch": 1.6367552045944005,
      "grad_norm": 0.08360590040683746,
      "learning_rate": 0.00010098641354922762,
      "loss": 0.1564,
      "step": 22800
    },
    {
      "epoch": 1.6374730796841348,
      "grad_norm": 0.03224130719900131,
      "learning_rate": 0.0001009332376166547,
      "loss": 0.0725,
      "step": 22810
    },
    {
      "epoch": 1.6381909547738693,
      "grad_norm": 0.7376675605773926,
      "learning_rate": 0.00010088006168408179,
      "loss": 0.1668,
      "step": 22820
    },
    {
      "epoch": 1.6389088298636039,
      "grad_norm": 7.7415361404418945,
      "learning_rate": 0.00010082688575150887,
      "loss": 0.0503,
      "step": 22830
    },
    {
      "epoch": 1.6396267049533382,
      "grad_norm": 0.027189146727323532,
      "learning_rate": 0.00010077370981893595,
      "loss": 0.0082,
      "step": 22840
    },
    {
      "epoch": 1.6403445800430725,
      "grad_norm": 0.07742409408092499,
      "learning_rate": 0.00010072053388636304,
      "loss": 0.114,
      "step": 22850
    },
    {
      "epoch": 1.6410624551328068,
      "grad_norm": 0.30494317412376404,
      "learning_rate": 0.00010066735795379012,
      "loss": 0.0864,
      "step": 22860
    },
    {
      "epoch": 1.6417803302225413,
      "grad_norm": 0.031814295798540115,
      "learning_rate": 0.0001006141820212172,
      "loss": 0.132,
      "step": 22870
    },
    {
      "epoch": 1.6424982053122756,
      "grad_norm": 0.005080582108348608,
      "learning_rate": 0.00010056100608864429,
      "loss": 0.0178,
      "step": 22880
    },
    {
      "epoch": 1.6432160804020102,
      "grad_norm": 0.01031913235783577,
      "learning_rate": 0.00010050783015607137,
      "loss": 0.0854,
      "step": 22890
    },
    {
      "epoch": 1.6439339554917445,
      "grad_norm": 9.44592571258545,
      "learning_rate": 0.00010045465422349845,
      "loss": 0.0696,
      "step": 22900
    },
    {
      "epoch": 1.6446518305814788,
      "grad_norm": 0.009378808550536633,
      "learning_rate": 0.00010040147829092552,
      "loss": 0.2875,
      "step": 22910
    },
    {
      "epoch": 1.645369705671213,
      "grad_norm": 0.029652049764990807,
      "learning_rate": 0.00010034830235835262,
      "loss": 0.2082,
      "step": 22920
    },
    {
      "epoch": 1.6460875807609476,
      "grad_norm": 10.82992172241211,
      "learning_rate": 0.00010029512642577969,
      "loss": 0.4149,
      "step": 22930
    },
    {
      "epoch": 1.646805455850682,
      "grad_norm": 0.6561086773872375,
      "learning_rate": 0.00010024195049320677,
      "loss": 0.0132,
      "step": 22940
    },
    {
      "epoch": 1.6475233309404165,
      "grad_norm": 0.028546536341309547,
      "learning_rate": 0.00010018877456063386,
      "loss": 0.0826,
      "step": 22950
    },
    {
      "epoch": 1.6482412060301508,
      "grad_norm": 0.13551850616931915,
      "learning_rate": 0.00010013559862806094,
      "loss": 0.1315,
      "step": 22960
    },
    {
      "epoch": 1.648959081119885,
      "grad_norm": 0.02543538250029087,
      "learning_rate": 0.00010008242269548802,
      "loss": 0.086,
      "step": 22970
    },
    {
      "epoch": 1.6496769562096194,
      "grad_norm": 0.041755687445402145,
      "learning_rate": 0.00010002924676291512,
      "loss": 0.1743,
      "step": 22980
    },
    {
      "epoch": 1.650394831299354,
      "grad_norm": 4.023508548736572,
      "learning_rate": 9.99760708303422e-05,
      "loss": 0.0817,
      "step": 22990
    },
    {
      "epoch": 1.6511127063890882,
      "grad_norm": 0.32603853940963745,
      "learning_rate": 9.992289489776927e-05,
      "loss": 0.159,
      "step": 23000
    },
    {
      "epoch": 1.6518305814788228,
      "grad_norm": 0.08421403169631958,
      "learning_rate": 9.986971896519635e-05,
      "loss": 0.1519,
      "step": 23010
    },
    {
      "epoch": 1.652548456568557,
      "grad_norm": 0.001492637093178928,
      "learning_rate": 9.981654303262345e-05,
      "loss": 0.0369,
      "step": 23020
    },
    {
      "epoch": 1.6532663316582914,
      "grad_norm": 11.497231483459473,
      "learning_rate": 9.976336710005053e-05,
      "loss": 0.1621,
      "step": 23030
    },
    {
      "epoch": 1.6539842067480257,
      "grad_norm": 0.001729791983962059,
      "learning_rate": 9.97101911674776e-05,
      "loss": 0.1517,
      "step": 23040
    },
    {
      "epoch": 1.6547020818377602,
      "grad_norm": 0.00554951885715127,
      "learning_rate": 9.965701523490469e-05,
      "loss": 0.2598,
      "step": 23050
    },
    {
      "epoch": 1.6554199569274948,
      "grad_norm": 0.9853953123092651,
      "learning_rate": 9.960383930233176e-05,
      "loss": 0.1309,
      "step": 23060
    },
    {
      "epoch": 1.656137832017229,
      "grad_norm": 1.1018658876419067,
      "learning_rate": 9.955066336975886e-05,
      "loss": 0.0645,
      "step": 23070
    },
    {
      "epoch": 1.6568557071069634,
      "grad_norm": 19.34296417236328,
      "learning_rate": 9.949748743718594e-05,
      "loss": 0.5058,
      "step": 23080
    },
    {
      "epoch": 1.6575735821966977,
      "grad_norm": 1.909698724746704,
      "learning_rate": 9.944431150461302e-05,
      "loss": 0.1394,
      "step": 23090
    },
    {
      "epoch": 1.658291457286432,
      "grad_norm": 9.09581470489502,
      "learning_rate": 9.93911355720401e-05,
      "loss": 0.0154,
      "step": 23100
    },
    {
      "epoch": 1.6590093323761665,
      "grad_norm": 0.098023422062397,
      "learning_rate": 9.933795963946718e-05,
      "loss": 0.5596,
      "step": 23110
    },
    {
      "epoch": 1.659727207465901,
      "grad_norm": 0.03382726386189461,
      "learning_rate": 9.928478370689427e-05,
      "loss": 0.1962,
      "step": 23120
    },
    {
      "epoch": 1.6604450825556354,
      "grad_norm": 0.7127995491027832,
      "learning_rate": 9.923160777432135e-05,
      "loss": 0.009,
      "step": 23130
    },
    {
      "epoch": 1.6611629576453697,
      "grad_norm": 13.109672546386719,
      "learning_rate": 9.917843184174843e-05,
      "loss": 0.0997,
      "step": 23140
    },
    {
      "epoch": 1.661880832735104,
      "grad_norm": 5.406836986541748,
      "learning_rate": 9.912525590917551e-05,
      "loss": 0.1553,
      "step": 23150
    },
    {
      "epoch": 1.6625987078248383,
      "grad_norm": 0.49163171648979187,
      "learning_rate": 9.90720799766026e-05,
      "loss": 0.2803,
      "step": 23160
    },
    {
      "epoch": 1.6633165829145728,
      "grad_norm": 22.617263793945312,
      "learning_rate": 9.901890404402968e-05,
      "loss": 0.0766,
      "step": 23170
    },
    {
      "epoch": 1.6640344580043074,
      "grad_norm": 1.985392689704895,
      "learning_rate": 9.896572811145676e-05,
      "loss": 0.13,
      "step": 23180
    },
    {
      "epoch": 1.6647523330940417,
      "grad_norm": 0.02550933137536049,
      "learning_rate": 9.891255217888384e-05,
      "loss": 0.1161,
      "step": 23190
    },
    {
      "epoch": 1.665470208183776,
      "grad_norm": 0.18019579350948334,
      "learning_rate": 9.885937624631092e-05,
      "loss": 0.17,
      "step": 23200
    },
    {
      "epoch": 1.6661880832735103,
      "grad_norm": 0.5134957432746887,
      "learning_rate": 9.880620031373801e-05,
      "loss": 0.1395,
      "step": 23210
    },
    {
      "epoch": 1.6669059583632448,
      "grad_norm": 0.47427845001220703,
      "learning_rate": 9.875302438116509e-05,
      "loss": 0.0373,
      "step": 23220
    },
    {
      "epoch": 1.6676238334529792,
      "grad_norm": 2.029019832611084,
      "learning_rate": 9.869984844859217e-05,
      "loss": 0.1728,
      "step": 23230
    },
    {
      "epoch": 1.6683417085427137,
      "grad_norm": 0.16566285490989685,
      "learning_rate": 9.864667251601925e-05,
      "loss": 0.2141,
      "step": 23240
    },
    {
      "epoch": 1.669059583632448,
      "grad_norm": 1.2802091836929321,
      "learning_rate": 9.859349658344633e-05,
      "loss": 0.0367,
      "step": 23250
    },
    {
      "epoch": 1.6697774587221823,
      "grad_norm": 20.219066619873047,
      "learning_rate": 9.854032065087342e-05,
      "loss": 0.2434,
      "step": 23260
    },
    {
      "epoch": 1.6704953338119166,
      "grad_norm": 18.325340270996094,
      "learning_rate": 9.84871447183005e-05,
      "loss": 0.1635,
      "step": 23270
    },
    {
      "epoch": 1.6712132089016511,
      "grad_norm": 0.061322763562202454,
      "learning_rate": 9.843396878572758e-05,
      "loss": 0.2499,
      "step": 23280
    },
    {
      "epoch": 1.6719310839913855,
      "grad_norm": 1.716892957687378,
      "learning_rate": 9.838079285315467e-05,
      "loss": 0.0062,
      "step": 23290
    },
    {
      "epoch": 1.67264895908112,
      "grad_norm": 4.576338768005371,
      "learning_rate": 9.832761692058174e-05,
      "loss": 0.0439,
      "step": 23300
    },
    {
      "epoch": 1.6733668341708543,
      "grad_norm": 19.15918731689453,
      "learning_rate": 9.827444098800883e-05,
      "loss": 0.3168,
      "step": 23310
    },
    {
      "epoch": 1.6740847092605886,
      "grad_norm": 12.742942810058594,
      "learning_rate": 9.822126505543591e-05,
      "loss": 0.0246,
      "step": 23320
    },
    {
      "epoch": 1.674802584350323,
      "grad_norm": 0.08237291127443314,
      "learning_rate": 9.816808912286299e-05,
      "loss": 0.4062,
      "step": 23330
    },
    {
      "epoch": 1.6755204594400575,
      "grad_norm": 0.23709964752197266,
      "learning_rate": 9.811491319029009e-05,
      "loss": 0.1126,
      "step": 23340
    },
    {
      "epoch": 1.6762383345297918,
      "grad_norm": 0.17369958758354187,
      "learning_rate": 9.806173725771715e-05,
      "loss": 0.0225,
      "step": 23350
    },
    {
      "epoch": 1.6769562096195263,
      "grad_norm": 0.3319706916809082,
      "learning_rate": 9.800856132514424e-05,
      "loss": 0.0919,
      "step": 23360
    },
    {
      "epoch": 1.6776740847092606,
      "grad_norm": 0.004795867018401623,
      "learning_rate": 9.795538539257132e-05,
      "loss": 0.1105,
      "step": 23370
    },
    {
      "epoch": 1.678391959798995,
      "grad_norm": 0.010644322261214256,
      "learning_rate": 9.79022094599984e-05,
      "loss": 0.142,
      "step": 23380
    },
    {
      "epoch": 1.6791098348887292,
      "grad_norm": 0.009760178625583649,
      "learning_rate": 9.78490335274255e-05,
      "loss": 0.0127,
      "step": 23390
    },
    {
      "epoch": 1.6798277099784638,
      "grad_norm": 0.023619303479790688,
      "learning_rate": 9.779585759485256e-05,
      "loss": 0.0719,
      "step": 23400
    },
    {
      "epoch": 1.6805455850681983,
      "grad_norm": 2.2294535636901855,
      "learning_rate": 9.774268166227966e-05,
      "loss": 0.216,
      "step": 23410
    },
    {
      "epoch": 1.6812634601579326,
      "grad_norm": 0.040990449488162994,
      "learning_rate": 9.768950572970675e-05,
      "loss": 0.3985,
      "step": 23420
    },
    {
      "epoch": 1.681981335247667,
      "grad_norm": 6.50404691696167,
      "learning_rate": 9.763632979713381e-05,
      "loss": 0.1624,
      "step": 23430
    },
    {
      "epoch": 1.6826992103374012,
      "grad_norm": 0.011241610161960125,
      "learning_rate": 9.758315386456091e-05,
      "loss": 0.1915,
      "step": 23440
    },
    {
      "epoch": 1.6834170854271355,
      "grad_norm": 17.14116668701172,
      "learning_rate": 9.752997793198799e-05,
      "loss": 0.2304,
      "step": 23450
    },
    {
      "epoch": 1.68413496051687,
      "grad_norm": 0.05824265256524086,
      "learning_rate": 9.747680199941507e-05,
      "loss": 0.0461,
      "step": 23460
    },
    {
      "epoch": 1.6848528356066046,
      "grad_norm": 3.2944061756134033,
      "learning_rate": 9.742362606684216e-05,
      "loss": 0.2003,
      "step": 23470
    },
    {
      "epoch": 1.685570710696339,
      "grad_norm": 13.107295989990234,
      "learning_rate": 9.737045013426923e-05,
      "loss": 0.1187,
      "step": 23480
    },
    {
      "epoch": 1.6862885857860732,
      "grad_norm": 0.053209349513053894,
      "learning_rate": 9.731727420169632e-05,
      "loss": 0.2886,
      "step": 23490
    },
    {
      "epoch": 1.6870064608758075,
      "grad_norm": 1.1490888595581055,
      "learning_rate": 9.726409826912341e-05,
      "loss": 0.0461,
      "step": 23500
    },
    {
      "epoch": 1.6877243359655418,
      "grad_norm": 44.13519287109375,
      "learning_rate": 9.721092233655048e-05,
      "loss": 0.1585,
      "step": 23510
    },
    {
      "epoch": 1.6884422110552764,
      "grad_norm": 0.03865555301308632,
      "learning_rate": 9.715774640397757e-05,
      "loss": 0.1816,
      "step": 23520
    },
    {
      "epoch": 1.689160086145011,
      "grad_norm": 15.77775764465332,
      "learning_rate": 9.710457047140465e-05,
      "loss": 0.2346,
      "step": 23530
    },
    {
      "epoch": 1.6898779612347452,
      "grad_norm": 2.956526756286621,
      "learning_rate": 9.705139453883173e-05,
      "loss": 0.2213,
      "step": 23540
    },
    {
      "epoch": 1.6905958363244795,
      "grad_norm": 6.263036727905273,
      "learning_rate": 9.699821860625882e-05,
      "loss": 0.2361,
      "step": 23550
    },
    {
      "epoch": 1.6913137114142138,
      "grad_norm": 0.05068254470825195,
      "learning_rate": 9.694504267368589e-05,
      "loss": 0.4882,
      "step": 23560
    },
    {
      "epoch": 1.6920315865039484,
      "grad_norm": 0.006734287366271019,
      "learning_rate": 9.689186674111298e-05,
      "loss": 0.0058,
      "step": 23570
    },
    {
      "epoch": 1.6927494615936827,
      "grad_norm": 0.022133421152830124,
      "learning_rate": 9.683869080854006e-05,
      "loss": 0.1085,
      "step": 23580
    },
    {
      "epoch": 1.6934673366834172,
      "grad_norm": 15.249712944030762,
      "learning_rate": 9.678551487596714e-05,
      "loss": 0.3266,
      "step": 23590
    },
    {
      "epoch": 1.6941852117731515,
      "grad_norm": 0.06818530708551407,
      "learning_rate": 9.673233894339423e-05,
      "loss": 0.3096,
      "step": 23600
    },
    {
      "epoch": 1.6949030868628858,
      "grad_norm": 0.4908759295940399,
      "learning_rate": 9.66791630108213e-05,
      "loss": 0.0697,
      "step": 23610
    },
    {
      "epoch": 1.6956209619526201,
      "grad_norm": 0.6524357795715332,
      "learning_rate": 9.662598707824839e-05,
      "loss": 0.3026,
      "step": 23620
    },
    {
      "epoch": 1.6963388370423547,
      "grad_norm": 0.009066508151590824,
      "learning_rate": 9.657281114567547e-05,
      "loss": 0.1452,
      "step": 23630
    },
    {
      "epoch": 1.697056712132089,
      "grad_norm": 27.959476470947266,
      "learning_rate": 9.651963521310255e-05,
      "loss": 0.1638,
      "step": 23640
    },
    {
      "epoch": 1.6977745872218235,
      "grad_norm": 10.952719688415527,
      "learning_rate": 9.646645928052964e-05,
      "loss": 0.2022,
      "step": 23650
    },
    {
      "epoch": 1.6984924623115578,
      "grad_norm": 9.793607711791992,
      "learning_rate": 9.641328334795672e-05,
      "loss": 0.147,
      "step": 23660
    },
    {
      "epoch": 1.6992103374012921,
      "grad_norm": 12.966941833496094,
      "learning_rate": 9.63601074153838e-05,
      "loss": 0.1705,
      "step": 23670
    },
    {
      "epoch": 1.6999282124910264,
      "grad_norm": 0.7746778726577759,
      "learning_rate": 9.630693148281088e-05,
      "loss": 0.1691,
      "step": 23680
    },
    {
      "epoch": 1.700646087580761,
      "grad_norm": 8.072113037109375,
      "learning_rate": 9.625375555023796e-05,
      "loss": 0.1824,
      "step": 23690
    },
    {
      "epoch": 1.7013639626704955,
      "grad_norm": 19.849660873413086,
      "learning_rate": 9.620057961766506e-05,
      "loss": 0.1453,
      "step": 23700
    },
    {
      "epoch": 1.7020818377602298,
      "grad_norm": 0.23295092582702637,
      "learning_rate": 9.614740368509214e-05,
      "loss": 0.214,
      "step": 23710
    },
    {
      "epoch": 1.7027997128499641,
      "grad_norm": 0.057404082268476486,
      "learning_rate": 9.609422775251921e-05,
      "loss": 0.0971,
      "step": 23720
    },
    {
      "epoch": 1.7035175879396984,
      "grad_norm": 0.02911083586513996,
      "learning_rate": 9.60410518199463e-05,
      "loss": 0.157,
      "step": 23730
    },
    {
      "epoch": 1.7042354630294327,
      "grad_norm": 3.2753148078918457,
      "learning_rate": 9.598787588737337e-05,
      "loss": 0.2046,
      "step": 23740
    },
    {
      "epoch": 1.7049533381191673,
      "grad_norm": 0.06295809894800186,
      "learning_rate": 9.593469995480047e-05,
      "loss": 0.2289,
      "step": 23750
    },
    {
      "epoch": 1.7056712132089018,
      "grad_norm": 2.2176194190979004,
      "learning_rate": 9.588152402222755e-05,
      "loss": 0.2453,
      "step": 23760
    },
    {
      "epoch": 1.7063890882986361,
      "grad_norm": 18.21997833251953,
      "learning_rate": 9.582834808965463e-05,
      "loss": 0.0825,
      "step": 23770
    },
    {
      "epoch": 1.7071069633883704,
      "grad_norm": 0.002781887073069811,
      "learning_rate": 9.57751721570817e-05,
      "loss": 0.0036,
      "step": 23780
    },
    {
      "epoch": 1.7078248384781047,
      "grad_norm": 28.820566177368164,
      "learning_rate": 9.57219962245088e-05,
      "loss": 0.328,
      "step": 23790
    },
    {
      "epoch": 1.708542713567839,
      "grad_norm": 0.01855333149433136,
      "learning_rate": 9.566882029193588e-05,
      "loss": 0.0596,
      "step": 23800
    },
    {
      "epoch": 1.7092605886575736,
      "grad_norm": 29.353378295898438,
      "learning_rate": 9.561564435936296e-05,
      "loss": 0.1202,
      "step": 23810
    },
    {
      "epoch": 1.709978463747308,
      "grad_norm": 0.46302342414855957,
      "learning_rate": 9.556246842679004e-05,
      "loss": 0.3601,
      "step": 23820
    },
    {
      "epoch": 1.7106963388370424,
      "grad_norm": 0.33401209115982056,
      "learning_rate": 9.550929249421712e-05,
      "loss": 0.4511,
      "step": 23830
    },
    {
      "epoch": 1.7114142139267767,
      "grad_norm": 0.015352900139987469,
      "learning_rate": 9.545611656164421e-05,
      "loss": 0.1261,
      "step": 23840
    },
    {
      "epoch": 1.712132089016511,
      "grad_norm": 17.496374130249023,
      "learning_rate": 9.540294062907129e-05,
      "loss": 0.0768,
      "step": 23850
    },
    {
      "epoch": 1.7128499641062453,
      "grad_norm": 14.805328369140625,
      "learning_rate": 9.534976469649837e-05,
      "loss": 0.147,
      "step": 23860
    },
    {
      "epoch": 1.7135678391959799,
      "grad_norm": 12.064725875854492,
      "learning_rate": 9.529658876392545e-05,
      "loss": 0.0921,
      "step": 23870
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.40822410583496094,
      "learning_rate": 9.524341283135253e-05,
      "loss": 0.1528,
      "step": 23880
    },
    {
      "epoch": 1.7150035893754487,
      "grad_norm": 2.698476552963257,
      "learning_rate": 9.519023689877962e-05,
      "loss": 0.0332,
      "step": 23890
    },
    {
      "epoch": 1.715721464465183,
      "grad_norm": 0.07287300378084183,
      "learning_rate": 9.51370609662067e-05,
      "loss": 0.1706,
      "step": 23900
    },
    {
      "epoch": 1.7164393395549173,
      "grad_norm": 0.007001729682087898,
      "learning_rate": 9.508388503363378e-05,
      "loss": 0.1092,
      "step": 23910
    },
    {
      "epoch": 1.7171572146446519,
      "grad_norm": 0.1572493314743042,
      "learning_rate": 9.503070910106087e-05,
      "loss": 0.1132,
      "step": 23920
    },
    {
      "epoch": 1.7178750897343862,
      "grad_norm": 0.06913544982671738,
      "learning_rate": 9.497753316848794e-05,
      "loss": 0.0372,
      "step": 23930
    },
    {
      "epoch": 1.7185929648241207,
      "grad_norm": 1.8698824644088745,
      "learning_rate": 9.492435723591503e-05,
      "loss": 0.0666,
      "step": 23940
    },
    {
      "epoch": 1.719310839913855,
      "grad_norm": 12.490949630737305,
      "learning_rate": 9.487118130334211e-05,
      "loss": 0.2329,
      "step": 23950
    },
    {
      "epoch": 1.7200287150035893,
      "grad_norm": 21.383407592773438,
      "learning_rate": 9.481800537076919e-05,
      "loss": 0.0756,
      "step": 23960
    },
    {
      "epoch": 1.7207465900933236,
      "grad_norm": 5.894167900085449,
      "learning_rate": 9.476482943819628e-05,
      "loss": 0.1955,
      "step": 23970
    },
    {
      "epoch": 1.7214644651830582,
      "grad_norm": 7.8095784187316895,
      "learning_rate": 9.471165350562335e-05,
      "loss": 0.0978,
      "step": 23980
    },
    {
      "epoch": 1.7221823402727925,
      "grad_norm": 46.472450256347656,
      "learning_rate": 9.465847757305044e-05,
      "loss": 0.5095,
      "step": 23990
    },
    {
      "epoch": 1.722900215362527,
      "grad_norm": 0.35099831223487854,
      "learning_rate": 9.460530164047752e-05,
      "loss": 0.2891,
      "step": 24000
    },
    {
      "epoch": 1.7236180904522613,
      "grad_norm": 1.0797765254974365,
      "learning_rate": 9.45521257079046e-05,
      "loss": 0.2793,
      "step": 24010
    },
    {
      "epoch": 1.7243359655419956,
      "grad_norm": 12.320660591125488,
      "learning_rate": 9.44989497753317e-05,
      "loss": 0.2966,
      "step": 24020
    },
    {
      "epoch": 1.72505384063173,
      "grad_norm": 25.178194046020508,
      "learning_rate": 9.444577384275876e-05,
      "loss": 0.1263,
      "step": 24030
    },
    {
      "epoch": 1.7257717157214645,
      "grad_norm": 0.06294797360897064,
      "learning_rate": 9.439259791018585e-05,
      "loss": 0.2711,
      "step": 24040
    },
    {
      "epoch": 1.726489590811199,
      "grad_norm": 2.190195322036743,
      "learning_rate": 9.433942197761295e-05,
      "loss": 0.1072,
      "step": 24050
    },
    {
      "epoch": 1.7272074659009333,
      "grad_norm": 0.18408873677253723,
      "learning_rate": 9.428624604504001e-05,
      "loss": 0.1537,
      "step": 24060
    },
    {
      "epoch": 1.7279253409906676,
      "grad_norm": 0.2784081995487213,
      "learning_rate": 9.42330701124671e-05,
      "loss": 0.0935,
      "step": 24070
    },
    {
      "epoch": 1.728643216080402,
      "grad_norm": 0.02504703588783741,
      "learning_rate": 9.417989417989419e-05,
      "loss": 0.2031,
      "step": 24080
    },
    {
      "epoch": 1.7293610911701363,
      "grad_norm": 14.539844512939453,
      "learning_rate": 9.412671824732126e-05,
      "loss": 0.3483,
      "step": 24090
    },
    {
      "epoch": 1.7300789662598708,
      "grad_norm": 7.557932376861572,
      "learning_rate": 9.407354231474836e-05,
      "loss": 0.0916,
      "step": 24100
    },
    {
      "epoch": 1.7307968413496053,
      "grad_norm": 14.636466979980469,
      "learning_rate": 9.402036638217542e-05,
      "loss": 0.2589,
      "step": 24110
    },
    {
      "epoch": 1.7315147164393396,
      "grad_norm": 0.023005397990345955,
      "learning_rate": 9.396719044960252e-05,
      "loss": 0.064,
      "step": 24120
    },
    {
      "epoch": 1.732232591529074,
      "grad_norm": 0.13585324585437775,
      "learning_rate": 9.39140145170296e-05,
      "loss": 0.0094,
      "step": 24130
    },
    {
      "epoch": 1.7329504666188082,
      "grad_norm": 14.29709529876709,
      "learning_rate": 9.386083858445668e-05,
      "loss": 0.2811,
      "step": 24140
    },
    {
      "epoch": 1.7336683417085426,
      "grad_norm": 0.024507619440555573,
      "learning_rate": 9.380766265188377e-05,
      "loss": 0.3027,
      "step": 24150
    },
    {
      "epoch": 1.734386216798277,
      "grad_norm": 14.114970207214355,
      "learning_rate": 9.375448671931083e-05,
      "loss": 0.3159,
      "step": 24160
    },
    {
      "epoch": 1.7351040918880116,
      "grad_norm": 0.8253015279769897,
      "learning_rate": 9.370131078673793e-05,
      "loss": 0.0314,
      "step": 24170
    },
    {
      "epoch": 1.735821966977746,
      "grad_norm": 12.342255592346191,
      "learning_rate": 9.364813485416501e-05,
      "loss": 0.2564,
      "step": 24180
    },
    {
      "epoch": 1.7365398420674802,
      "grad_norm": 21.102874755859375,
      "learning_rate": 9.359495892159209e-05,
      "loss": 0.2542,
      "step": 24190
    },
    {
      "epoch": 1.7372577171572146,
      "grad_norm": 21.53339385986328,
      "learning_rate": 9.354178298901918e-05,
      "loss": 0.3055,
      "step": 24200
    },
    {
      "epoch": 1.737975592246949,
      "grad_norm": 4.546843528747559,
      "learning_rate": 9.348860705644626e-05,
      "loss": 0.3912,
      "step": 24210
    },
    {
      "epoch": 1.7386934673366834,
      "grad_norm": 0.01359129510819912,
      "learning_rate": 9.343543112387334e-05,
      "loss": 0.0496,
      "step": 24220
    },
    {
      "epoch": 1.739411342426418,
      "grad_norm": 16.09521484375,
      "learning_rate": 9.338225519130043e-05,
      "loss": 0.0909,
      "step": 24230
    },
    {
      "epoch": 1.7401292175161522,
      "grad_norm": 1.2949634790420532,
      "learning_rate": 9.33290792587275e-05,
      "loss": 0.1384,
      "step": 24240
    },
    {
      "epoch": 1.7408470926058865,
      "grad_norm": 0.006498539820313454,
      "learning_rate": 9.327590332615459e-05,
      "loss": 0.02,
      "step": 24250
    },
    {
      "epoch": 1.7415649676956209,
      "grad_norm": 0.0034668853040784597,
      "learning_rate": 9.322272739358167e-05,
      "loss": 0.0084,
      "step": 24260
    },
    {
      "epoch": 1.7422828427853554,
      "grad_norm": 0.005864493083208799,
      "learning_rate": 9.316955146100875e-05,
      "loss": 0.1563,
      "step": 24270
    },
    {
      "epoch": 1.7430007178750897,
      "grad_norm": 10.2062406539917,
      "learning_rate": 9.311637552843584e-05,
      "loss": 0.3721,
      "step": 24280
    },
    {
      "epoch": 1.7437185929648242,
      "grad_norm": 5.828640937805176,
      "learning_rate": 9.306319959586292e-05,
      "loss": 0.0487,
      "step": 24290
    },
    {
      "epoch": 1.7444364680545585,
      "grad_norm": 16.03667449951172,
      "learning_rate": 9.301002366329e-05,
      "loss": 0.1525,
      "step": 24300
    },
    {
      "epoch": 1.7451543431442929,
      "grad_norm": 0.2838180959224701,
      "learning_rate": 9.295684773071708e-05,
      "loss": 0.0363,
      "step": 24310
    },
    {
      "epoch": 1.7458722182340272,
      "grad_norm": 2.8381552696228027,
      "learning_rate": 9.290367179814416e-05,
      "loss": 0.0854,
      "step": 24320
    },
    {
      "epoch": 1.7465900933237617,
      "grad_norm": 1.6370484828948975,
      "learning_rate": 9.285049586557125e-05,
      "loss": 0.6144,
      "step": 24330
    },
    {
      "epoch": 1.747307968413496,
      "grad_norm": 0.18122678995132446,
      "learning_rate": 9.279731993299833e-05,
      "loss": 0.1183,
      "step": 24340
    },
    {
      "epoch": 1.7480258435032305,
      "grad_norm": 6.3772501945495605,
      "learning_rate": 9.274414400042541e-05,
      "loss": 0.099,
      "step": 24350
    },
    {
      "epoch": 1.7487437185929648,
      "grad_norm": 16.51947593688965,
      "learning_rate": 9.269096806785249e-05,
      "loss": 0.0522,
      "step": 24360
    },
    {
      "epoch": 1.7494615936826992,
      "grad_norm": 0.9150474667549133,
      "learning_rate": 9.263779213527957e-05,
      "loss": 0.1074,
      "step": 24370
    },
    {
      "epoch": 1.7501794687724335,
      "grad_norm": 0.01906909979879856,
      "learning_rate": 9.258461620270666e-05,
      "loss": 0.1499,
      "step": 24380
    },
    {
      "epoch": 1.750897343862168,
      "grad_norm": 0.12742221355438232,
      "learning_rate": 9.253144027013374e-05,
      "loss": 0.0901,
      "step": 24390
    },
    {
      "epoch": 1.7516152189519025,
      "grad_norm": 0.9050431251525879,
      "learning_rate": 9.247826433756082e-05,
      "loss": 0.1113,
      "step": 24400
    },
    {
      "epoch": 1.7523330940416368,
      "grad_norm": 0.004900224041193724,
      "learning_rate": 9.24250884049879e-05,
      "loss": 0.0587,
      "step": 24410
    },
    {
      "epoch": 1.7530509691313712,
      "grad_norm": 0.10363461822271347,
      "learning_rate": 9.2371912472415e-05,
      "loss": 0.0013,
      "step": 24420
    },
    {
      "epoch": 1.7537688442211055,
      "grad_norm": 0.00518142431974411,
      "learning_rate": 9.231873653984208e-05,
      "loss": 0.3097,
      "step": 24430
    },
    {
      "epoch": 1.7544867193108398,
      "grad_norm": 0.07610712945461273,
      "learning_rate": 9.226556060726916e-05,
      "loss": 0.3997,
      "step": 24440
    },
    {
      "epoch": 1.7552045944005743,
      "grad_norm": 0.035366058349609375,
      "learning_rate": 9.221238467469623e-05,
      "loss": 0.0694,
      "step": 24450
    },
    {
      "epoch": 1.7559224694903088,
      "grad_norm": 10.458704948425293,
      "learning_rate": 9.215920874212331e-05,
      "loss": 0.1482,
      "step": 24460
    },
    {
      "epoch": 1.7566403445800431,
      "grad_norm": 2.3958663940429688,
      "learning_rate": 9.210603280955041e-05,
      "loss": 0.1343,
      "step": 24470
    },
    {
      "epoch": 1.7573582196697775,
      "grad_norm": 20.52143096923828,
      "learning_rate": 9.205285687697749e-05,
      "loss": 0.0536,
      "step": 24480
    },
    {
      "epoch": 1.7580760947595118,
      "grad_norm": 0.05318060889840126,
      "learning_rate": 9.199968094440457e-05,
      "loss": 0.1709,
      "step": 24490
    },
    {
      "epoch": 1.758793969849246,
      "grad_norm": 0.20664425194263458,
      "learning_rate": 9.194650501183165e-05,
      "loss": 0.1552,
      "step": 24500
    },
    {
      "epoch": 1.7595118449389806,
      "grad_norm": 8.532238960266113,
      "learning_rate": 9.189332907925873e-05,
      "loss": 0.1375,
      "step": 24510
    },
    {
      "epoch": 1.7602297200287151,
      "grad_norm": 0.03192117065191269,
      "learning_rate": 9.184015314668582e-05,
      "loss": 0.2284,
      "step": 24520
    },
    {
      "epoch": 1.7609475951184494,
      "grad_norm": 25.746091842651367,
      "learning_rate": 9.17869772141129e-05,
      "loss": 0.1002,
      "step": 24530
    },
    {
      "epoch": 1.7616654702081838,
      "grad_norm": 0.02926507778465748,
      "learning_rate": 9.173380128153998e-05,
      "loss": 0.0489,
      "step": 24540
    },
    {
      "epoch": 1.762383345297918,
      "grad_norm": 0.014639211818575859,
      "learning_rate": 9.168062534896707e-05,
      "loss": 0.2942,
      "step": 24550
    },
    {
      "epoch": 1.7631012203876526,
      "grad_norm": 0.012381958775222301,
      "learning_rate": 9.162744941639414e-05,
      "loss": 0.1273,
      "step": 24560
    },
    {
      "epoch": 1.763819095477387,
      "grad_norm": 0.7696174383163452,
      "learning_rate": 9.157427348382123e-05,
      "loss": 0.063,
      "step": 24570
    },
    {
      "epoch": 1.7645369705671214,
      "grad_norm": 3.6756861209869385,
      "learning_rate": 9.152109755124831e-05,
      "loss": 0.3095,
      "step": 24580
    },
    {
      "epoch": 1.7652548456568558,
      "grad_norm": 5.486376762390137,
      "learning_rate": 9.146792161867539e-05,
      "loss": 0.3239,
      "step": 24590
    },
    {
      "epoch": 1.76597272074659,
      "grad_norm": 1.7943413257598877,
      "learning_rate": 9.141474568610248e-05,
      "loss": 0.025,
      "step": 24600
    },
    {
      "epoch": 1.7666905958363244,
      "grad_norm": 10.20151424407959,
      "learning_rate": 9.136156975352955e-05,
      "loss": 0.1866,
      "step": 24610
    },
    {
      "epoch": 1.767408470926059,
      "grad_norm": 19.924633026123047,
      "learning_rate": 9.130839382095664e-05,
      "loss": 0.3429,
      "step": 24620
    },
    {
      "epoch": 1.7681263460157932,
      "grad_norm": 1.814720630645752,
      "learning_rate": 9.125521788838372e-05,
      "loss": 0.0565,
      "step": 24630
    },
    {
      "epoch": 1.7688442211055277,
      "grad_norm": 17.69355583190918,
      "learning_rate": 9.12020419558108e-05,
      "loss": 0.0741,
      "step": 24640
    },
    {
      "epoch": 1.769562096195262,
      "grad_norm": 15.227067947387695,
      "learning_rate": 9.114886602323789e-05,
      "loss": 0.1442,
      "step": 24650
    },
    {
      "epoch": 1.7702799712849964,
      "grad_norm": 0.5244192481040955,
      "learning_rate": 9.109569009066496e-05,
      "loss": 0.1039,
      "step": 24660
    },
    {
      "epoch": 1.7709978463747307,
      "grad_norm": 0.030267322435975075,
      "learning_rate": 9.104251415809205e-05,
      "loss": 0.2738,
      "step": 24670
    },
    {
      "epoch": 1.7717157214644652,
      "grad_norm": 0.1327982097864151,
      "learning_rate": 9.098933822551914e-05,
      "loss": 0.0897,
      "step": 24680
    },
    {
      "epoch": 1.7724335965541995,
      "grad_norm": 35.21099090576172,
      "learning_rate": 9.093616229294621e-05,
      "loss": 0.2762,
      "step": 24690
    },
    {
      "epoch": 1.773151471643934,
      "grad_norm": 0.8794258236885071,
      "learning_rate": 9.08829863603733e-05,
      "loss": 0.201,
      "step": 24700
    },
    {
      "epoch": 1.7738693467336684,
      "grad_norm": 0.27967962622642517,
      "learning_rate": 9.082981042780038e-05,
      "loss": 0.105,
      "step": 24710
    },
    {
      "epoch": 1.7745872218234027,
      "grad_norm": 0.014954755082726479,
      "learning_rate": 9.077663449522746e-05,
      "loss": 0.0671,
      "step": 24720
    },
    {
      "epoch": 1.775305096913137,
      "grad_norm": 0.005644889548420906,
      "learning_rate": 9.072345856265456e-05,
      "loss": 0.0403,
      "step": 24730
    },
    {
      "epoch": 1.7760229720028715,
      "grad_norm": 0.013958466239273548,
      "learning_rate": 9.067028263008162e-05,
      "loss": 0.4641,
      "step": 24740
    },
    {
      "epoch": 1.776740847092606,
      "grad_norm": 10.800578117370605,
      "learning_rate": 9.061710669750871e-05,
      "loss": 0.0979,
      "step": 24750
    },
    {
      "epoch": 1.7774587221823404,
      "grad_norm": 0.020905254408717155,
      "learning_rate": 9.05639307649358e-05,
      "loss": 0.0152,
      "step": 24760
    },
    {
      "epoch": 1.7781765972720747,
      "grad_norm": 0.04112463444471359,
      "learning_rate": 9.051075483236287e-05,
      "loss": 0.2357,
      "step": 24770
    },
    {
      "epoch": 1.778894472361809,
      "grad_norm": 0.11595875024795532,
      "learning_rate": 9.045757889978997e-05,
      "loss": 0.0843,
      "step": 24780
    },
    {
      "epoch": 1.7796123474515433,
      "grad_norm": 1.953765869140625,
      "learning_rate": 9.040440296721703e-05,
      "loss": 0.1096,
      "step": 24790
    },
    {
      "epoch": 1.7803302225412778,
      "grad_norm": 0.6830888390541077,
      "learning_rate": 9.035122703464413e-05,
      "loss": 0.0809,
      "step": 24800
    },
    {
      "epoch": 1.7810480976310124,
      "grad_norm": 25.074687957763672,
      "learning_rate": 9.02980511020712e-05,
      "loss": 0.3216,
      "step": 24810
    },
    {
      "epoch": 1.7817659727207467,
      "grad_norm": 9.743034362792969,
      "learning_rate": 9.024487516949828e-05,
      "loss": 0.3511,
      "step": 24820
    },
    {
      "epoch": 1.782483847810481,
      "grad_norm": 1.0355056524276733,
      "learning_rate": 9.019169923692538e-05,
      "loss": 0.0746,
      "step": 24830
    },
    {
      "epoch": 1.7832017229002153,
      "grad_norm": 1.3163572549819946,
      "learning_rate": 9.013852330435246e-05,
      "loss": 0.2435,
      "step": 24840
    },
    {
      "epoch": 1.7839195979899496,
      "grad_norm": 0.01385891530662775,
      "learning_rate": 9.008534737177954e-05,
      "loss": 0.0323,
      "step": 24850
    },
    {
      "epoch": 1.7846374730796841,
      "grad_norm": 0.49945512413978577,
      "learning_rate": 9.003217143920662e-05,
      "loss": 0.1564,
      "step": 24860
    },
    {
      "epoch": 1.7853553481694187,
      "grad_norm": 15.095918655395508,
      "learning_rate": 8.99789955066337e-05,
      "loss": 0.2217,
      "step": 24870
    },
    {
      "epoch": 1.786073223259153,
      "grad_norm": 0.05702308192849159,
      "learning_rate": 8.992581957406079e-05,
      "loss": 0.4527,
      "step": 24880
    },
    {
      "epoch": 1.7867910983488873,
      "grad_norm": 17.656322479248047,
      "learning_rate": 8.987264364148787e-05,
      "loss": 0.0603,
      "step": 24890
    },
    {
      "epoch": 1.7875089734386216,
      "grad_norm": 0.07394762337207794,
      "learning_rate": 8.981946770891495e-05,
      "loss": 0.1064,
      "step": 24900
    },
    {
      "epoch": 1.7882268485283561,
      "grad_norm": 0.043608132749795914,
      "learning_rate": 8.976629177634203e-05,
      "loss": 0.0963,
      "step": 24910
    },
    {
      "epoch": 1.7889447236180904,
      "grad_norm": 14.177700996398926,
      "learning_rate": 8.97131158437691e-05,
      "loss": 0.1349,
      "step": 24920
    },
    {
      "epoch": 1.789662598707825,
      "grad_norm": 0.009667455218732357,
      "learning_rate": 8.96599399111962e-05,
      "loss": 0.1998,
      "step": 24930
    },
    {
      "epoch": 1.7903804737975593,
      "grad_norm": 16.919775009155273,
      "learning_rate": 8.960676397862328e-05,
      "loss": 0.3455,
      "step": 24940
    },
    {
      "epoch": 1.7910983488872936,
      "grad_norm": 11.684746742248535,
      "learning_rate": 8.955358804605036e-05,
      "loss": 0.2739,
      "step": 24950
    },
    {
      "epoch": 1.7918162239770279,
      "grad_norm": 24.77497673034668,
      "learning_rate": 8.950041211347745e-05,
      "loss": 0.2054,
      "step": 24960
    },
    {
      "epoch": 1.7925340990667624,
      "grad_norm": 11.97361946105957,
      "learning_rate": 8.944723618090453e-05,
      "loss": 0.0575,
      "step": 24970
    },
    {
      "epoch": 1.7932519741564967,
      "grad_norm": 18.231796264648438,
      "learning_rate": 8.939406024833161e-05,
      "loss": 0.1235,
      "step": 24980
    },
    {
      "epoch": 1.7939698492462313,
      "grad_norm": 0.006766924634575844,
      "learning_rate": 8.934088431575869e-05,
      "loss": 0.0125,
      "step": 24990
    },
    {
      "epoch": 1.7946877243359656,
      "grad_norm": 1.599654197692871,
      "learning_rate": 8.928770838318577e-05,
      "loss": 0.1358,
      "step": 25000
    },
    {
      "epoch": 1.7954055994256999,
      "grad_norm": 11.317030906677246,
      "learning_rate": 8.923453245061286e-05,
      "loss": 0.1107,
      "step": 25010
    },
    {
      "epoch": 1.7961234745154342,
      "grad_norm": 12.660192489624023,
      "learning_rate": 8.918135651803994e-05,
      "loss": 0.1493,
      "step": 25020
    },
    {
      "epoch": 1.7968413496051687,
      "grad_norm": 5.280001163482666,
      "learning_rate": 8.912818058546702e-05,
      "loss": 0.1644,
      "step": 25030
    },
    {
      "epoch": 1.797559224694903,
      "grad_norm": 13.381943702697754,
      "learning_rate": 8.90750046528941e-05,
      "loss": 0.0629,
      "step": 25040
    },
    {
      "epoch": 1.7982770997846376,
      "grad_norm": 1.860592007637024,
      "learning_rate": 8.902182872032118e-05,
      "loss": 0.1595,
      "step": 25050
    },
    {
      "epoch": 1.7989949748743719,
      "grad_norm": 0.043560825288295746,
      "learning_rate": 8.896865278774827e-05,
      "loss": 0.0784,
      "step": 25060
    },
    {
      "epoch": 1.7997128499641062,
      "grad_norm": 0.44913575053215027,
      "learning_rate": 8.891547685517535e-05,
      "loss": 0.0121,
      "step": 25070
    },
    {
      "epoch": 1.8004307250538405,
      "grad_norm": 5.216299533843994,
      "learning_rate": 8.886230092260243e-05,
      "loss": 0.0176,
      "step": 25080
    },
    {
      "epoch": 1.801148600143575,
      "grad_norm": 2.70037579536438,
      "learning_rate": 8.880912499002951e-05,
      "loss": 0.1352,
      "step": 25090
    },
    {
      "epoch": 1.8018664752333096,
      "grad_norm": 0.004860064946115017,
      "learning_rate": 8.87559490574566e-05,
      "loss": 0.0779,
      "step": 25100
    },
    {
      "epoch": 1.8025843503230439,
      "grad_norm": 0.24580374360084534,
      "learning_rate": 8.870277312488368e-05,
      "loss": 0.3005,
      "step": 25110
    },
    {
      "epoch": 1.8033022254127782,
      "grad_norm": 0.21333478391170502,
      "learning_rate": 8.864959719231076e-05,
      "loss": 0.2142,
      "step": 25120
    },
    {
      "epoch": 1.8040201005025125,
      "grad_norm": 0.25233063101768494,
      "learning_rate": 8.859642125973784e-05,
      "loss": 0.2248,
      "step": 25130
    },
    {
      "epoch": 1.8047379755922468,
      "grad_norm": 10.5123929977417,
      "learning_rate": 8.854324532716492e-05,
      "loss": 0.2633,
      "step": 25140
    },
    {
      "epoch": 1.8054558506819813,
      "grad_norm": 0.025092914700508118,
      "learning_rate": 8.849006939459202e-05,
      "loss": 0.1866,
      "step": 25150
    },
    {
      "epoch": 1.8061737257717159,
      "grad_norm": 14.809197425842285,
      "learning_rate": 8.84368934620191e-05,
      "loss": 0.3677,
      "step": 25160
    },
    {
      "epoch": 1.8068916008614502,
      "grad_norm": 0.0061084190383553505,
      "learning_rate": 8.838371752944618e-05,
      "loss": 0.1139,
      "step": 25170
    },
    {
      "epoch": 1.8076094759511845,
      "grad_norm": 0.06382817775011063,
      "learning_rate": 8.833054159687327e-05,
      "loss": 0.0366,
      "step": 25180
    },
    {
      "epoch": 1.8083273510409188,
      "grad_norm": 11.238335609436035,
      "learning_rate": 8.827736566430033e-05,
      "loss": 0.1424,
      "step": 25190
    },
    {
      "epoch": 1.809045226130653,
      "grad_norm": 0.3626538813114166,
      "learning_rate": 8.822418973172743e-05,
      "loss": 0.1857,
      "step": 25200
    },
    {
      "epoch": 1.8097631012203876,
      "grad_norm": 0.003379745641723275,
      "learning_rate": 8.817101379915451e-05,
      "loss": 0.02,
      "step": 25210
    },
    {
      "epoch": 1.8104809763101222,
      "grad_norm": 0.26312318444252014,
      "learning_rate": 8.811783786658159e-05,
      "loss": 0.102,
      "step": 25220
    },
    {
      "epoch": 1.8111988513998565,
      "grad_norm": 18.612472534179688,
      "learning_rate": 8.806466193400868e-05,
      "loss": 0.2791,
      "step": 25230
    },
    {
      "epoch": 1.8119167264895908,
      "grad_norm": 19.720991134643555,
      "learning_rate": 8.801148600143575e-05,
      "loss": 0.1265,
      "step": 25240
    },
    {
      "epoch": 1.812634601579325,
      "grad_norm": 0.0018941567977890372,
      "learning_rate": 8.795831006886284e-05,
      "loss": 0.0887,
      "step": 25250
    },
    {
      "epoch": 1.8133524766690596,
      "grad_norm": 0.3263638913631439,
      "learning_rate": 8.790513413628992e-05,
      "loss": 0.0651,
      "step": 25260
    },
    {
      "epoch": 1.814070351758794,
      "grad_norm": 0.08307401835918427,
      "learning_rate": 8.7851958203717e-05,
      "loss": 0.0074,
      "step": 25270
    },
    {
      "epoch": 1.8147882268485285,
      "grad_norm": 0.2656897306442261,
      "learning_rate": 8.779878227114409e-05,
      "loss": 0.1026,
      "step": 25280
    },
    {
      "epoch": 1.8155061019382628,
      "grad_norm": 13.423126220703125,
      "learning_rate": 8.774560633857116e-05,
      "loss": 0.0278,
      "step": 25290
    },
    {
      "epoch": 1.816223977027997,
      "grad_norm": 10.615422248840332,
      "learning_rate": 8.769243040599825e-05,
      "loss": 0.2446,
      "step": 25300
    },
    {
      "epoch": 1.8169418521177314,
      "grad_norm": 0.0011071166954934597,
      "learning_rate": 8.763925447342534e-05,
      "loss": 0.1281,
      "step": 25310
    },
    {
      "epoch": 1.817659727207466,
      "grad_norm": 4.117055416107178,
      "learning_rate": 8.758607854085241e-05,
      "loss": 0.2018,
      "step": 25320
    },
    {
      "epoch": 1.8183776022972002,
      "grad_norm": 10.001319885253906,
      "learning_rate": 8.75329026082795e-05,
      "loss": 0.0817,
      "step": 25330
    },
    {
      "epoch": 1.8190954773869348,
      "grad_norm": 0.07269760221242905,
      "learning_rate": 8.747972667570658e-05,
      "loss": 0.1029,
      "step": 25340
    },
    {
      "epoch": 1.819813352476669,
      "grad_norm": 0.45524874329566956,
      "learning_rate": 8.742655074313366e-05,
      "loss": 0.191,
      "step": 25350
    },
    {
      "epoch": 1.8205312275664034,
      "grad_norm": 8.984281539916992,
      "learning_rate": 8.737337481056075e-05,
      "loss": 0.2043,
      "step": 25360
    },
    {
      "epoch": 1.8212491026561377,
      "grad_norm": 0.7915439009666443,
      "learning_rate": 8.732019887798782e-05,
      "loss": 0.0137,
      "step": 25370
    },
    {
      "epoch": 1.8219669777458722,
      "grad_norm": 0.030913978815078735,
      "learning_rate": 8.726702294541491e-05,
      "loss": 0.3548,
      "step": 25380
    },
    {
      "epoch": 1.8226848528356066,
      "grad_norm": 2.5572509765625,
      "learning_rate": 8.721384701284199e-05,
      "loss": 0.1462,
      "step": 25390
    },
    {
      "epoch": 1.823402727925341,
      "grad_norm": 0.4849027991294861,
      "learning_rate": 8.716067108026907e-05,
      "loss": 0.1165,
      "step": 25400
    },
    {
      "epoch": 1.8241206030150754,
      "grad_norm": 21.025062561035156,
      "learning_rate": 8.710749514769616e-05,
      "loss": 0.1409,
      "step": 25410
    },
    {
      "epoch": 1.8248384781048097,
      "grad_norm": 23.715532302856445,
      "learning_rate": 8.705431921512323e-05,
      "loss": 0.2747,
      "step": 25420
    },
    {
      "epoch": 1.825556353194544,
      "grad_norm": 1.3010988235473633,
      "learning_rate": 8.700114328255032e-05,
      "loss": 0.0876,
      "step": 25430
    },
    {
      "epoch": 1.8262742282842785,
      "grad_norm": 2.897852659225464,
      "learning_rate": 8.69479673499774e-05,
      "loss": 0.0981,
      "step": 25440
    },
    {
      "epoch": 1.826992103374013,
      "grad_norm": 8.73204231262207,
      "learning_rate": 8.689479141740448e-05,
      "loss": 0.1745,
      "step": 25450
    },
    {
      "epoch": 1.8277099784637474,
      "grad_norm": 0.18871760368347168,
      "learning_rate": 8.684161548483158e-05,
      "loss": 0.0956,
      "step": 25460
    },
    {
      "epoch": 1.8284278535534817,
      "grad_norm": 5.294118404388428,
      "learning_rate": 8.678843955225866e-05,
      "loss": 0.1136,
      "step": 25470
    },
    {
      "epoch": 1.829145728643216,
      "grad_norm": 0.0834876075387001,
      "learning_rate": 8.673526361968573e-05,
      "loss": 0.0513,
      "step": 25480
    },
    {
      "epoch": 1.8298636037329503,
      "grad_norm": 0.0029404987581074238,
      "learning_rate": 8.668208768711281e-05,
      "loss": 0.1477,
      "step": 25490
    },
    {
      "epoch": 1.8305814788226848,
      "grad_norm": 1.0936863422393799,
      "learning_rate": 8.66289117545399e-05,
      "loss": 0.2235,
      "step": 25500
    },
    {
      "epoch": 1.8312993539124194,
      "grad_norm": 0.009067904204130173,
      "learning_rate": 8.657573582196699e-05,
      "loss": 0.2063,
      "step": 25510
    },
    {
      "epoch": 1.8320172290021537,
      "grad_norm": 16.350505828857422,
      "learning_rate": 8.652255988939407e-05,
      "loss": 0.2834,
      "step": 25520
    },
    {
      "epoch": 1.832735104091888,
      "grad_norm": 0.2945788502693176,
      "learning_rate": 8.646938395682115e-05,
      "loss": 0.0563,
      "step": 25530
    },
    {
      "epoch": 1.8334529791816223,
      "grad_norm": 16.110347747802734,
      "learning_rate": 8.641620802424823e-05,
      "loss": 0.0703,
      "step": 25540
    },
    {
      "epoch": 1.8341708542713566,
      "grad_norm": 0.07190795242786407,
      "learning_rate": 8.63630320916753e-05,
      "loss": 0.3452,
      "step": 25550
    },
    {
      "epoch": 1.8348887293610912,
      "grad_norm": 0.2736138701438904,
      "learning_rate": 8.63098561591024e-05,
      "loss": 0.0964,
      "step": 25560
    },
    {
      "epoch": 1.8356066044508257,
      "grad_norm": 0.00874122604727745,
      "learning_rate": 8.625668022652948e-05,
      "loss": 0.2714,
      "step": 25570
    },
    {
      "epoch": 1.83632447954056,
      "grad_norm": 0.008788594976067543,
      "learning_rate": 8.620350429395656e-05,
      "loss": 0.0649,
      "step": 25580
    },
    {
      "epoch": 1.8370423546302943,
      "grad_norm": 1.8814867734909058,
      "learning_rate": 8.615032836138364e-05,
      "loss": 0.1776,
      "step": 25590
    },
    {
      "epoch": 1.8377602297200286,
      "grad_norm": 18.250141143798828,
      "learning_rate": 8.609715242881073e-05,
      "loss": 0.1107,
      "step": 25600
    },
    {
      "epoch": 1.8384781048097631,
      "grad_norm": 17.113309860229492,
      "learning_rate": 8.604397649623781e-05,
      "loss": 0.1829,
      "step": 25610
    },
    {
      "epoch": 1.8391959798994975,
      "grad_norm": 1.6701878309249878,
      "learning_rate": 8.599080056366489e-05,
      "loss": 0.1306,
      "step": 25620
    },
    {
      "epoch": 1.839913854989232,
      "grad_norm": 0.5826166272163391,
      "learning_rate": 8.593762463109197e-05,
      "loss": 0.2902,
      "step": 25630
    },
    {
      "epoch": 1.8406317300789663,
      "grad_norm": 5.560284614562988,
      "learning_rate": 8.588444869851905e-05,
      "loss": 0.155,
      "step": 25640
    },
    {
      "epoch": 1.8413496051687006,
      "grad_norm": 1.5614993572235107,
      "learning_rate": 8.583127276594614e-05,
      "loss": 0.1186,
      "step": 25650
    },
    {
      "epoch": 1.842067480258435,
      "grad_norm": 3.9192490577697754,
      "learning_rate": 8.577809683337322e-05,
      "loss": 0.152,
      "step": 25660
    },
    {
      "epoch": 1.8427853553481695,
      "grad_norm": 30.63982391357422,
      "learning_rate": 8.57249209008003e-05,
      "loss": 0.1627,
      "step": 25670
    },
    {
      "epoch": 1.8435032304379038,
      "grad_norm": 0.22146905958652496,
      "learning_rate": 8.567174496822738e-05,
      "loss": 0.0376,
      "step": 25680
    },
    {
      "epoch": 1.8442211055276383,
      "grad_norm": 0.08786159008741379,
      "learning_rate": 8.561856903565447e-05,
      "loss": 0.0686,
      "step": 25690
    },
    {
      "epoch": 1.8449389806173726,
      "grad_norm": 19.61520004272461,
      "learning_rate": 8.556539310308155e-05,
      "loss": 0.137,
      "step": 25700
    },
    {
      "epoch": 1.845656855707107,
      "grad_norm": 12.136229515075684,
      "learning_rate": 8.551221717050863e-05,
      "loss": 0.1379,
      "step": 25710
    },
    {
      "epoch": 1.8463747307968412,
      "grad_norm": 8.852381706237793,
      "learning_rate": 8.545904123793571e-05,
      "loss": 0.083,
      "step": 25720
    },
    {
      "epoch": 1.8470926058865758,
      "grad_norm": 5.007494926452637,
      "learning_rate": 8.54058653053628e-05,
      "loss": 0.1144,
      "step": 25730
    },
    {
      "epoch": 1.8478104809763103,
      "grad_norm": 0.010912369005382061,
      "learning_rate": 8.535268937278988e-05,
      "loss": 0.0678,
      "step": 25740
    },
    {
      "epoch": 1.8485283560660446,
      "grad_norm": 5.0990424156188965,
      "learning_rate": 8.529951344021696e-05,
      "loss": 0.0733,
      "step": 25750
    },
    {
      "epoch": 1.849246231155779,
      "grad_norm": 3.0536062717437744,
      "learning_rate": 8.524633750764404e-05,
      "loss": 0.0617,
      "step": 25760
    },
    {
      "epoch": 1.8499641062455132,
      "grad_norm": 0.8651410341262817,
      "learning_rate": 8.519316157507112e-05,
      "loss": 0.0292,
      "step": 25770
    },
    {
      "epoch": 1.8506819813352475,
      "grad_norm": 0.660275936126709,
      "learning_rate": 8.513998564249821e-05,
      "loss": 0.2315,
      "step": 25780
    },
    {
      "epoch": 1.851399856424982,
      "grad_norm": 10.59653377532959,
      "learning_rate": 8.50868097099253e-05,
      "loss": 0.0214,
      "step": 25790
    },
    {
      "epoch": 1.8521177315147166,
      "grad_norm": 0.005653583910316229,
      "learning_rate": 8.503363377735237e-05,
      "loss": 0.15,
      "step": 25800
    },
    {
      "epoch": 1.852835606604451,
      "grad_norm": 0.7000471353530884,
      "learning_rate": 8.498045784477945e-05,
      "loss": 0.3824,
      "step": 25810
    },
    {
      "epoch": 1.8535534816941852,
      "grad_norm": 23.148746490478516,
      "learning_rate": 8.492728191220653e-05,
      "loss": 0.2481,
      "step": 25820
    },
    {
      "epoch": 1.8542713567839195,
      "grad_norm": 30.8814697265625,
      "learning_rate": 8.487410597963363e-05,
      "loss": 0.3266,
      "step": 25830
    },
    {
      "epoch": 1.8549892318736538,
      "grad_norm": 0.0719723030924797,
      "learning_rate": 8.48209300470607e-05,
      "loss": 0.0499,
      "step": 25840
    },
    {
      "epoch": 1.8557071069633884,
      "grad_norm": 0.06318062543869019,
      "learning_rate": 8.476775411448778e-05,
      "loss": 0.0323,
      "step": 25850
    },
    {
      "epoch": 1.856424982053123,
      "grad_norm": 0.7660349607467651,
      "learning_rate": 8.471457818191488e-05,
      "loss": 0.2938,
      "step": 25860
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 2.253455400466919,
      "learning_rate": 8.466140224934194e-05,
      "loss": 0.123,
      "step": 25870
    },
    {
      "epoch": 1.8578607322325915,
      "grad_norm": 0.8643945455551147,
      "learning_rate": 8.460822631676904e-05,
      "loss": 0.0949,
      "step": 25880
    },
    {
      "epoch": 1.8585786073223258,
      "grad_norm": 0.37936827540397644,
      "learning_rate": 8.455505038419612e-05,
      "loss": 0.1779,
      "step": 25890
    },
    {
      "epoch": 1.8592964824120601,
      "grad_norm": 0.6126671433448792,
      "learning_rate": 8.45018744516232e-05,
      "loss": 0.0569,
      "step": 25900
    },
    {
      "epoch": 1.8600143575017947,
      "grad_norm": 0.07643573731184006,
      "learning_rate": 8.444869851905029e-05,
      "loss": 0.2629,
      "step": 25910
    },
    {
      "epoch": 1.8607322325915292,
      "grad_norm": 1.3278616666793823,
      "learning_rate": 8.439552258647735e-05,
      "loss": 0.0844,
      "step": 25920
    },
    {
      "epoch": 1.8614501076812635,
      "grad_norm": 0.009464186616241932,
      "learning_rate": 8.434234665390445e-05,
      "loss": 0.0742,
      "step": 25930
    },
    {
      "epoch": 1.8621679827709978,
      "grad_norm": 0.39333677291870117,
      "learning_rate": 8.428917072133154e-05,
      "loss": 0.1083,
      "step": 25940
    },
    {
      "epoch": 1.8628858578607321,
      "grad_norm": 0.010395105928182602,
      "learning_rate": 8.42359947887586e-05,
      "loss": 0.2419,
      "step": 25950
    },
    {
      "epoch": 1.8636037329504667,
      "grad_norm": 4.828535556793213,
      "learning_rate": 8.41828188561857e-05,
      "loss": 0.2416,
      "step": 25960
    },
    {
      "epoch": 1.864321608040201,
      "grad_norm": 0.22879907488822937,
      "learning_rate": 8.412964292361278e-05,
      "loss": 0.1939,
      "step": 25970
    },
    {
      "epoch": 1.8650394831299355,
      "grad_norm": 13.449529647827148,
      "learning_rate": 8.407646699103986e-05,
      "loss": 0.2351,
      "step": 25980
    },
    {
      "epoch": 1.8657573582196698,
      "grad_norm": 2.75850772857666,
      "learning_rate": 8.402329105846695e-05,
      "loss": 0.2251,
      "step": 25990
    },
    {
      "epoch": 1.8664752333094041,
      "grad_norm": 0.005752613302320242,
      "learning_rate": 8.397011512589402e-05,
      "loss": 0.0543,
      "step": 26000
    },
    {
      "epoch": 1.8671931083991384,
      "grad_norm": 0.02536931075155735,
      "learning_rate": 8.391693919332111e-05,
      "loss": 0.1157,
      "step": 26010
    },
    {
      "epoch": 1.867910983488873,
      "grad_norm": 0.16043543815612793,
      "learning_rate": 8.386376326074819e-05,
      "loss": 0.1265,
      "step": 26020
    },
    {
      "epoch": 1.8686288585786073,
      "grad_norm": 17.188358306884766,
      "learning_rate": 8.381058732817527e-05,
      "loss": 0.2518,
      "step": 26030
    },
    {
      "epoch": 1.8693467336683418,
      "grad_norm": 17.02492904663086,
      "learning_rate": 8.375741139560236e-05,
      "loss": 0.0819,
      "step": 26040
    },
    {
      "epoch": 1.8700646087580761,
      "grad_norm": 13.226855278015137,
      "learning_rate": 8.370423546302943e-05,
      "loss": 0.1734,
      "step": 26050
    },
    {
      "epoch": 1.8707824838478104,
      "grad_norm": 0.2136637568473816,
      "learning_rate": 8.365105953045652e-05,
      "loss": 0.2069,
      "step": 26060
    },
    {
      "epoch": 1.8715003589375447,
      "grad_norm": 7.19990348815918,
      "learning_rate": 8.35978835978836e-05,
      "loss": 0.1,
      "step": 26070
    },
    {
      "epoch": 1.8722182340272793,
      "grad_norm": 0.0020347514655441046,
      "learning_rate": 8.354470766531068e-05,
      "loss": 0.0027,
      "step": 26080
    },
    {
      "epoch": 1.8729361091170138,
      "grad_norm": 15.33427619934082,
      "learning_rate": 8.349153173273777e-05,
      "loss": 0.1822,
      "step": 26090
    },
    {
      "epoch": 1.8736539842067481,
      "grad_norm": 0.004443536512553692,
      "learning_rate": 8.343835580016485e-05,
      "loss": 0.0335,
      "step": 26100
    },
    {
      "epoch": 1.8743718592964824,
      "grad_norm": 5.633415222167969,
      "learning_rate": 8.338517986759193e-05,
      "loss": 0.0315,
      "step": 26110
    },
    {
      "epoch": 1.8750897343862167,
      "grad_norm": 0.05666559189558029,
      "learning_rate": 8.333200393501901e-05,
      "loss": 0.4019,
      "step": 26120
    },
    {
      "epoch": 1.875807609475951,
      "grad_norm": 0.36333802342414856,
      "learning_rate": 8.327882800244609e-05,
      "loss": 0.0296,
      "step": 26130
    },
    {
      "epoch": 1.8765254845656856,
      "grad_norm": 0.16586843132972717,
      "learning_rate": 8.322565206987318e-05,
      "loss": 0.17,
      "step": 26140
    },
    {
      "epoch": 1.87724335965542,
      "grad_norm": 9.028572082519531,
      "learning_rate": 8.317247613730026e-05,
      "loss": 0.0309,
      "step": 26150
    },
    {
      "epoch": 1.8779612347451544,
      "grad_norm": 3.666814088821411,
      "learning_rate": 8.311930020472734e-05,
      "loss": 0.0911,
      "step": 26160
    },
    {
      "epoch": 1.8786791098348887,
      "grad_norm": 0.00524438451975584,
      "learning_rate": 8.306612427215442e-05,
      "loss": 0.0688,
      "step": 26170
    },
    {
      "epoch": 1.879396984924623,
      "grad_norm": 18.320920944213867,
      "learning_rate": 8.30129483395815e-05,
      "loss": 0.1299,
      "step": 26180
    },
    {
      "epoch": 1.8801148600143573,
      "grad_norm": 0.44536322355270386,
      "learning_rate": 8.29597724070086e-05,
      "loss": 0.2894,
      "step": 26190
    },
    {
      "epoch": 1.8808327351040919,
      "grad_norm": 0.012760051526129246,
      "learning_rate": 8.290659647443568e-05,
      "loss": 0.0411,
      "step": 26200
    },
    {
      "epoch": 1.8815506101938264,
      "grad_norm": 0.04733124375343323,
      "learning_rate": 8.285342054186275e-05,
      "loss": 0.3952,
      "step": 26210
    },
    {
      "epoch": 1.8822684852835607,
      "grad_norm": 0.8164160251617432,
      "learning_rate": 8.280024460928983e-05,
      "loss": 0.1558,
      "step": 26220
    },
    {
      "epoch": 1.882986360373295,
      "grad_norm": 0.3392156660556793,
      "learning_rate": 8.274706867671693e-05,
      "loss": 0.1714,
      "step": 26230
    },
    {
      "epoch": 1.8837042354630293,
      "grad_norm": 0.14107196033000946,
      "learning_rate": 8.269389274414401e-05,
      "loss": 0.1489,
      "step": 26240
    },
    {
      "epoch": 1.8844221105527639,
      "grad_norm": 0.0896724984049797,
      "learning_rate": 8.264071681157109e-05,
      "loss": 0.4371,
      "step": 26250
    },
    {
      "epoch": 1.8851399856424982,
      "grad_norm": 0.022670147940516472,
      "learning_rate": 8.258754087899817e-05,
      "loss": 0.4154,
      "step": 26260
    },
    {
      "epoch": 1.8858578607322327,
      "grad_norm": 0.3034951388835907,
      "learning_rate": 8.253436494642525e-05,
      "loss": 0.192,
      "step": 26270
    },
    {
      "epoch": 1.886575735821967,
      "grad_norm": 0.005135009530931711,
      "learning_rate": 8.248118901385234e-05,
      "loss": 0.1343,
      "step": 26280
    },
    {
      "epoch": 1.8872936109117013,
      "grad_norm": 0.008027472533285618,
      "learning_rate": 8.242801308127942e-05,
      "loss": 0.0279,
      "step": 26290
    },
    {
      "epoch": 1.8880114860014356,
      "grad_norm": 5.7126665115356445,
      "learning_rate": 8.23748371487065e-05,
      "loss": 0.2645,
      "step": 26300
    },
    {
      "epoch": 1.8887293610911702,
      "grad_norm": 20.708677291870117,
      "learning_rate": 8.232166121613358e-05,
      "loss": 0.0616,
      "step": 26310
    },
    {
      "epoch": 1.8894472361809045,
      "grad_norm": 5.681593418121338,
      "learning_rate": 8.226848528356066e-05,
      "loss": 0.3425,
      "step": 26320
    },
    {
      "epoch": 1.890165111270639,
      "grad_norm": 0.06923933327198029,
      "learning_rate": 8.221530935098775e-05,
      "loss": 0.295,
      "step": 26330
    },
    {
      "epoch": 1.8908829863603733,
      "grad_norm": 4.339682102203369,
      "learning_rate": 8.216213341841483e-05,
      "loss": 0.1797,
      "step": 26340
    },
    {
      "epoch": 1.8916008614501076,
      "grad_norm": 0.012983283028006554,
      "learning_rate": 8.210895748584191e-05,
      "loss": 0.0209,
      "step": 26350
    },
    {
      "epoch": 1.892318736539842,
      "grad_norm": 0.5283896923065186,
      "learning_rate": 8.2055781553269e-05,
      "loss": 0.0761,
      "step": 26360
    },
    {
      "epoch": 1.8930366116295765,
      "grad_norm": 0.16047969460487366,
      "learning_rate": 8.200260562069608e-05,
      "loss": 0.0408,
      "step": 26370
    },
    {
      "epoch": 1.8937544867193108,
      "grad_norm": 6.297918796539307,
      "learning_rate": 8.194942968812316e-05,
      "loss": 0.1192,
      "step": 26380
    },
    {
      "epoch": 1.8944723618090453,
      "grad_norm": 3.144834280014038,
      "learning_rate": 8.189625375555024e-05,
      "loss": 0.1664,
      "step": 26390
    },
    {
      "epoch": 1.8951902368987796,
      "grad_norm": 0.36062902212142944,
      "learning_rate": 8.184307782297732e-05,
      "loss": 0.0905,
      "step": 26400
    },
    {
      "epoch": 1.895908111988514,
      "grad_norm": 2.2365126609802246,
      "learning_rate": 8.178990189040441e-05,
      "loss": 0.1935,
      "step": 26410
    },
    {
      "epoch": 1.8966259870782483,
      "grad_norm": 2.136274814605713,
      "learning_rate": 8.173672595783149e-05,
      "loss": 0.079,
      "step": 26420
    },
    {
      "epoch": 1.8973438621679828,
      "grad_norm": 0.030718065798282623,
      "learning_rate": 8.168355002525857e-05,
      "loss": 0.0684,
      "step": 26430
    },
    {
      "epoch": 1.8980617372577173,
      "grad_norm": 2.7708678245544434,
      "learning_rate": 8.163037409268565e-05,
      "loss": 0.0807,
      "step": 26440
    },
    {
      "epoch": 1.8987796123474516,
      "grad_norm": 4.9806013107299805,
      "learning_rate": 8.157719816011273e-05,
      "loss": 0.047,
      "step": 26450
    },
    {
      "epoch": 1.899497487437186,
      "grad_norm": 17.35427474975586,
      "learning_rate": 8.152402222753982e-05,
      "loss": 0.1366,
      "step": 26460
    },
    {
      "epoch": 1.9002153625269202,
      "grad_norm": 0.04088716208934784,
      "learning_rate": 8.14708462949669e-05,
      "loss": 0.14,
      "step": 26470
    },
    {
      "epoch": 1.9009332376166546,
      "grad_norm": 16.590700149536133,
      "learning_rate": 8.141767036239398e-05,
      "loss": 0.1878,
      "step": 26480
    },
    {
      "epoch": 1.901651112706389,
      "grad_norm": 16.375293731689453,
      "learning_rate": 8.136449442982108e-05,
      "loss": 0.2013,
      "step": 26490
    },
    {
      "epoch": 1.9023689877961236,
      "grad_norm": 0.6512712836265564,
      "learning_rate": 8.131131849724814e-05,
      "loss": 0.2419,
      "step": 26500
    },
    {
      "epoch": 1.903086862885858,
      "grad_norm": 0.09434468299150467,
      "learning_rate": 8.125814256467523e-05,
      "loss": 0.017,
      "step": 26510
    },
    {
      "epoch": 1.9038047379755922,
      "grad_norm": 0.018369833007454872,
      "learning_rate": 8.120496663210231e-05,
      "loss": 0.0749,
      "step": 26520
    },
    {
      "epoch": 1.9045226130653266,
      "grad_norm": 1.479784369468689,
      "learning_rate": 8.11517906995294e-05,
      "loss": 0.0017,
      "step": 26530
    },
    {
      "epoch": 1.9052404881550609,
      "grad_norm": 0.022336680442094803,
      "learning_rate": 8.109861476695649e-05,
      "loss": 0.2186,
      "step": 26540
    },
    {
      "epoch": 1.9059583632447954,
      "grad_norm": 8.107298851013184,
      "learning_rate": 8.104543883438355e-05,
      "loss": 0.0695,
      "step": 26550
    },
    {
      "epoch": 1.90667623833453,
      "grad_norm": 0.007875016890466213,
      "learning_rate": 8.099226290181065e-05,
      "loss": 0.0368,
      "step": 26560
    },
    {
      "epoch": 1.9073941134242642,
      "grad_norm": 0.2988432049751282,
      "learning_rate": 8.093908696923773e-05,
      "loss": 0.3126,
      "step": 26570
    },
    {
      "epoch": 1.9081119885139985,
      "grad_norm": 4.150997161865234,
      "learning_rate": 8.08859110366648e-05,
      "loss": 0.3707,
      "step": 26580
    },
    {
      "epoch": 1.9088298636037329,
      "grad_norm": 0.13726972043514252,
      "learning_rate": 8.08327351040919e-05,
      "loss": 0.0902,
      "step": 26590
    },
    {
      "epoch": 1.9095477386934674,
      "grad_norm": 0.03501134738326073,
      "learning_rate": 8.077955917151896e-05,
      "loss": 0.2256,
      "step": 26600
    },
    {
      "epoch": 1.9102656137832017,
      "grad_norm": 5.013171672821045,
      "learning_rate": 8.072638323894606e-05,
      "loss": 0.0387,
      "step": 26610
    },
    {
      "epoch": 1.9109834888729362,
      "grad_norm": 0.005212008953094482,
      "learning_rate": 8.067320730637315e-05,
      "loss": 0.1704,
      "step": 26620
    },
    {
      "epoch": 1.9117013639626705,
      "grad_norm": 0.007751346565783024,
      "learning_rate": 8.062003137380022e-05,
      "loss": 0.1707,
      "step": 26630
    },
    {
      "epoch": 1.9124192390524049,
      "grad_norm": 0.005576079245656729,
      "learning_rate": 8.056685544122731e-05,
      "loss": 0.0397,
      "step": 26640
    },
    {
      "epoch": 1.9131371141421392,
      "grad_norm": 0.033721473067998886,
      "learning_rate": 8.051367950865439e-05,
      "loss": 0.4548,
      "step": 26650
    },
    {
      "epoch": 1.9138549892318737,
      "grad_norm": 0.029362821951508522,
      "learning_rate": 8.046050357608147e-05,
      "loss": 0.0826,
      "step": 26660
    },
    {
      "epoch": 1.914572864321608,
      "grad_norm": 0.3967721462249756,
      "learning_rate": 8.040732764350856e-05,
      "loss": 0.1421,
      "step": 26670
    },
    {
      "epoch": 1.9152907394113425,
      "grad_norm": 0.013524332083761692,
      "learning_rate": 8.035415171093563e-05,
      "loss": 0.233,
      "step": 26680
    },
    {
      "epoch": 1.9160086145010768,
      "grad_norm": 0.08103767782449722,
      "learning_rate": 8.030097577836272e-05,
      "loss": 0.2862,
      "step": 26690
    },
    {
      "epoch": 1.9167264895908112,
      "grad_norm": 0.20643748342990875,
      "learning_rate": 8.02477998457898e-05,
      "loss": 0.0271,
      "step": 26700
    },
    {
      "epoch": 1.9174443646805455,
      "grad_norm": 11.896209716796875,
      "learning_rate": 8.019462391321688e-05,
      "loss": 0.0449,
      "step": 26710
    },
    {
      "epoch": 1.91816223977028,
      "grad_norm": 0.5601661801338196,
      "learning_rate": 8.014144798064397e-05,
      "loss": 0.3332,
      "step": 26720
    },
    {
      "epoch": 1.9188801148600143,
      "grad_norm": 0.011751912534236908,
      "learning_rate": 8.008827204807105e-05,
      "loss": 0.1488,
      "step": 26730
    },
    {
      "epoch": 1.9195979899497488,
      "grad_norm": 0.05198816955089569,
      "learning_rate": 8.003509611549813e-05,
      "loss": 0.0397,
      "step": 26740
    },
    {
      "epoch": 1.9203158650394831,
      "grad_norm": 0.042355798184871674,
      "learning_rate": 7.998192018292521e-05,
      "loss": 0.1228,
      "step": 26750
    },
    {
      "epoch": 1.9210337401292175,
      "grad_norm": 16.001020431518555,
      "learning_rate": 7.992874425035229e-05,
      "loss": 0.1589,
      "step": 26760
    },
    {
      "epoch": 1.9217516152189518,
      "grad_norm": 0.8312916159629822,
      "learning_rate": 7.987556831777938e-05,
      "loss": 0.1546,
      "step": 26770
    },
    {
      "epoch": 1.9224694903086863,
      "grad_norm": 0.38250425457954407,
      "learning_rate": 7.982239238520646e-05,
      "loss": 0.0952,
      "step": 26780
    },
    {
      "epoch": 1.9231873653984208,
      "grad_norm": 0.023107139393687248,
      "learning_rate": 7.976921645263354e-05,
      "loss": 0.1663,
      "step": 26790
    },
    {
      "epoch": 1.9239052404881551,
      "grad_norm": 15.957599639892578,
      "learning_rate": 7.971604052006062e-05,
      "loss": 0.239,
      "step": 26800
    },
    {
      "epoch": 1.9246231155778895,
      "grad_norm": 6.4045891761779785,
      "learning_rate": 7.96628645874877e-05,
      "loss": 0.2644,
      "step": 26810
    },
    {
      "epoch": 1.9253409906676238,
      "grad_norm": 14.616825103759766,
      "learning_rate": 7.96096886549148e-05,
      "loss": 0.3341,
      "step": 26820
    },
    {
      "epoch": 1.926058865757358,
      "grad_norm": 28.245281219482422,
      "learning_rate": 7.955651272234187e-05,
      "loss": 0.0858,
      "step": 26830
    },
    {
      "epoch": 1.9267767408470926,
      "grad_norm": 15.71523666381836,
      "learning_rate": 7.950333678976895e-05,
      "loss": 0.1486,
      "step": 26840
    },
    {
      "epoch": 1.9274946159368271,
      "grad_norm": 0.22971543669700623,
      "learning_rate": 7.945016085719603e-05,
      "loss": 0.0681,
      "step": 26850
    },
    {
      "epoch": 1.9282124910265614,
      "grad_norm": 16.553247451782227,
      "learning_rate": 7.939698492462313e-05,
      "loss": 0.0763,
      "step": 26860
    },
    {
      "epoch": 1.9289303661162958,
      "grad_norm": 16.96935272216797,
      "learning_rate": 7.93438089920502e-05,
      "loss": 0.1589,
      "step": 26870
    },
    {
      "epoch": 1.92964824120603,
      "grad_norm": 5.019385814666748,
      "learning_rate": 7.929063305947728e-05,
      "loss": 0.2033,
      "step": 26880
    },
    {
      "epoch": 1.9303661162957644,
      "grad_norm": 2.6415231227874756,
      "learning_rate": 7.923745712690436e-05,
      "loss": 0.3366,
      "step": 26890
    },
    {
      "epoch": 1.931083991385499,
      "grad_norm": 0.2608795464038849,
      "learning_rate": 7.918428119433144e-05,
      "loss": 0.0975,
      "step": 26900
    },
    {
      "epoch": 1.9318018664752334,
      "grad_norm": 1.55556321144104,
      "learning_rate": 7.913110526175854e-05,
      "loss": 0.0786,
      "step": 26910
    },
    {
      "epoch": 1.9325197415649678,
      "grad_norm": 0.02445361763238907,
      "learning_rate": 7.907792932918562e-05,
      "loss": 0.1344,
      "step": 26920
    },
    {
      "epoch": 1.933237616654702,
      "grad_norm": 0.026470784097909927,
      "learning_rate": 7.90247533966127e-05,
      "loss": 0.0019,
      "step": 26930
    },
    {
      "epoch": 1.9339554917444364,
      "grad_norm": 0.10580060631036758,
      "learning_rate": 7.897157746403977e-05,
      "loss": 0.1723,
      "step": 26940
    },
    {
      "epoch": 1.934673366834171,
      "grad_norm": 11.476329803466797,
      "learning_rate": 7.891840153146685e-05,
      "loss": 0.4938,
      "step": 26950
    },
    {
      "epoch": 1.9353912419239052,
      "grad_norm": 4.556092262268066,
      "learning_rate": 7.886522559889395e-05,
      "loss": 0.1094,
      "step": 26960
    },
    {
      "epoch": 1.9361091170136397,
      "grad_norm": 0.02041279897093773,
      "learning_rate": 7.881204966632103e-05,
      "loss": 0.1635,
      "step": 26970
    },
    {
      "epoch": 1.936826992103374,
      "grad_norm": 0.017272833734750748,
      "learning_rate": 7.87588737337481e-05,
      "loss": 0.2181,
      "step": 26980
    },
    {
      "epoch": 1.9375448671931084,
      "grad_norm": 1.9777556657791138,
      "learning_rate": 7.87056978011752e-05,
      "loss": 0.1431,
      "step": 26990
    },
    {
      "epoch": 1.9382627422828427,
      "grad_norm": 0.18457329273223877,
      "learning_rate": 7.865252186860227e-05,
      "loss": 0.1417,
      "step": 27000
    },
    {
      "epoch": 1.9389806173725772,
      "grad_norm": 0.10865574330091476,
      "learning_rate": 7.859934593602936e-05,
      "loss": 0.0814,
      "step": 27010
    },
    {
      "epoch": 1.9396984924623115,
      "grad_norm": 0.8213405609130859,
      "learning_rate": 7.854617000345644e-05,
      "loss": 0.1206,
      "step": 27020
    },
    {
      "epoch": 1.940416367552046,
      "grad_norm": 0.009363084100186825,
      "learning_rate": 7.849299407088352e-05,
      "loss": 0.0466,
      "step": 27030
    },
    {
      "epoch": 1.9411342426417804,
      "grad_norm": 1.4984384775161743,
      "learning_rate": 7.843981813831061e-05,
      "loss": 0.1797,
      "step": 27040
    },
    {
      "epoch": 1.9418521177315147,
      "grad_norm": 1.595829725265503,
      "learning_rate": 7.838664220573768e-05,
      "loss": 0.1154,
      "step": 27050
    },
    {
      "epoch": 1.942569992821249,
      "grad_norm": 8.997438430786133,
      "learning_rate": 7.833346627316477e-05,
      "loss": 0.1387,
      "step": 27060
    },
    {
      "epoch": 1.9432878679109835,
      "grad_norm": 0.07990647852420807,
      "learning_rate": 7.828029034059185e-05,
      "loss": 0.0774,
      "step": 27070
    },
    {
      "epoch": 1.9440057430007178,
      "grad_norm": 0.11585187911987305,
      "learning_rate": 7.822711440801893e-05,
      "loss": 0.016,
      "step": 27080
    },
    {
      "epoch": 1.9447236180904524,
      "grad_norm": 0.01642695814371109,
      "learning_rate": 7.817393847544602e-05,
      "loss": 0.1495,
      "step": 27090
    },
    {
      "epoch": 1.9454414931801867,
      "grad_norm": 5.790060043334961,
      "learning_rate": 7.81207625428731e-05,
      "loss": 0.0969,
      "step": 27100
    },
    {
      "epoch": 1.946159368269921,
      "grad_norm": 0.15014120936393738,
      "learning_rate": 7.806758661030018e-05,
      "loss": 0.0939,
      "step": 27110
    },
    {
      "epoch": 1.9468772433596553,
      "grad_norm": 16.737348556518555,
      "learning_rate": 7.801441067772727e-05,
      "loss": 0.2285,
      "step": 27120
    },
    {
      "epoch": 1.9475951184493898,
      "grad_norm": 9.055830001831055,
      "learning_rate": 7.796123474515434e-05,
      "loss": 0.0383,
      "step": 27130
    },
    {
      "epoch": 1.9483129935391243,
      "grad_norm": 0.0233205147087574,
      "learning_rate": 7.790805881258143e-05,
      "loss": 0.0757,
      "step": 27140
    },
    {
      "epoch": 1.9490308686288587,
      "grad_norm": 0.5550373792648315,
      "learning_rate": 7.785488288000851e-05,
      "loss": 0.1687,
      "step": 27150
    },
    {
      "epoch": 1.949748743718593,
      "grad_norm": 0.004416369833052158,
      "learning_rate": 7.780170694743559e-05,
      "loss": 0.0264,
      "step": 27160
    },
    {
      "epoch": 1.9504666188083273,
      "grad_norm": 3.424774169921875,
      "learning_rate": 7.774853101486268e-05,
      "loss": 0.1926,
      "step": 27170
    },
    {
      "epoch": 1.9511844938980616,
      "grad_norm": 17.276233673095703,
      "learning_rate": 7.769535508228975e-05,
      "loss": 0.5898,
      "step": 27180
    },
    {
      "epoch": 1.9519023689877961,
      "grad_norm": 0.017728405073285103,
      "learning_rate": 7.764217914971684e-05,
      "loss": 0.0645,
      "step": 27190
    },
    {
      "epoch": 1.9526202440775307,
      "grad_norm": 0.8376131653785706,
      "learning_rate": 7.758900321714392e-05,
      "loss": 0.0419,
      "step": 27200
    },
    {
      "epoch": 1.953338119167265,
      "grad_norm": 0.008871505968272686,
      "learning_rate": 7.7535827284571e-05,
      "loss": 0.1077,
      "step": 27210
    },
    {
      "epoch": 1.9540559942569993,
      "grad_norm": 2.5893819332122803,
      "learning_rate": 7.74826513519981e-05,
      "loss": 0.0528,
      "step": 27220
    },
    {
      "epoch": 1.9547738693467336,
      "grad_norm": 0.008654870092868805,
      "learning_rate": 7.742947541942516e-05,
      "loss": 0.1137,
      "step": 27230
    },
    {
      "epoch": 1.955491744436468,
      "grad_norm": 0.6567927002906799,
      "learning_rate": 7.737629948685225e-05,
      "loss": 0.0963,
      "step": 27240
    },
    {
      "epoch": 1.9562096195262024,
      "grad_norm": 0.12277881801128387,
      "learning_rate": 7.732312355427935e-05,
      "loss": 0.0846,
      "step": 27250
    },
    {
      "epoch": 1.956927494615937,
      "grad_norm": 23.498722076416016,
      "learning_rate": 7.726994762170641e-05,
      "loss": 0.0387,
      "step": 27260
    },
    {
      "epoch": 1.9576453697056713,
      "grad_norm": 0.2848052978515625,
      "learning_rate": 7.721677168913351e-05,
      "loss": 0.2002,
      "step": 27270
    },
    {
      "epoch": 1.9583632447954056,
      "grad_norm": 1.088677167892456,
      "learning_rate": 7.716359575656059e-05,
      "loss": 0.0316,
      "step": 27280
    },
    {
      "epoch": 1.9590811198851399,
      "grad_norm": 6.791596412658691,
      "learning_rate": 7.711041982398767e-05,
      "loss": 0.154,
      "step": 27290
    },
    {
      "epoch": 1.9597989949748744,
      "grad_norm": 11.503074645996094,
      "learning_rate": 7.705724389141476e-05,
      "loss": 0.1656,
      "step": 27300
    },
    {
      "epoch": 1.9605168700646087,
      "grad_norm": 9.38873291015625,
      "learning_rate": 7.700406795884182e-05,
      "loss": 0.2268,
      "step": 27310
    },
    {
      "epoch": 1.9612347451543433,
      "grad_norm": 0.26956257224082947,
      "learning_rate": 7.695089202626892e-05,
      "loss": 0.133,
      "step": 27320
    },
    {
      "epoch": 1.9619526202440776,
      "grad_norm": 29.225799560546875,
      "learning_rate": 7.6897716093696e-05,
      "loss": 0.1759,
      "step": 27330
    },
    {
      "epoch": 1.9626704953338119,
      "grad_norm": 0.0016623404808342457,
      "learning_rate": 7.684454016112308e-05,
      "loss": 0.1639,
      "step": 27340
    },
    {
      "epoch": 1.9633883704235462,
      "grad_norm": 1.0317918062210083,
      "learning_rate": 7.679136422855017e-05,
      "loss": 0.1831,
      "step": 27350
    },
    {
      "epoch": 1.9641062455132807,
      "grad_norm": 1.6960378885269165,
      "learning_rate": 7.673818829597724e-05,
      "loss": 0.1739,
      "step": 27360
    },
    {
      "epoch": 1.964824120603015,
      "grad_norm": 11.010982513427734,
      "learning_rate": 7.668501236340433e-05,
      "loss": 0.2642,
      "step": 27370
    },
    {
      "epoch": 1.9655419956927496,
      "grad_norm": 18.566511154174805,
      "learning_rate": 7.663183643083141e-05,
      "loss": 0.3403,
      "step": 27380
    },
    {
      "epoch": 1.9662598707824839,
      "grad_norm": 11.373825073242188,
      "learning_rate": 7.657866049825849e-05,
      "loss": 0.0748,
      "step": 27390
    },
    {
      "epoch": 1.9669777458722182,
      "grad_norm": 0.1738942712545395,
      "learning_rate": 7.652548456568558e-05,
      "loss": 0.1672,
      "step": 27400
    },
    {
      "epoch": 1.9676956209619525,
      "grad_norm": 8.408265113830566,
      "learning_rate": 7.647230863311266e-05,
      "loss": 0.3539,
      "step": 27410
    },
    {
      "epoch": 1.968413496051687,
      "grad_norm": 0.42487767338752747,
      "learning_rate": 7.641913270053974e-05,
      "loss": 0.0734,
      "step": 27420
    },
    {
      "epoch": 1.9691313711414213,
      "grad_norm": 0.005841698497533798,
      "learning_rate": 7.636595676796682e-05,
      "loss": 0.1117,
      "step": 27430
    },
    {
      "epoch": 1.9698492462311559,
      "grad_norm": 0.1985766887664795,
      "learning_rate": 7.63127808353939e-05,
      "loss": 0.1353,
      "step": 27440
    },
    {
      "epoch": 1.9705671213208902,
      "grad_norm": 0.0006309100426733494,
      "learning_rate": 7.625960490282099e-05,
      "loss": 0.1425,
      "step": 27450
    },
    {
      "epoch": 1.9712849964106245,
      "grad_norm": 32.10719299316406,
      "learning_rate": 7.620642897024807e-05,
      "loss": 0.1531,
      "step": 27460
    },
    {
      "epoch": 1.9720028715003588,
      "grad_norm": 3.18304181098938,
      "learning_rate": 7.615325303767515e-05,
      "loss": 0.3153,
      "step": 27470
    },
    {
      "epoch": 1.9727207465900933,
      "grad_norm": 0.018535779789090157,
      "learning_rate": 7.610007710510223e-05,
      "loss": 0.1144,
      "step": 27480
    },
    {
      "epoch": 1.9734386216798279,
      "grad_norm": 1.060197353363037,
      "learning_rate": 7.604690117252932e-05,
      "loss": 0.208,
      "step": 27490
    },
    {
      "epoch": 1.9741564967695622,
      "grad_norm": 0.06806528568267822,
      "learning_rate": 7.59937252399564e-05,
      "loss": 0.0385,
      "step": 27500
    },
    {
      "epoch": 1.9748743718592965,
      "grad_norm": 0.009756281971931458,
      "learning_rate": 7.594054930738348e-05,
      "loss": 0.0541,
      "step": 27510
    },
    {
      "epoch": 1.9755922469490308,
      "grad_norm": 0.22324799001216888,
      "learning_rate": 7.588737337481056e-05,
      "loss": 0.024,
      "step": 27520
    },
    {
      "epoch": 1.976310122038765,
      "grad_norm": 1.864173173904419,
      "learning_rate": 7.583419744223764e-05,
      "loss": 0.2616,
      "step": 27530
    },
    {
      "epoch": 1.9770279971284996,
      "grad_norm": 3.793177843093872,
      "learning_rate": 7.578102150966473e-05,
      "loss": 0.1707,
      "step": 27540
    },
    {
      "epoch": 1.9777458722182342,
      "grad_norm": 0.014879096299409866,
      "learning_rate": 7.572784557709181e-05,
      "loss": 0.1317,
      "step": 27550
    },
    {
      "epoch": 1.9784637473079685,
      "grad_norm": 14.290844917297363,
      "learning_rate": 7.56746696445189e-05,
      "loss": 0.268,
      "step": 27560
    },
    {
      "epoch": 1.9791816223977028,
      "grad_norm": 0.004390525631606579,
      "learning_rate": 7.562149371194597e-05,
      "loss": 0.3003,
      "step": 27570
    },
    {
      "epoch": 1.979899497487437,
      "grad_norm": 0.5976142883300781,
      "learning_rate": 7.556831777937305e-05,
      "loss": 0.0094,
      "step": 27580
    },
    {
      "epoch": 1.9806173725771714,
      "grad_norm": 25.259140014648438,
      "learning_rate": 7.551514184680015e-05,
      "loss": 0.0386,
      "step": 27590
    },
    {
      "epoch": 1.981335247666906,
      "grad_norm": 23.698293685913086,
      "learning_rate": 7.546196591422723e-05,
      "loss": 0.1875,
      "step": 27600
    },
    {
      "epoch": 1.9820531227566405,
      "grad_norm": 0.009133530780673027,
      "learning_rate": 7.54087899816543e-05,
      "loss": 0.1427,
      "step": 27610
    },
    {
      "epoch": 1.9827709978463748,
      "grad_norm": 0.002245426643639803,
      "learning_rate": 7.53556140490814e-05,
      "loss": 0.0036,
      "step": 27620
    },
    {
      "epoch": 1.983488872936109,
      "grad_norm": 0.006705775391310453,
      "learning_rate": 7.530243811650846e-05,
      "loss": 0.0054,
      "step": 27630
    },
    {
      "epoch": 1.9842067480258434,
      "grad_norm": 0.012954487465322018,
      "learning_rate": 7.524926218393556e-05,
      "loss": 0.1917,
      "step": 27640
    },
    {
      "epoch": 1.984924623115578,
      "grad_norm": 5.569690227508545,
      "learning_rate": 7.519608625136264e-05,
      "loss": 0.0305,
      "step": 27650
    },
    {
      "epoch": 1.9856424982053122,
      "grad_norm": 6.54760217666626,
      "learning_rate": 7.514291031878972e-05,
      "loss": 0.2336,
      "step": 27660
    },
    {
      "epoch": 1.9863603732950468,
      "grad_norm": 17.385398864746094,
      "learning_rate": 7.508973438621681e-05,
      "loss": 0.2368,
      "step": 27670
    },
    {
      "epoch": 1.987078248384781,
      "grad_norm": 8.727506637573242,
      "learning_rate": 7.503655845364387e-05,
      "loss": 0.0645,
      "step": 27680
    },
    {
      "epoch": 1.9877961234745154,
      "grad_norm": 22.490379333496094,
      "learning_rate": 7.498338252107097e-05,
      "loss": 0.3052,
      "step": 27690
    },
    {
      "epoch": 1.9885139985642497,
      "grad_norm": 0.004825690761208534,
      "learning_rate": 7.493020658849805e-05,
      "loss": 0.1008,
      "step": 27700
    },
    {
      "epoch": 1.9892318736539842,
      "grad_norm": 6.578916072845459,
      "learning_rate": 7.487703065592513e-05,
      "loss": 0.2305,
      "step": 27710
    },
    {
      "epoch": 1.9899497487437185,
      "grad_norm": 0.06343302875757217,
      "learning_rate": 7.482385472335222e-05,
      "loss": 0.3605,
      "step": 27720
    },
    {
      "epoch": 1.990667623833453,
      "grad_norm": 0.10196991264820099,
      "learning_rate": 7.477067879077929e-05,
      "loss": 0.1204,
      "step": 27730
    },
    {
      "epoch": 1.9913854989231874,
      "grad_norm": 0.004511257167905569,
      "learning_rate": 7.471750285820638e-05,
      "loss": 0.0564,
      "step": 27740
    },
    {
      "epoch": 1.9921033740129217,
      "grad_norm": 23.29157257080078,
      "learning_rate": 7.466432692563347e-05,
      "loss": 0.0399,
      "step": 27750
    },
    {
      "epoch": 1.992821249102656,
      "grad_norm": 25.304407119750977,
      "learning_rate": 7.461115099306054e-05,
      "loss": 0.1736,
      "step": 27760
    },
    {
      "epoch": 1.9935391241923905,
      "grad_norm": 0.04880409687757492,
      "learning_rate": 7.455797506048763e-05,
      "loss": 0.0051,
      "step": 27770
    },
    {
      "epoch": 1.994256999282125,
      "grad_norm": 0.1916847825050354,
      "learning_rate": 7.450479912791471e-05,
      "loss": 0.2128,
      "step": 27780
    },
    {
      "epoch": 1.9949748743718594,
      "grad_norm": 12.128067970275879,
      "learning_rate": 7.445162319534179e-05,
      "loss": 0.0409,
      "step": 27790
    },
    {
      "epoch": 1.9956927494615937,
      "grad_norm": 4.943843841552734,
      "learning_rate": 7.439844726276888e-05,
      "loss": 0.1731,
      "step": 27800
    },
    {
      "epoch": 1.996410624551328,
      "grad_norm": 19.425195693969727,
      "learning_rate": 7.434527133019595e-05,
      "loss": 0.2081,
      "step": 27810
    },
    {
      "epoch": 1.9971284996410623,
      "grad_norm": 0.08354882895946503,
      "learning_rate": 7.429209539762304e-05,
      "loss": 0.2183,
      "step": 27820
    },
    {
      "epoch": 1.9978463747307968,
      "grad_norm": 0.0021448791958391666,
      "learning_rate": 7.423891946505012e-05,
      "loss": 0.1372,
      "step": 27830
    },
    {
      "epoch": 1.9985642498205314,
      "grad_norm": 0.25405293703079224,
      "learning_rate": 7.41857435324772e-05,
      "loss": 0.2314,
      "step": 27840
    },
    {
      "epoch": 1.9992821249102657,
      "grad_norm": 0.12090986967086792,
      "learning_rate": 7.41325675999043e-05,
      "loss": 0.0066,
      "step": 27850
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.16896048188209534,
      "learning_rate": 7.407939166733136e-05,
      "loss": 0.1608,
      "step": 27860
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9646551363122409,
      "eval_f1_class_0_Oxidored": 0.9701975666927113,
      "eval_f1_class_1_Transfer": 0.9599200621738647,
      "eval_f1_class_2_Hydrolas": 0.9580296001767175,
      "eval_f1_class_3_Lyases": 0.952,
      "eval_f1_class_4_Isomeras": 0.9404883011190234,
      "eval_f1_class_5_Ligases": 0.9878542510121457,
      "eval_f1_class_6_Transloc": 0.9805105682130113,
      "eval_f1_macro": 0.9641429070553534,
      "eval_f1_micro": 0.9646551363122409,
      "eval_loss": 0.15089869499206543,
      "eval_precision_macro": 0.9633656941384524,
      "eval_precision_micro": 0.9646551363122409,
      "eval_recall_macro": 0.965047209880197,
      "eval_recall_micro": 0.9646551363122409,
      "eval_runtime": 201.8309,
      "eval_samples_per_second": 118.312,
      "eval_steps_per_second": 14.79,
      "step": 27860
    }
  ],
  "logging_steps": 10,
  "max_steps": 41790,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.543071643869192e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
