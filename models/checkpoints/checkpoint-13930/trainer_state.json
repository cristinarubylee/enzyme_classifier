{
  "best_global_step": 13930,
  "best_metric": 0.9414076300657586,
  "best_model_checkpoint": "..\\models\\checkpoints\\checkpoint-13930",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 13930,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 7.178750897343863e-05,
      "grad_norm": 2.2443807125091553,
      "learning_rate": 0.0,
      "loss": 1.9568,
      "step": 1
    },
    {
      "epoch": 0.0007178750897343862,
      "grad_norm": 2.565307855606079,
      "learning_rate": 4.3072505384063175e-07,
      "loss": 1.9357,
      "step": 10
    },
    {
      "epoch": 0.0014357501794687725,
      "grad_norm": 1.2856775522232056,
      "learning_rate": 9.093084469968892e-07,
      "loss": 1.9221,
      "step": 20
    },
    {
      "epoch": 0.0021536252692031586,
      "grad_norm": 1.0540913343429565,
      "learning_rate": 1.3878918401531467e-06,
      "loss": 1.9348,
      "step": 30
    },
    {
      "epoch": 0.002871500358937545,
      "grad_norm": 2.216089963912964,
      "learning_rate": 1.8664752333094043e-06,
      "loss": 1.9577,
      "step": 40
    },
    {
      "epoch": 0.003589375448671931,
      "grad_norm": 1.276774525642395,
      "learning_rate": 2.3450586264656617e-06,
      "loss": 1.9291,
      "step": 50
    },
    {
      "epoch": 0.004307250538406317,
      "grad_norm": 1.6183334589004517,
      "learning_rate": 2.823642019621919e-06,
      "loss": 1.9286,
      "step": 60
    },
    {
      "epoch": 0.005025125628140704,
      "grad_norm": 2.501128911972046,
      "learning_rate": 3.3022254127781766e-06,
      "loss": 1.9422,
      "step": 70
    },
    {
      "epoch": 0.00574300071787509,
      "grad_norm": 0.9974015355110168,
      "learning_rate": 3.780808805934434e-06,
      "loss": 1.9344,
      "step": 80
    },
    {
      "epoch": 0.006460875807609476,
      "grad_norm": 1.145450234413147,
      "learning_rate": 4.259392199090692e-06,
      "loss": 1.9481,
      "step": 90
    },
    {
      "epoch": 0.007178750897343862,
      "grad_norm": 1.1056952476501465,
      "learning_rate": 4.73797559224695e-06,
      "loss": 1.9415,
      "step": 100
    },
    {
      "epoch": 0.007896625987078248,
      "grad_norm": 1.0103105306625366,
      "learning_rate": 5.216558985403207e-06,
      "loss": 1.9318,
      "step": 110
    },
    {
      "epoch": 0.008614501076812634,
      "grad_norm": 2.1496083736419678,
      "learning_rate": 5.6951423785594645e-06,
      "loss": 1.9239,
      "step": 120
    },
    {
      "epoch": 0.00933237616654702,
      "grad_norm": 1.3574650287628174,
      "learning_rate": 6.173725771715722e-06,
      "loss": 1.9339,
      "step": 130
    },
    {
      "epoch": 0.010050251256281407,
      "grad_norm": 1.2681936025619507,
      "learning_rate": 6.652309164871979e-06,
      "loss": 1.9339,
      "step": 140
    },
    {
      "epoch": 0.010768126346015794,
      "grad_norm": 1.3008111715316772,
      "learning_rate": 7.130892558028236e-06,
      "loss": 1.891,
      "step": 150
    },
    {
      "epoch": 0.01148600143575018,
      "grad_norm": 1.5573484897613525,
      "learning_rate": 7.609475951184494e-06,
      "loss": 1.9174,
      "step": 160
    },
    {
      "epoch": 0.012203876525484566,
      "grad_norm": 1.5936486721038818,
      "learning_rate": 8.088059344340752e-06,
      "loss": 1.9233,
      "step": 170
    },
    {
      "epoch": 0.012921751615218953,
      "grad_norm": 1.6593544483184814,
      "learning_rate": 8.566642737497008e-06,
      "loss": 1.9271,
      "step": 180
    },
    {
      "epoch": 0.013639626704953339,
      "grad_norm": 1.5412921905517578,
      "learning_rate": 9.045226130653267e-06,
      "loss": 1.9111,
      "step": 190
    },
    {
      "epoch": 0.014357501794687724,
      "grad_norm": 1.3005762100219727,
      "learning_rate": 9.523809523809523e-06,
      "loss": 1.905,
      "step": 200
    },
    {
      "epoch": 0.01507537688442211,
      "grad_norm": 2.003216028213501,
      "learning_rate": 1.0002392916965782e-05,
      "loss": 1.9019,
      "step": 210
    },
    {
      "epoch": 0.015793251974156496,
      "grad_norm": 1.508143424987793,
      "learning_rate": 1.048097631012204e-05,
      "loss": 1.9105,
      "step": 220
    },
    {
      "epoch": 0.016511127063890883,
      "grad_norm": 1.9425115585327148,
      "learning_rate": 1.0959559703278297e-05,
      "loss": 1.9011,
      "step": 230
    },
    {
      "epoch": 0.01722900215362527,
      "grad_norm": 0.7284009456634521,
      "learning_rate": 1.1438143096434554e-05,
      "loss": 1.9182,
      "step": 240
    },
    {
      "epoch": 0.017946877243359655,
      "grad_norm": 1.8167644739151,
      "learning_rate": 1.1916726489590812e-05,
      "loss": 1.9072,
      "step": 250
    },
    {
      "epoch": 0.01866475233309404,
      "grad_norm": 1.2865263223648071,
      "learning_rate": 1.239530988274707e-05,
      "loss": 1.9043,
      "step": 260
    },
    {
      "epoch": 0.019382627422828428,
      "grad_norm": 1.484291434288025,
      "learning_rate": 1.2873893275903327e-05,
      "loss": 1.9121,
      "step": 270
    },
    {
      "epoch": 0.020100502512562814,
      "grad_norm": 2.0690457820892334,
      "learning_rate": 1.3352476669059586e-05,
      "loss": 1.9012,
      "step": 280
    },
    {
      "epoch": 0.0208183776022972,
      "grad_norm": 1.9599432945251465,
      "learning_rate": 1.3831060062215842e-05,
      "loss": 1.8854,
      "step": 290
    },
    {
      "epoch": 0.021536252692031587,
      "grad_norm": 1.151863694190979,
      "learning_rate": 1.4309643455372099e-05,
      "loss": 1.8832,
      "step": 300
    },
    {
      "epoch": 0.022254127781765973,
      "grad_norm": 2.4154529571533203,
      "learning_rate": 1.4788226848528358e-05,
      "loss": 1.8812,
      "step": 310
    },
    {
      "epoch": 0.02297200287150036,
      "grad_norm": 1.7338829040527344,
      "learning_rate": 1.5266810241684614e-05,
      "loss": 1.9107,
      "step": 320
    },
    {
      "epoch": 0.023689877961234746,
      "grad_norm": 1.2140504121780396,
      "learning_rate": 1.574539363484087e-05,
      "loss": 1.9016,
      "step": 330
    },
    {
      "epoch": 0.024407753050969132,
      "grad_norm": 1.81125807762146,
      "learning_rate": 1.622397702799713e-05,
      "loss": 1.9119,
      "step": 340
    },
    {
      "epoch": 0.02512562814070352,
      "grad_norm": 1.7674694061279297,
      "learning_rate": 1.6702560421153386e-05,
      "loss": 1.905,
      "step": 350
    },
    {
      "epoch": 0.025843503230437905,
      "grad_norm": 1.6668033599853516,
      "learning_rate": 1.7181143814309644e-05,
      "loss": 1.8963,
      "step": 360
    },
    {
      "epoch": 0.02656137832017229,
      "grad_norm": 1.1810059547424316,
      "learning_rate": 1.76597272074659e-05,
      "loss": 1.8769,
      "step": 370
    },
    {
      "epoch": 0.027279253409906678,
      "grad_norm": 1.7451016902923584,
      "learning_rate": 1.8138310600622162e-05,
      "loss": 1.9161,
      "step": 380
    },
    {
      "epoch": 0.02799712849964106,
      "grad_norm": 1.1852329969406128,
      "learning_rate": 1.8616893993778416e-05,
      "loss": 1.8557,
      "step": 390
    },
    {
      "epoch": 0.028715003589375447,
      "grad_norm": 2.6491968631744385,
      "learning_rate": 1.9095477386934673e-05,
      "loss": 1.9002,
      "step": 400
    },
    {
      "epoch": 0.029432878679109833,
      "grad_norm": 1.6303825378417969,
      "learning_rate": 1.9574060780090934e-05,
      "loss": 1.8836,
      "step": 410
    },
    {
      "epoch": 0.03015075376884422,
      "grad_norm": 0.9596300721168518,
      "learning_rate": 2.005264417324719e-05,
      "loss": 1.8815,
      "step": 420
    },
    {
      "epoch": 0.030868628858578606,
      "grad_norm": 2.0150625705718994,
      "learning_rate": 2.0531227566403445e-05,
      "loss": 1.8421,
      "step": 430
    },
    {
      "epoch": 0.03158650394831299,
      "grad_norm": 1.2371537685394287,
      "learning_rate": 2.1009810959559703e-05,
      "loss": 1.8913,
      "step": 440
    },
    {
      "epoch": 0.03230437903804738,
      "grad_norm": 1.2437065839767456,
      "learning_rate": 2.1488394352715964e-05,
      "loss": 1.9136,
      "step": 450
    },
    {
      "epoch": 0.033022254127781765,
      "grad_norm": 2.2579267024993896,
      "learning_rate": 2.1966977745872218e-05,
      "loss": 1.9229,
      "step": 460
    },
    {
      "epoch": 0.03374012921751615,
      "grad_norm": 1.4224483966827393,
      "learning_rate": 2.2445561139028475e-05,
      "loss": 1.8696,
      "step": 470
    },
    {
      "epoch": 0.03445800430725054,
      "grad_norm": 1.451357364654541,
      "learning_rate": 2.2924144532184736e-05,
      "loss": 1.8942,
      "step": 480
    },
    {
      "epoch": 0.035175879396984924,
      "grad_norm": 0.9637187123298645,
      "learning_rate": 2.3402727925340993e-05,
      "loss": 1.8321,
      "step": 490
    },
    {
      "epoch": 0.03589375448671931,
      "grad_norm": 2.0366294384002686,
      "learning_rate": 2.3881311318497247e-05,
      "loss": 1.8452,
      "step": 500
    },
    {
      "epoch": 0.0366116295764537,
      "grad_norm": 1.0826271772384644,
      "learning_rate": 2.4359894711653508e-05,
      "loss": 1.8962,
      "step": 510
    },
    {
      "epoch": 0.03732950466618808,
      "grad_norm": 2.024017333984375,
      "learning_rate": 2.4838478104809766e-05,
      "loss": 1.873,
      "step": 520
    },
    {
      "epoch": 0.03804737975592247,
      "grad_norm": 2.615903854370117,
      "learning_rate": 2.5317061497966023e-05,
      "loss": 1.9043,
      "step": 530
    },
    {
      "epoch": 0.038765254845656856,
      "grad_norm": 1.0314217805862427,
      "learning_rate": 2.5795644891122277e-05,
      "loss": 1.8677,
      "step": 540
    },
    {
      "epoch": 0.03948312993539124,
      "grad_norm": 2.5536932945251465,
      "learning_rate": 2.6274228284278535e-05,
      "loss": 1.8159,
      "step": 550
    },
    {
      "epoch": 0.04020100502512563,
      "grad_norm": 1.8632893562316895,
      "learning_rate": 2.6752811677434795e-05,
      "loss": 1.9295,
      "step": 560
    },
    {
      "epoch": 0.040918880114860015,
      "grad_norm": 1.3956190347671509,
      "learning_rate": 2.7231395070591053e-05,
      "loss": 1.8864,
      "step": 570
    },
    {
      "epoch": 0.0416367552045944,
      "grad_norm": 2.197035074234009,
      "learning_rate": 2.770997846374731e-05,
      "loss": 1.8535,
      "step": 580
    },
    {
      "epoch": 0.04235463029432879,
      "grad_norm": 1.1177005767822266,
      "learning_rate": 2.8188561856903568e-05,
      "loss": 1.8338,
      "step": 590
    },
    {
      "epoch": 0.043072505384063174,
      "grad_norm": 1.2030924558639526,
      "learning_rate": 2.8667145250059825e-05,
      "loss": 1.8391,
      "step": 600
    },
    {
      "epoch": 0.04379038047379756,
      "grad_norm": 2.2328665256500244,
      "learning_rate": 2.914572864321608e-05,
      "loss": 1.8409,
      "step": 610
    },
    {
      "epoch": 0.04450825556353195,
      "grad_norm": 1.3226879835128784,
      "learning_rate": 2.9624312036372337e-05,
      "loss": 1.8205,
      "step": 620
    },
    {
      "epoch": 0.04522613065326633,
      "grad_norm": 1.7998862266540527,
      "learning_rate": 3.01028954295286e-05,
      "loss": 1.8197,
      "step": 630
    },
    {
      "epoch": 0.04594400574300072,
      "grad_norm": 2.222020387649536,
      "learning_rate": 3.0581478822684855e-05,
      "loss": 1.9478,
      "step": 640
    },
    {
      "epoch": 0.046661880832735106,
      "grad_norm": 1.8798229694366455,
      "learning_rate": 3.106006221584111e-05,
      "loss": 1.8183,
      "step": 650
    },
    {
      "epoch": 0.04737975592246949,
      "grad_norm": 1.698262095451355,
      "learning_rate": 3.153864560899737e-05,
      "loss": 1.8574,
      "step": 660
    },
    {
      "epoch": 0.04809763101220388,
      "grad_norm": 1.603096604347229,
      "learning_rate": 3.201722900215363e-05,
      "loss": 1.881,
      "step": 670
    },
    {
      "epoch": 0.048815506101938265,
      "grad_norm": 1.2774364948272705,
      "learning_rate": 3.2495812395309884e-05,
      "loss": 1.8413,
      "step": 680
    },
    {
      "epoch": 0.04953338119167265,
      "grad_norm": 1.5902067422866821,
      "learning_rate": 3.297439578846614e-05,
      "loss": 1.8358,
      "step": 690
    },
    {
      "epoch": 0.05025125628140704,
      "grad_norm": 1.68280029296875,
      "learning_rate": 3.34529791816224e-05,
      "loss": 1.7922,
      "step": 700
    },
    {
      "epoch": 0.050969131371141424,
      "grad_norm": 1.599617600440979,
      "learning_rate": 3.393156257477866e-05,
      "loss": 1.8275,
      "step": 710
    },
    {
      "epoch": 0.05168700646087581,
      "grad_norm": 1.6982134580612183,
      "learning_rate": 3.4410145967934914e-05,
      "loss": 1.8144,
      "step": 720
    },
    {
      "epoch": 0.0524048815506102,
      "grad_norm": 1.9980179071426392,
      "learning_rate": 3.488872936109117e-05,
      "loss": 1.7624,
      "step": 730
    },
    {
      "epoch": 0.05312275664034458,
      "grad_norm": 1.4489531517028809,
      "learning_rate": 3.536731275424743e-05,
      "loss": 1.8356,
      "step": 740
    },
    {
      "epoch": 0.05384063173007897,
      "grad_norm": 1.7002869844436646,
      "learning_rate": 3.5845896147403686e-05,
      "loss": 1.8743,
      "step": 750
    },
    {
      "epoch": 0.054558506819813356,
      "grad_norm": 2.1740975379943848,
      "learning_rate": 3.6324479540559944e-05,
      "loss": 1.7607,
      "step": 760
    },
    {
      "epoch": 0.05527638190954774,
      "grad_norm": 1.541532278060913,
      "learning_rate": 3.68030629337162e-05,
      "loss": 1.7592,
      "step": 770
    },
    {
      "epoch": 0.05599425699928212,
      "grad_norm": 1.8421859741210938,
      "learning_rate": 3.728164632687246e-05,
      "loss": 1.7964,
      "step": 780
    },
    {
      "epoch": 0.05671213208901651,
      "grad_norm": 1.821946382522583,
      "learning_rate": 3.7760229720028716e-05,
      "loss": 1.7281,
      "step": 790
    },
    {
      "epoch": 0.057430007178750894,
      "grad_norm": 2.726100206375122,
      "learning_rate": 3.8238813113184974e-05,
      "loss": 1.6456,
      "step": 800
    },
    {
      "epoch": 0.05814788226848528,
      "grad_norm": 2.074702262878418,
      "learning_rate": 3.871739650634123e-05,
      "loss": 1.701,
      "step": 810
    },
    {
      "epoch": 0.05886575735821967,
      "grad_norm": 1.9369333982467651,
      "learning_rate": 3.919597989949749e-05,
      "loss": 1.7419,
      "step": 820
    },
    {
      "epoch": 0.05958363244795405,
      "grad_norm": 1.8708157539367676,
      "learning_rate": 3.9674563292653746e-05,
      "loss": 1.7963,
      "step": 830
    },
    {
      "epoch": 0.06030150753768844,
      "grad_norm": 2.3492534160614014,
      "learning_rate": 4.015314668581e-05,
      "loss": 1.7238,
      "step": 840
    },
    {
      "epoch": 0.061019382627422826,
      "grad_norm": 1.5069295167922974,
      "learning_rate": 4.063173007896626e-05,
      "loss": 1.7361,
      "step": 850
    },
    {
      "epoch": 0.06173725771715721,
      "grad_norm": 2.4299814701080322,
      "learning_rate": 4.111031347212252e-05,
      "loss": 1.6967,
      "step": 860
    },
    {
      "epoch": 0.0624551328068916,
      "grad_norm": 2.202486276626587,
      "learning_rate": 4.1588896865278775e-05,
      "loss": 1.7011,
      "step": 870
    },
    {
      "epoch": 0.06317300789662599,
      "grad_norm": 2.4245569705963135,
      "learning_rate": 4.206748025843503e-05,
      "loss": 1.694,
      "step": 880
    },
    {
      "epoch": 0.06389088298636038,
      "grad_norm": 2.711670160293579,
      "learning_rate": 4.254606365159129e-05,
      "loss": 1.6484,
      "step": 890
    },
    {
      "epoch": 0.06460875807609476,
      "grad_norm": 2.273820161819458,
      "learning_rate": 4.3024647044747555e-05,
      "loss": 1.6244,
      "step": 900
    },
    {
      "epoch": 0.06532663316582915,
      "grad_norm": 3.159074544906616,
      "learning_rate": 4.3503230437903805e-05,
      "loss": 1.6236,
      "step": 910
    },
    {
      "epoch": 0.06604450825556353,
      "grad_norm": 2.4758360385894775,
      "learning_rate": 4.398181383106006e-05,
      "loss": 1.6669,
      "step": 920
    },
    {
      "epoch": 0.06676238334529792,
      "grad_norm": 2.3365976810455322,
      "learning_rate": 4.446039722421632e-05,
      "loss": 1.5381,
      "step": 930
    },
    {
      "epoch": 0.0674802584350323,
      "grad_norm": 2.6437838077545166,
      "learning_rate": 4.493898061737258e-05,
      "loss": 1.5091,
      "step": 940
    },
    {
      "epoch": 0.0681981335247667,
      "grad_norm": 2.309617757797241,
      "learning_rate": 4.5417564010528835e-05,
      "loss": 1.5474,
      "step": 950
    },
    {
      "epoch": 0.06891600861450108,
      "grad_norm": 2.955366611480713,
      "learning_rate": 4.58961474036851e-05,
      "loss": 1.5733,
      "step": 960
    },
    {
      "epoch": 0.06963388370423547,
      "grad_norm": 2.0069541931152344,
      "learning_rate": 4.6374730796841356e-05,
      "loss": 1.4538,
      "step": 970
    },
    {
      "epoch": 0.07035175879396985,
      "grad_norm": 2.2787604331970215,
      "learning_rate": 4.685331418999761e-05,
      "loss": 1.6158,
      "step": 980
    },
    {
      "epoch": 0.07106963388370424,
      "grad_norm": 3.990144729614258,
      "learning_rate": 4.7331897583153865e-05,
      "loss": 1.4793,
      "step": 990
    },
    {
      "epoch": 0.07178750897343862,
      "grad_norm": 3.4244203567504883,
      "learning_rate": 4.781048097631012e-05,
      "loss": 1.6441,
      "step": 1000
    },
    {
      "epoch": 0.07250538406317301,
      "grad_norm": 5.179141044616699,
      "learning_rate": 4.828906436946638e-05,
      "loss": 1.4999,
      "step": 1010
    },
    {
      "epoch": 0.0732232591529074,
      "grad_norm": 3.4903416633605957,
      "learning_rate": 4.876764776262264e-05,
      "loss": 1.505,
      "step": 1020
    },
    {
      "epoch": 0.07394113424264179,
      "grad_norm": 2.5894253253936768,
      "learning_rate": 4.92462311557789e-05,
      "loss": 1.4935,
      "step": 1030
    },
    {
      "epoch": 0.07465900933237617,
      "grad_norm": 4.7708420753479,
      "learning_rate": 4.972481454893516e-05,
      "loss": 1.4477,
      "step": 1040
    },
    {
      "epoch": 0.07537688442211055,
      "grad_norm": 2.4397072792053223,
      "learning_rate": 5.020339794209141e-05,
      "loss": 1.4105,
      "step": 1050
    },
    {
      "epoch": 0.07609475951184494,
      "grad_norm": 2.709057569503784,
      "learning_rate": 5.068198133524767e-05,
      "loss": 1.4431,
      "step": 1060
    },
    {
      "epoch": 0.07681263460157932,
      "grad_norm": 2.3625783920288086,
      "learning_rate": 5.1160564728403924e-05,
      "loss": 1.5343,
      "step": 1070
    },
    {
      "epoch": 0.07753050969131371,
      "grad_norm": 4.42650842666626,
      "learning_rate": 5.163914812156019e-05,
      "loss": 1.3878,
      "step": 1080
    },
    {
      "epoch": 0.07824838478104809,
      "grad_norm": 2.8134679794311523,
      "learning_rate": 5.211773151471644e-05,
      "loss": 1.4475,
      "step": 1090
    },
    {
      "epoch": 0.07896625987078248,
      "grad_norm": 3.06288743019104,
      "learning_rate": 5.25963149078727e-05,
      "loss": 1.413,
      "step": 1100
    },
    {
      "epoch": 0.07968413496051686,
      "grad_norm": 3.0025134086608887,
      "learning_rate": 5.3074898301028954e-05,
      "loss": 1.4636,
      "step": 1110
    },
    {
      "epoch": 0.08040201005025126,
      "grad_norm": 2.1776576042175293,
      "learning_rate": 5.355348169418522e-05,
      "loss": 1.4215,
      "step": 1120
    },
    {
      "epoch": 0.08111988513998564,
      "grad_norm": 4.126188278198242,
      "learning_rate": 5.4032065087341475e-05,
      "loss": 1.3194,
      "step": 1130
    },
    {
      "epoch": 0.08183776022972003,
      "grad_norm": 3.1780917644500732,
      "learning_rate": 5.4510648480497726e-05,
      "loss": 1.3628,
      "step": 1140
    },
    {
      "epoch": 0.08255563531945441,
      "grad_norm": 2.6346211433410645,
      "learning_rate": 5.498923187365399e-05,
      "loss": 1.4062,
      "step": 1150
    },
    {
      "epoch": 0.0832735104091888,
      "grad_norm": 3.326159715652466,
      "learning_rate": 5.546781526681024e-05,
      "loss": 1.3053,
      "step": 1160
    },
    {
      "epoch": 0.08399138549892318,
      "grad_norm": 3.6236445903778076,
      "learning_rate": 5.5946398659966505e-05,
      "loss": 1.435,
      "step": 1170
    },
    {
      "epoch": 0.08470926058865758,
      "grad_norm": 3.560957431793213,
      "learning_rate": 5.6424982053122756e-05,
      "loss": 1.3781,
      "step": 1180
    },
    {
      "epoch": 0.08542713567839195,
      "grad_norm": 3.8284475803375244,
      "learning_rate": 5.690356544627902e-05,
      "loss": 1.2626,
      "step": 1190
    },
    {
      "epoch": 0.08614501076812635,
      "grad_norm": 2.075970411300659,
      "learning_rate": 5.738214883943528e-05,
      "loss": 1.3245,
      "step": 1200
    },
    {
      "epoch": 0.08686288585786073,
      "grad_norm": 3.9108822345733643,
      "learning_rate": 5.786073223259153e-05,
      "loss": 1.4092,
      "step": 1210
    },
    {
      "epoch": 0.08758076094759512,
      "grad_norm": 3.8019468784332275,
      "learning_rate": 5.833931562574779e-05,
      "loss": 1.2928,
      "step": 1220
    },
    {
      "epoch": 0.0882986360373295,
      "grad_norm": 3.748494863510132,
      "learning_rate": 5.881789901890404e-05,
      "loss": 1.3432,
      "step": 1230
    },
    {
      "epoch": 0.0890165111270639,
      "grad_norm": 3.99947452545166,
      "learning_rate": 5.929648241206031e-05,
      "loss": 1.3838,
      "step": 1240
    },
    {
      "epoch": 0.08973438621679827,
      "grad_norm": 3.401789665222168,
      "learning_rate": 5.977506580521656e-05,
      "loss": 1.2676,
      "step": 1250
    },
    {
      "epoch": 0.09045226130653267,
      "grad_norm": 5.539731025695801,
      "learning_rate": 6.025364919837282e-05,
      "loss": 1.2266,
      "step": 1260
    },
    {
      "epoch": 0.09117013639626705,
      "grad_norm": 3.4242360591888428,
      "learning_rate": 6.073223259152908e-05,
      "loss": 1.2603,
      "step": 1270
    },
    {
      "epoch": 0.09188801148600144,
      "grad_norm": 3.390345335006714,
      "learning_rate": 6.121081598468533e-05,
      "loss": 1.1189,
      "step": 1280
    },
    {
      "epoch": 0.09260588657573582,
      "grad_norm": 4.255853652954102,
      "learning_rate": 6.16893993778416e-05,
      "loss": 1.3492,
      "step": 1290
    },
    {
      "epoch": 0.09332376166547021,
      "grad_norm": 5.684360980987549,
      "learning_rate": 6.216798277099784e-05,
      "loss": 1.2454,
      "step": 1300
    },
    {
      "epoch": 0.09404163675520459,
      "grad_norm": 3.348914384841919,
      "learning_rate": 6.26465661641541e-05,
      "loss": 1.2322,
      "step": 1310
    },
    {
      "epoch": 0.09475951184493898,
      "grad_norm": 2.8491337299346924,
      "learning_rate": 6.312514955731036e-05,
      "loss": 1.3105,
      "step": 1320
    },
    {
      "epoch": 0.09547738693467336,
      "grad_norm": 4.290123462677002,
      "learning_rate": 6.360373295046662e-05,
      "loss": 1.0627,
      "step": 1330
    },
    {
      "epoch": 0.09619526202440776,
      "grad_norm": 3.3092825412750244,
      "learning_rate": 6.408231634362289e-05,
      "loss": 1.2538,
      "step": 1340
    },
    {
      "epoch": 0.09691313711414214,
      "grad_norm": 2.836055278778076,
      "learning_rate": 6.456089973677913e-05,
      "loss": 1.1857,
      "step": 1350
    },
    {
      "epoch": 0.09763101220387653,
      "grad_norm": 5.234561443328857,
      "learning_rate": 6.50394831299354e-05,
      "loss": 1.237,
      "step": 1360
    },
    {
      "epoch": 0.09834888729361091,
      "grad_norm": 4.0797834396362305,
      "learning_rate": 6.551806652309165e-05,
      "loss": 1.3178,
      "step": 1370
    },
    {
      "epoch": 0.0990667623833453,
      "grad_norm": 4.727780818939209,
      "learning_rate": 6.59966499162479e-05,
      "loss": 1.0666,
      "step": 1380
    },
    {
      "epoch": 0.09978463747307968,
      "grad_norm": 8.29603385925293,
      "learning_rate": 6.647523330940418e-05,
      "loss": 1.0612,
      "step": 1390
    },
    {
      "epoch": 0.10050251256281408,
      "grad_norm": 8.412070274353027,
      "learning_rate": 6.695381670256042e-05,
      "loss": 1.0741,
      "step": 1400
    },
    {
      "epoch": 0.10122038765254845,
      "grad_norm": 5.2341532707214355,
      "learning_rate": 6.743240009571669e-05,
      "loss": 1.2502,
      "step": 1410
    },
    {
      "epoch": 0.10193826274228285,
      "grad_norm": 3.5218753814697266,
      "learning_rate": 6.791098348887293e-05,
      "loss": 1.1084,
      "step": 1420
    },
    {
      "epoch": 0.10265613783201723,
      "grad_norm": 4.137821197509766,
      "learning_rate": 6.83895668820292e-05,
      "loss": 1.067,
      "step": 1430
    },
    {
      "epoch": 0.10337401292175162,
      "grad_norm": 8.240632057189941,
      "learning_rate": 6.886815027518545e-05,
      "loss": 1.299,
      "step": 1440
    },
    {
      "epoch": 0.104091888011486,
      "grad_norm": 6.401393413543701,
      "learning_rate": 6.93467336683417e-05,
      "loss": 1.2005,
      "step": 1450
    },
    {
      "epoch": 0.1048097631012204,
      "grad_norm": 5.582906723022461,
      "learning_rate": 6.982531706149798e-05,
      "loss": 1.1599,
      "step": 1460
    },
    {
      "epoch": 0.10552763819095477,
      "grad_norm": 5.363931655883789,
      "learning_rate": 7.030390045465422e-05,
      "loss": 1.083,
      "step": 1470
    },
    {
      "epoch": 0.10624551328068917,
      "grad_norm": 6.151541233062744,
      "learning_rate": 7.078248384781049e-05,
      "loss": 1.0527,
      "step": 1480
    },
    {
      "epoch": 0.10696338837042355,
      "grad_norm": 4.001503944396973,
      "learning_rate": 7.126106724096674e-05,
      "loss": 1.0438,
      "step": 1490
    },
    {
      "epoch": 0.10768126346015794,
      "grad_norm": 6.217864513397217,
      "learning_rate": 7.1739650634123e-05,
      "loss": 1.1803,
      "step": 1500
    },
    {
      "epoch": 0.10839913854989232,
      "grad_norm": 2.8016159534454346,
      "learning_rate": 7.221823402727925e-05,
      "loss": 1.1704,
      "step": 1510
    },
    {
      "epoch": 0.10911701363962671,
      "grad_norm": 4.4364142417907715,
      "learning_rate": 7.269681742043551e-05,
      "loss": 1.1072,
      "step": 1520
    },
    {
      "epoch": 0.10983488872936109,
      "grad_norm": 2.4713282585144043,
      "learning_rate": 7.317540081359178e-05,
      "loss": 0.9767,
      "step": 1530
    },
    {
      "epoch": 0.11055276381909548,
      "grad_norm": 6.3961005210876465,
      "learning_rate": 7.365398420674802e-05,
      "loss": 1.1699,
      "step": 1540
    },
    {
      "epoch": 0.11127063890882986,
      "grad_norm": 2.6396937370300293,
      "learning_rate": 7.41325675999043e-05,
      "loss": 1.1372,
      "step": 1550
    },
    {
      "epoch": 0.11198851399856424,
      "grad_norm": 7.047487258911133,
      "learning_rate": 7.461115099306054e-05,
      "loss": 1.1002,
      "step": 1560
    },
    {
      "epoch": 0.11270638908829864,
      "grad_norm": 2.6243550777435303,
      "learning_rate": 7.508973438621681e-05,
      "loss": 1.0593,
      "step": 1570
    },
    {
      "epoch": 0.11342426417803302,
      "grad_norm": 4.237468719482422,
      "learning_rate": 7.556831777937305e-05,
      "loss": 1.1633,
      "step": 1580
    },
    {
      "epoch": 0.11414213926776741,
      "grad_norm": 4.69016695022583,
      "learning_rate": 7.604690117252932e-05,
      "loss": 1.1564,
      "step": 1590
    },
    {
      "epoch": 0.11486001435750179,
      "grad_norm": 3.33019757270813,
      "learning_rate": 7.652548456568558e-05,
      "loss": 0.9475,
      "step": 1600
    },
    {
      "epoch": 0.11557788944723618,
      "grad_norm": 7.647523403167725,
      "learning_rate": 7.700406795884182e-05,
      "loss": 0.963,
      "step": 1610
    },
    {
      "epoch": 0.11629576453697056,
      "grad_norm": 6.932960510253906,
      "learning_rate": 7.74826513519981e-05,
      "loss": 1.1205,
      "step": 1620
    },
    {
      "epoch": 0.11701363962670495,
      "grad_norm": 6.048612594604492,
      "learning_rate": 7.796123474515434e-05,
      "loss": 1.0081,
      "step": 1630
    },
    {
      "epoch": 0.11773151471643933,
      "grad_norm": 9.383432388305664,
      "learning_rate": 7.843981813831061e-05,
      "loss": 1.0969,
      "step": 1640
    },
    {
      "epoch": 0.11844938980617373,
      "grad_norm": 6.47802734375,
      "learning_rate": 7.891840153146685e-05,
      "loss": 1.1282,
      "step": 1650
    },
    {
      "epoch": 0.1191672648959081,
      "grad_norm": 6.443363666534424,
      "learning_rate": 7.939698492462313e-05,
      "loss": 1.0008,
      "step": 1660
    },
    {
      "epoch": 0.1198851399856425,
      "grad_norm": 5.676765441894531,
      "learning_rate": 7.987556831777938e-05,
      "loss": 0.8855,
      "step": 1670
    },
    {
      "epoch": 0.12060301507537688,
      "grad_norm": 11.1361722946167,
      "learning_rate": 8.035415171093563e-05,
      "loss": 1.1525,
      "step": 1680
    },
    {
      "epoch": 0.12132089016511127,
      "grad_norm": 5.4564924240112305,
      "learning_rate": 8.08327351040919e-05,
      "loss": 1.0345,
      "step": 1690
    },
    {
      "epoch": 0.12203876525484565,
      "grad_norm": 5.289679527282715,
      "learning_rate": 8.131131849724814e-05,
      "loss": 0.7943,
      "step": 1700
    },
    {
      "epoch": 0.12275664034458005,
      "grad_norm": 5.74692964553833,
      "learning_rate": 8.178990189040441e-05,
      "loss": 1.0827,
      "step": 1710
    },
    {
      "epoch": 0.12347451543431442,
      "grad_norm": 5.383884906768799,
      "learning_rate": 8.226848528356066e-05,
      "loss": 1.0641,
      "step": 1720
    },
    {
      "epoch": 0.12419239052404882,
      "grad_norm": 7.843161106109619,
      "learning_rate": 8.274706867671693e-05,
      "loss": 0.8659,
      "step": 1730
    },
    {
      "epoch": 0.1249102656137832,
      "grad_norm": 8.115278244018555,
      "learning_rate": 8.322565206987318e-05,
      "loss": 0.9888,
      "step": 1740
    },
    {
      "epoch": 0.12562814070351758,
      "grad_norm": 5.084176540374756,
      "learning_rate": 8.370423546302943e-05,
      "loss": 0.9691,
      "step": 1750
    },
    {
      "epoch": 0.12634601579325197,
      "grad_norm": 3.238266706466675,
      "learning_rate": 8.41828188561857e-05,
      "loss": 0.9925,
      "step": 1760
    },
    {
      "epoch": 0.12706389088298636,
      "grad_norm": 5.760537624359131,
      "learning_rate": 8.466140224934194e-05,
      "loss": 0.8247,
      "step": 1770
    },
    {
      "epoch": 0.12778176597272076,
      "grad_norm": 5.118504047393799,
      "learning_rate": 8.513998564249821e-05,
      "loss": 0.9862,
      "step": 1780
    },
    {
      "epoch": 0.12849964106245512,
      "grad_norm": 3.3820927143096924,
      "learning_rate": 8.561856903565447e-05,
      "loss": 0.8784,
      "step": 1790
    },
    {
      "epoch": 0.12921751615218952,
      "grad_norm": 15.020299911499023,
      "learning_rate": 8.609715242881073e-05,
      "loss": 0.99,
      "step": 1800
    },
    {
      "epoch": 0.1299353912419239,
      "grad_norm": 10.144827842712402,
      "learning_rate": 8.657573582196699e-05,
      "loss": 1.0072,
      "step": 1810
    },
    {
      "epoch": 0.1306532663316583,
      "grad_norm": 18.707775115966797,
      "learning_rate": 8.705431921512323e-05,
      "loss": 1.0446,
      "step": 1820
    },
    {
      "epoch": 0.13137114142139267,
      "grad_norm": 2.030277967453003,
      "learning_rate": 8.75329026082795e-05,
      "loss": 0.8966,
      "step": 1830
    },
    {
      "epoch": 0.13208901651112706,
      "grad_norm": 5.105950832366943,
      "learning_rate": 8.801148600143575e-05,
      "loss": 0.8731,
      "step": 1840
    },
    {
      "epoch": 0.13280689160086145,
      "grad_norm": 5.05544900894165,
      "learning_rate": 8.849006939459202e-05,
      "loss": 1.029,
      "step": 1850
    },
    {
      "epoch": 0.13352476669059585,
      "grad_norm": 6.325115203857422,
      "learning_rate": 8.896865278774827e-05,
      "loss": 0.8869,
      "step": 1860
    },
    {
      "epoch": 0.1342426417803302,
      "grad_norm": 7.615192890167236,
      "learning_rate": 8.944723618090453e-05,
      "loss": 0.998,
      "step": 1870
    },
    {
      "epoch": 0.1349605168700646,
      "grad_norm": 4.161067485809326,
      "learning_rate": 8.992581957406079e-05,
      "loss": 0.8176,
      "step": 1880
    },
    {
      "epoch": 0.135678391959799,
      "grad_norm": 10.043288230895996,
      "learning_rate": 9.040440296721703e-05,
      "loss": 0.9349,
      "step": 1890
    },
    {
      "epoch": 0.1363962670495334,
      "grad_norm": 11.2156343460083,
      "learning_rate": 9.08829863603733e-05,
      "loss": 0.7531,
      "step": 1900
    },
    {
      "epoch": 0.13711414213926776,
      "grad_norm": 5.6340227127075195,
      "learning_rate": 9.136156975352955e-05,
      "loss": 0.9626,
      "step": 1910
    },
    {
      "epoch": 0.13783201722900215,
      "grad_norm": 9.184479713439941,
      "learning_rate": 9.184015314668582e-05,
      "loss": 0.9832,
      "step": 1920
    },
    {
      "epoch": 0.13854989231873654,
      "grad_norm": 7.968409538269043,
      "learning_rate": 9.231873653984208e-05,
      "loss": 0.8454,
      "step": 1930
    },
    {
      "epoch": 0.13926776740847094,
      "grad_norm": 6.879753112792969,
      "learning_rate": 9.279731993299833e-05,
      "loss": 0.7955,
      "step": 1940
    },
    {
      "epoch": 0.1399856424982053,
      "grad_norm": 8.788783073425293,
      "learning_rate": 9.327590332615459e-05,
      "loss": 0.7551,
      "step": 1950
    },
    {
      "epoch": 0.1407035175879397,
      "grad_norm": 6.267629623413086,
      "learning_rate": 9.375448671931083e-05,
      "loss": 0.792,
      "step": 1960
    },
    {
      "epoch": 0.1414213926776741,
      "grad_norm": 6.02031946182251,
      "learning_rate": 9.42330701124671e-05,
      "loss": 0.7463,
      "step": 1970
    },
    {
      "epoch": 0.14213926776740848,
      "grad_norm": 13.969943046569824,
      "learning_rate": 9.471165350562335e-05,
      "loss": 1.2166,
      "step": 1980
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 9.194106101989746,
      "learning_rate": 9.519023689877962e-05,
      "loss": 0.9409,
      "step": 1990
    },
    {
      "epoch": 0.14357501794687724,
      "grad_norm": 14.815942764282227,
      "learning_rate": 9.566882029193588e-05,
      "loss": 1.0299,
      "step": 2000
    },
    {
      "epoch": 0.14429289303661164,
      "grad_norm": 5.014829635620117,
      "learning_rate": 9.614740368509214e-05,
      "loss": 0.8435,
      "step": 2010
    },
    {
      "epoch": 0.14501076812634603,
      "grad_norm": 12.062332153320312,
      "learning_rate": 9.662598707824839e-05,
      "loss": 0.7693,
      "step": 2020
    },
    {
      "epoch": 0.1457286432160804,
      "grad_norm": 5.35463809967041,
      "learning_rate": 9.710457047140465e-05,
      "loss": 0.8387,
      "step": 2030
    },
    {
      "epoch": 0.1464465183058148,
      "grad_norm": 2.4742238521575928,
      "learning_rate": 9.758315386456091e-05,
      "loss": 0.8814,
      "step": 2040
    },
    {
      "epoch": 0.14716439339554918,
      "grad_norm": 6.616336345672607,
      "learning_rate": 9.806173725771715e-05,
      "loss": 0.9781,
      "step": 2050
    },
    {
      "epoch": 0.14788226848528357,
      "grad_norm": 10.939091682434082,
      "learning_rate": 9.854032065087342e-05,
      "loss": 0.7903,
      "step": 2060
    },
    {
      "epoch": 0.14860014357501794,
      "grad_norm": 7.1362624168396,
      "learning_rate": 9.901890404402968e-05,
      "loss": 0.8672,
      "step": 2070
    },
    {
      "epoch": 0.14931801866475233,
      "grad_norm": 6.353552341461182,
      "learning_rate": 9.949748743718594e-05,
      "loss": 0.694,
      "step": 2080
    },
    {
      "epoch": 0.15003589375448673,
      "grad_norm": 9.171842575073242,
      "learning_rate": 9.99760708303422e-05,
      "loss": 0.9326,
      "step": 2090
    },
    {
      "epoch": 0.1507537688442211,
      "grad_norm": 3.3821277618408203,
      "learning_rate": 0.00010045465422349845,
      "loss": 0.9364,
      "step": 2100
    },
    {
      "epoch": 0.15147164393395549,
      "grad_norm": 5.643245697021484,
      "learning_rate": 0.0001009332376166547,
      "loss": 0.9211,
      "step": 2110
    },
    {
      "epoch": 0.15218951902368988,
      "grad_norm": 13.525012969970703,
      "learning_rate": 0.00010141182100981097,
      "loss": 0.9425,
      "step": 2120
    },
    {
      "epoch": 0.15290739411342427,
      "grad_norm": 6.765262603759766,
      "learning_rate": 0.00010189040440296722,
      "loss": 0.8873,
      "step": 2130
    },
    {
      "epoch": 0.15362526920315864,
      "grad_norm": 17.292112350463867,
      "learning_rate": 0.00010236898779612347,
      "loss": 0.7105,
      "step": 2140
    },
    {
      "epoch": 0.15434314429289303,
      "grad_norm": 9.959692001342773,
      "learning_rate": 0.00010284757118927974,
      "loss": 1.0505,
      "step": 2150
    },
    {
      "epoch": 0.15506101938262742,
      "grad_norm": 6.958469390869141,
      "learning_rate": 0.000103326154582436,
      "loss": 0.8959,
      "step": 2160
    },
    {
      "epoch": 0.15577889447236182,
      "grad_norm": 3.8736472129821777,
      "learning_rate": 0.00010380473797559225,
      "loss": 0.7775,
      "step": 2170
    },
    {
      "epoch": 0.15649676956209618,
      "grad_norm": 5.373655796051025,
      "learning_rate": 0.0001042833213687485,
      "loss": 0.7484,
      "step": 2180
    },
    {
      "epoch": 0.15721464465183058,
      "grad_norm": 7.68709659576416,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.9461,
      "step": 2190
    },
    {
      "epoch": 0.15793251974156497,
      "grad_norm": 6.573485851287842,
      "learning_rate": 0.00010524048815506103,
      "loss": 0.8246,
      "step": 2200
    },
    {
      "epoch": 0.15865039483129936,
      "grad_norm": 24.33491325378418,
      "learning_rate": 0.00010571907154821727,
      "loss": 0.6506,
      "step": 2210
    },
    {
      "epoch": 0.15936826992103373,
      "grad_norm": 6.94605016708374,
      "learning_rate": 0.00010619765494137354,
      "loss": 0.8385,
      "step": 2220
    },
    {
      "epoch": 0.16008614501076812,
      "grad_norm": 9.930744171142578,
      "learning_rate": 0.0001066762383345298,
      "loss": 0.8252,
      "step": 2230
    },
    {
      "epoch": 0.16080402010050251,
      "grad_norm": 7.9328155517578125,
      "learning_rate": 0.00010715482172768606,
      "loss": 0.5707,
      "step": 2240
    },
    {
      "epoch": 0.1615218951902369,
      "grad_norm": 6.82647705078125,
      "learning_rate": 0.0001076334051208423,
      "loss": 0.7431,
      "step": 2250
    },
    {
      "epoch": 0.16223977027997127,
      "grad_norm": 6.889920234680176,
      "learning_rate": 0.00010811198851399857,
      "loss": 0.9678,
      "step": 2260
    },
    {
      "epoch": 0.16295764536970567,
      "grad_norm": 11.620723724365234,
      "learning_rate": 0.00010859057190715483,
      "loss": 0.9276,
      "step": 2270
    },
    {
      "epoch": 0.16367552045944006,
      "grad_norm": 5.477067947387695,
      "learning_rate": 0.00010906915530031107,
      "loss": 0.8079,
      "step": 2280
    },
    {
      "epoch": 0.16439339554917445,
      "grad_norm": 6.905364513397217,
      "learning_rate": 0.00010954773869346736,
      "loss": 0.7724,
      "step": 2290
    },
    {
      "epoch": 0.16511127063890882,
      "grad_norm": 3.5112037658691406,
      "learning_rate": 0.0001100263220866236,
      "loss": 0.8345,
      "step": 2300
    },
    {
      "epoch": 0.1658291457286432,
      "grad_norm": 3.988696813583374,
      "learning_rate": 0.00011050490547977986,
      "loss": 0.4657,
      "step": 2310
    },
    {
      "epoch": 0.1665470208183776,
      "grad_norm": 5.467114448547363,
      "learning_rate": 0.00011098348887293613,
      "loss": 0.8694,
      "step": 2320
    },
    {
      "epoch": 0.167264895908112,
      "grad_norm": 7.147907733917236,
      "learning_rate": 0.00011146207226609237,
      "loss": 0.7485,
      "step": 2330
    },
    {
      "epoch": 0.16798277099784636,
      "grad_norm": NaN,
      "learning_rate": 0.00011194065565924863,
      "loss": 0.7853,
      "step": 2340
    },
    {
      "epoch": 0.16870064608758076,
      "grad_norm": 14.907742500305176,
      "learning_rate": 0.00011241923905240487,
      "loss": 0.7109,
      "step": 2350
    },
    {
      "epoch": 0.16941852117731515,
      "grad_norm": 3.100329875946045,
      "learning_rate": 0.00011289782244556116,
      "loss": 0.7739,
      "step": 2360
    },
    {
      "epoch": 0.17013639626704954,
      "grad_norm": 8.877232551574707,
      "learning_rate": 0.0001133764058387174,
      "loss": 0.7135,
      "step": 2370
    },
    {
      "epoch": 0.1708542713567839,
      "grad_norm": 4.095461368560791,
      "learning_rate": 0.00011385498923187366,
      "loss": 0.6622,
      "step": 2380
    },
    {
      "epoch": 0.1715721464465183,
      "grad_norm": 5.261692523956299,
      "learning_rate": 0.00011433357262502993,
      "loss": 0.7047,
      "step": 2390
    },
    {
      "epoch": 0.1722900215362527,
      "grad_norm": 5.572995662689209,
      "learning_rate": 0.00011481215601818617,
      "loss": 0.61,
      "step": 2400
    },
    {
      "epoch": 0.1730078966259871,
      "grad_norm": 11.10497760772705,
      "learning_rate": 0.00011529073941134243,
      "loss": 0.6879,
      "step": 2410
    },
    {
      "epoch": 0.17372577171572146,
      "grad_norm": 6.496926784515381,
      "learning_rate": 0.00011576932280449868,
      "loss": 0.7433,
      "step": 2420
    },
    {
      "epoch": 0.17444364680545585,
      "grad_norm": 5.608358383178711,
      "learning_rate": 0.00011624790619765496,
      "loss": 0.8487,
      "step": 2430
    },
    {
      "epoch": 0.17516152189519024,
      "grad_norm": 8.422104835510254,
      "learning_rate": 0.0001167264895908112,
      "loss": 0.6935,
      "step": 2440
    },
    {
      "epoch": 0.17587939698492464,
      "grad_norm": 2.0980517864227295,
      "learning_rate": 0.00011720507298396746,
      "loss": 0.6888,
      "step": 2450
    },
    {
      "epoch": 0.176597272074659,
      "grad_norm": 14.499276161193848,
      "learning_rate": 0.00011768365637712373,
      "loss": 0.6267,
      "step": 2460
    },
    {
      "epoch": 0.1773151471643934,
      "grad_norm": 8.760613441467285,
      "learning_rate": 0.00011816223977027998,
      "loss": 0.7062,
      "step": 2470
    },
    {
      "epoch": 0.1780330222541278,
      "grad_norm": 8.471842765808105,
      "learning_rate": 0.00011864082316343623,
      "loss": 0.6625,
      "step": 2480
    },
    {
      "epoch": 0.17875089734386218,
      "grad_norm": 10.356825828552246,
      "learning_rate": 0.00011911940655659248,
      "loss": 0.8655,
      "step": 2490
    },
    {
      "epoch": 0.17946877243359655,
      "grad_norm": 9.727413177490234,
      "learning_rate": 0.00011959798994974876,
      "loss": 0.7242,
      "step": 2500
    },
    {
      "epoch": 0.18018664752333094,
      "grad_norm": 11.176281929016113,
      "learning_rate": 0.000120076573342905,
      "loss": 0.5737,
      "step": 2510
    },
    {
      "epoch": 0.18090452261306533,
      "grad_norm": 7.602197170257568,
      "learning_rate": 0.00012055515673606126,
      "loss": 0.6335,
      "step": 2520
    },
    {
      "epoch": 0.18162239770279973,
      "grad_norm": 6.86177921295166,
      "learning_rate": 0.00012103374012921753,
      "loss": 0.8596,
      "step": 2530
    },
    {
      "epoch": 0.1823402727925341,
      "grad_norm": 5.123377799987793,
      "learning_rate": 0.00012151232352237378,
      "loss": 0.6754,
      "step": 2540
    },
    {
      "epoch": 0.18305814788226848,
      "grad_norm": 12.986041069030762,
      "learning_rate": 0.00012199090691553004,
      "loss": 0.7505,
      "step": 2550
    },
    {
      "epoch": 0.18377602297200288,
      "grad_norm": 11.9631986618042,
      "learning_rate": 0.00012246949030868628,
      "loss": 0.717,
      "step": 2560
    },
    {
      "epoch": 0.18449389806173727,
      "grad_norm": 4.9883599281311035,
      "learning_rate": 0.00012294807370184256,
      "loss": 0.6439,
      "step": 2570
    },
    {
      "epoch": 0.18521177315147164,
      "grad_norm": 9.047348976135254,
      "learning_rate": 0.00012342665709499882,
      "loss": 0.6022,
      "step": 2580
    },
    {
      "epoch": 0.18592964824120603,
      "grad_norm": 6.597160339355469,
      "learning_rate": 0.00012390524048815505,
      "loss": 0.6684,
      "step": 2590
    },
    {
      "epoch": 0.18664752333094042,
      "grad_norm": 8.051156997680664,
      "learning_rate": 0.00012438382388131134,
      "loss": 0.4393,
      "step": 2600
    },
    {
      "epoch": 0.1873653984206748,
      "grad_norm": 9.522806167602539,
      "learning_rate": 0.0001248624072744676,
      "loss": 0.6191,
      "step": 2610
    },
    {
      "epoch": 0.18808327351040918,
      "grad_norm": 4.117171287536621,
      "learning_rate": 0.00012534099066762382,
      "loss": 0.6132,
      "step": 2620
    },
    {
      "epoch": 0.18880114860014358,
      "grad_norm": 5.601903915405273,
      "learning_rate": 0.00012581957406078008,
      "loss": 0.6359,
      "step": 2630
    },
    {
      "epoch": 0.18951902368987797,
      "grad_norm": 7.992208003997803,
      "learning_rate": 0.00012629815745393637,
      "loss": 0.9735,
      "step": 2640
    },
    {
      "epoch": 0.19023689877961233,
      "grad_norm": 15.124784469604492,
      "learning_rate": 0.00012677674084709262,
      "loss": 0.735,
      "step": 2650
    },
    {
      "epoch": 0.19095477386934673,
      "grad_norm": 4.907828330993652,
      "learning_rate": 0.00012725532424024885,
      "loss": 0.7049,
      "step": 2660
    },
    {
      "epoch": 0.19167264895908112,
      "grad_norm": 9.75167179107666,
      "learning_rate": 0.00012773390763340514,
      "loss": 0.7065,
      "step": 2670
    },
    {
      "epoch": 0.19239052404881551,
      "grad_norm": 14.89174747467041,
      "learning_rate": 0.0001282124910265614,
      "loss": 0.7381,
      "step": 2680
    },
    {
      "epoch": 0.19310839913854988,
      "grad_norm": 9.204333305358887,
      "learning_rate": 0.00012869107441971763,
      "loss": 0.8075,
      "step": 2690
    },
    {
      "epoch": 0.19382627422828427,
      "grad_norm": 11.886959075927734,
      "learning_rate": 0.00012916965781287388,
      "loss": 0.6364,
      "step": 2700
    },
    {
      "epoch": 0.19454414931801867,
      "grad_norm": 9.528482437133789,
      "learning_rate": 0.00012964824120603017,
      "loss": 0.4559,
      "step": 2710
    },
    {
      "epoch": 0.19526202440775306,
      "grad_norm": 7.994287967681885,
      "learning_rate": 0.00013012682459918643,
      "loss": 0.6437,
      "step": 2720
    },
    {
      "epoch": 0.19597989949748743,
      "grad_norm": 10.91869068145752,
      "learning_rate": 0.00013060540799234266,
      "loss": 0.6766,
      "step": 2730
    },
    {
      "epoch": 0.19669777458722182,
      "grad_norm": 8.779091835021973,
      "learning_rate": 0.00013108399138549894,
      "loss": 0.5577,
      "step": 2740
    },
    {
      "epoch": 0.1974156496769562,
      "grad_norm": 7.145724296569824,
      "learning_rate": 0.0001315625747786552,
      "loss": 0.5535,
      "step": 2750
    },
    {
      "epoch": 0.1981335247666906,
      "grad_norm": 13.842679023742676,
      "learning_rate": 0.00013204115817181143,
      "loss": 0.7507,
      "step": 2760
    },
    {
      "epoch": 0.19885139985642497,
      "grad_norm": 5.403249740600586,
      "learning_rate": 0.00013251974156496769,
      "loss": 0.9176,
      "step": 2770
    },
    {
      "epoch": 0.19956927494615936,
      "grad_norm": 5.739748954772949,
      "learning_rate": 0.00013299832495812397,
      "loss": 0.4811,
      "step": 2780
    },
    {
      "epoch": 0.20028715003589376,
      "grad_norm": 5.3962626457214355,
      "learning_rate": 0.00013347690835128023,
      "loss": 0.7281,
      "step": 2790
    },
    {
      "epoch": 0.20100502512562815,
      "grad_norm": 9.795866966247559,
      "learning_rate": 0.00013395549174443646,
      "loss": 0.5901,
      "step": 2800
    },
    {
      "epoch": 0.20172290021536252,
      "grad_norm": 8.577753067016602,
      "learning_rate": 0.00013443407513759274,
      "loss": 0.7144,
      "step": 2810
    },
    {
      "epoch": 0.2024407753050969,
      "grad_norm": 10.673067092895508,
      "learning_rate": 0.000134912658530749,
      "loss": 0.8284,
      "step": 2820
    },
    {
      "epoch": 0.2031586503948313,
      "grad_norm": 6.139074802398682,
      "learning_rate": 0.00013539124192390523,
      "loss": 0.581,
      "step": 2830
    },
    {
      "epoch": 0.2038765254845657,
      "grad_norm": 6.311159610748291,
      "learning_rate": 0.0001358698253170615,
      "loss": 0.5736,
      "step": 2840
    },
    {
      "epoch": 0.20459440057430006,
      "grad_norm": 11.692688941955566,
      "learning_rate": 0.00013634840871021777,
      "loss": 0.6098,
      "step": 2850
    },
    {
      "epoch": 0.20531227566403445,
      "grad_norm": 5.0836181640625,
      "learning_rate": 0.00013682699210337403,
      "loss": 0.6399,
      "step": 2860
    },
    {
      "epoch": 0.20603015075376885,
      "grad_norm": 8.14512825012207,
      "learning_rate": 0.00013730557549653026,
      "loss": 0.707,
      "step": 2870
    },
    {
      "epoch": 0.20674802584350324,
      "grad_norm": 13.538914680480957,
      "learning_rate": 0.00013778415888968654,
      "loss": 0.5331,
      "step": 2880
    },
    {
      "epoch": 0.2074659009332376,
      "grad_norm": 18.874252319335938,
      "learning_rate": 0.0001382627422828428,
      "loss": 0.8376,
      "step": 2890
    },
    {
      "epoch": 0.208183776022972,
      "grad_norm": 13.792805671691895,
      "learning_rate": 0.00013874132567599903,
      "loss": 0.8831,
      "step": 2900
    },
    {
      "epoch": 0.2089016511127064,
      "grad_norm": 25.923690795898438,
      "learning_rate": 0.0001392199090691553,
      "loss": 0.8851,
      "step": 2910
    },
    {
      "epoch": 0.2096195262024408,
      "grad_norm": 11.843143463134766,
      "learning_rate": 0.00013969849246231157,
      "loss": 0.7247,
      "step": 2920
    },
    {
      "epoch": 0.21033740129217515,
      "grad_norm": 9.90977668762207,
      "learning_rate": 0.00014017707585546783,
      "loss": 0.7203,
      "step": 2930
    },
    {
      "epoch": 0.21105527638190955,
      "grad_norm": 9.637714385986328,
      "learning_rate": 0.00014065565924862406,
      "loss": 0.6989,
      "step": 2940
    },
    {
      "epoch": 0.21177315147164394,
      "grad_norm": 5.723646640777588,
      "learning_rate": 0.00014113424264178035,
      "loss": 0.7532,
      "step": 2950
    },
    {
      "epoch": 0.21249102656137833,
      "grad_norm": 17.064239501953125,
      "learning_rate": 0.0001416128260349366,
      "loss": 0.5989,
      "step": 2960
    },
    {
      "epoch": 0.2132089016511127,
      "grad_norm": 9.054686546325684,
      "learning_rate": 0.00014209140942809283,
      "loss": 0.5036,
      "step": 2970
    },
    {
      "epoch": 0.2139267767408471,
      "grad_norm": 14.086661338806152,
      "learning_rate": 0.0001425699928212491,
      "loss": 0.6187,
      "step": 2980
    },
    {
      "epoch": 0.21464465183058148,
      "grad_norm": 12.19704532623291,
      "learning_rate": 0.00014304857621440538,
      "loss": 0.7291,
      "step": 2990
    },
    {
      "epoch": 0.21536252692031588,
      "grad_norm": 22.42854881286621,
      "learning_rate": 0.00014352715960756163,
      "loss": 0.8748,
      "step": 3000
    },
    {
      "epoch": 0.21608040201005024,
      "grad_norm": 9.714693069458008,
      "learning_rate": 0.00014400574300071786,
      "loss": 0.5655,
      "step": 3010
    },
    {
      "epoch": 0.21679827709978464,
      "grad_norm": 14.807022094726562,
      "learning_rate": 0.00014448432639387415,
      "loss": 0.8005,
      "step": 3020
    },
    {
      "epoch": 0.21751615218951903,
      "grad_norm": 18.7128849029541,
      "learning_rate": 0.0001449629097870304,
      "loss": 0.673,
      "step": 3030
    },
    {
      "epoch": 0.21823402727925342,
      "grad_norm": 9.080941200256348,
      "learning_rate": 0.00014544149318018666,
      "loss": 0.5411,
      "step": 3040
    },
    {
      "epoch": 0.2189519023689878,
      "grad_norm": 10.273338317871094,
      "learning_rate": 0.0001459200765733429,
      "loss": 0.5779,
      "step": 3050
    },
    {
      "epoch": 0.21966977745872218,
      "grad_norm": 4.234987735748291,
      "learning_rate": 0.00014639865996649918,
      "loss": 0.563,
      "step": 3060
    },
    {
      "epoch": 0.22038765254845658,
      "grad_norm": 9.487408638000488,
      "learning_rate": 0.00014687724335965544,
      "loss": 0.7331,
      "step": 3070
    },
    {
      "epoch": 0.22110552763819097,
      "grad_norm": 26.212820053100586,
      "learning_rate": 0.00014735582675281167,
      "loss": 0.7097,
      "step": 3080
    },
    {
      "epoch": 0.22182340272792533,
      "grad_norm": 9.132706642150879,
      "learning_rate": 0.00014783441014596795,
      "loss": 0.5833,
      "step": 3090
    },
    {
      "epoch": 0.22254127781765973,
      "grad_norm": 1.9778395891189575,
      "learning_rate": 0.0001483129935391242,
      "loss": 0.7559,
      "step": 3100
    },
    {
      "epoch": 0.22325915290739412,
      "grad_norm": 9.06186294555664,
      "learning_rate": 0.00014879157693228047,
      "loss": 0.6076,
      "step": 3110
    },
    {
      "epoch": 0.22397702799712849,
      "grad_norm": 16.21387481689453,
      "learning_rate": 0.00014927016032543672,
      "loss": 0.7252,
      "step": 3120
    },
    {
      "epoch": 0.22469490308686288,
      "grad_norm": 6.9850873947143555,
      "learning_rate": 0.00014974874371859298,
      "loss": 0.4331,
      "step": 3130
    },
    {
      "epoch": 0.22541277817659727,
      "grad_norm": 3.291016101837158,
      "learning_rate": 0.00015022732711174924,
      "loss": 0.564,
      "step": 3140
    },
    {
      "epoch": 0.22613065326633167,
      "grad_norm": 4.393535614013672,
      "learning_rate": 0.00015070591050490547,
      "loss": 0.4705,
      "step": 3150
    },
    {
      "epoch": 0.22684852835606603,
      "grad_norm": 14.522414207458496,
      "learning_rate": 0.00015118449389806175,
      "loss": 0.5698,
      "step": 3160
    },
    {
      "epoch": 0.22756640344580042,
      "grad_norm": 9.782432556152344,
      "learning_rate": 0.000151663077291218,
      "loss": 0.7435,
      "step": 3170
    },
    {
      "epoch": 0.22828427853553482,
      "grad_norm": 7.542337894439697,
      "learning_rate": 0.00015214166068437427,
      "loss": 0.4843,
      "step": 3180
    },
    {
      "epoch": 0.2290021536252692,
      "grad_norm": 6.674903392791748,
      "learning_rate": 0.00015262024407753052,
      "loss": 0.5537,
      "step": 3190
    },
    {
      "epoch": 0.22972002871500358,
      "grad_norm": 4.597530364990234,
      "learning_rate": 0.00015309882747068678,
      "loss": 0.5325,
      "step": 3200
    },
    {
      "epoch": 0.23043790380473797,
      "grad_norm": 6.6177191734313965,
      "learning_rate": 0.00015357741086384304,
      "loss": 0.5083,
      "step": 3210
    },
    {
      "epoch": 0.23115577889447236,
      "grad_norm": 8.7581148147583,
      "learning_rate": 0.00015405599425699927,
      "loss": 0.6325,
      "step": 3220
    },
    {
      "epoch": 0.23187365398420676,
      "grad_norm": 2.469641923904419,
      "learning_rate": 0.00015453457765015555,
      "loss": 0.4229,
      "step": 3230
    },
    {
      "epoch": 0.23259152907394112,
      "grad_norm": 11.01044750213623,
      "learning_rate": 0.0001550131610433118,
      "loss": 0.539,
      "step": 3240
    },
    {
      "epoch": 0.23330940416367552,
      "grad_norm": 2.4956064224243164,
      "learning_rate": 0.00015549174443646807,
      "loss": 0.3817,
      "step": 3250
    },
    {
      "epoch": 0.2340272792534099,
      "grad_norm": 11.202369689941406,
      "learning_rate": 0.00015597032782962433,
      "loss": 0.5843,
      "step": 3260
    },
    {
      "epoch": 0.2347451543431443,
      "grad_norm": 7.298858165740967,
      "learning_rate": 0.00015644891122278058,
      "loss": 0.5296,
      "step": 3270
    },
    {
      "epoch": 0.23546302943287867,
      "grad_norm": 10.163223266601562,
      "learning_rate": 0.00015692749461593684,
      "loss": 0.6379,
      "step": 3280
    },
    {
      "epoch": 0.23618090452261306,
      "grad_norm": 9.196334838867188,
      "learning_rate": 0.00015740607800909307,
      "loss": 0.6336,
      "step": 3290
    },
    {
      "epoch": 0.23689877961234745,
      "grad_norm": 15.86329174041748,
      "learning_rate": 0.00015788466140224936,
      "loss": 0.6961,
      "step": 3300
    },
    {
      "epoch": 0.23761665470208185,
      "grad_norm": 9.804224967956543,
      "learning_rate": 0.00015836324479540561,
      "loss": 0.5858,
      "step": 3310
    },
    {
      "epoch": 0.2383345297918162,
      "grad_norm": 4.930755138397217,
      "learning_rate": 0.00015884182818856187,
      "loss": 0.5559,
      "step": 3320
    },
    {
      "epoch": 0.2390524048815506,
      "grad_norm": 2.841435670852661,
      "learning_rate": 0.00015932041158171813,
      "loss": 0.53,
      "step": 3330
    },
    {
      "epoch": 0.239770279971285,
      "grad_norm": 6.809842109680176,
      "learning_rate": 0.00015979899497487439,
      "loss": 0.5969,
      "step": 3340
    },
    {
      "epoch": 0.2404881550610194,
      "grad_norm": 7.742214202880859,
      "learning_rate": 0.00016027757836803064,
      "loss": 0.5026,
      "step": 3350
    },
    {
      "epoch": 0.24120603015075376,
      "grad_norm": 5.924073696136475,
      "learning_rate": 0.00016075616176118687,
      "loss": 0.5304,
      "step": 3360
    },
    {
      "epoch": 0.24192390524048815,
      "grad_norm": 9.11752700805664,
      "learning_rate": 0.00016123474515434316,
      "loss": 0.585,
      "step": 3370
    },
    {
      "epoch": 0.24264178033022255,
      "grad_norm": 1.3658801317214966,
      "learning_rate": 0.00016171332854749942,
      "loss": 0.6718,
      "step": 3380
    },
    {
      "epoch": 0.24335965541995694,
      "grad_norm": 7.507955551147461,
      "learning_rate": 0.00016219191194065567,
      "loss": 0.5923,
      "step": 3390
    },
    {
      "epoch": 0.2440775305096913,
      "grad_norm": 7.722149848937988,
      "learning_rate": 0.00016267049533381193,
      "loss": 0.4277,
      "step": 3400
    },
    {
      "epoch": 0.2447954055994257,
      "grad_norm": 13.858388900756836,
      "learning_rate": 0.0001631490787269682,
      "loss": 0.5101,
      "step": 3410
    },
    {
      "epoch": 0.2455132806891601,
      "grad_norm": 11.119125366210938,
      "learning_rate": 0.00016362766212012445,
      "loss": 0.5082,
      "step": 3420
    },
    {
      "epoch": 0.24623115577889448,
      "grad_norm": 6.957077503204346,
      "learning_rate": 0.00016410624551328068,
      "loss": 0.5271,
      "step": 3430
    },
    {
      "epoch": 0.24694903086862885,
      "grad_norm": 8.251750946044922,
      "learning_rate": 0.00016458482890643696,
      "loss": 0.3949,
      "step": 3440
    },
    {
      "epoch": 0.24766690595836324,
      "grad_norm": 11.67891788482666,
      "learning_rate": 0.00016506341229959322,
      "loss": 0.7389,
      "step": 3450
    },
    {
      "epoch": 0.24838478104809764,
      "grad_norm": 9.634100914001465,
      "learning_rate": 0.00016554199569274948,
      "loss": 0.6902,
      "step": 3460
    },
    {
      "epoch": 0.24910265613783203,
      "grad_norm": 5.616624355316162,
      "learning_rate": 0.00016602057908590573,
      "loss": 0.54,
      "step": 3470
    },
    {
      "epoch": 0.2498205312275664,
      "grad_norm": 6.906912326812744,
      "learning_rate": 0.000166499162479062,
      "loss": 0.5188,
      "step": 3480
    },
    {
      "epoch": 0.2505384063173008,
      "grad_norm": 19.347566604614258,
      "learning_rate": 0.00016697774587221825,
      "loss": 0.5423,
      "step": 3490
    },
    {
      "epoch": 0.25125628140703515,
      "grad_norm": 29.376249313354492,
      "learning_rate": 0.00016745632926537448,
      "loss": 0.5733,
      "step": 3500
    },
    {
      "epoch": 0.2519741564967696,
      "grad_norm": 19.529911041259766,
      "learning_rate": 0.00016793491265853076,
      "loss": 0.8297,
      "step": 3510
    },
    {
      "epoch": 0.25269203158650394,
      "grad_norm": 10.090792655944824,
      "learning_rate": 0.00016841349605168702,
      "loss": 0.6397,
      "step": 3520
    },
    {
      "epoch": 0.25340990667623836,
      "grad_norm": 5.996514797210693,
      "learning_rate": 0.00016889207944484328,
      "loss": 0.5551,
      "step": 3530
    },
    {
      "epoch": 0.2541277817659727,
      "grad_norm": 14.355762481689453,
      "learning_rate": 0.00016937066283799953,
      "loss": 0.4565,
      "step": 3540
    },
    {
      "epoch": 0.2548456568557071,
      "grad_norm": 5.475924968719482,
      "learning_rate": 0.0001698492462311558,
      "loss": 0.5119,
      "step": 3550
    },
    {
      "epoch": 0.2555635319454415,
      "grad_norm": 8.232370376586914,
      "learning_rate": 0.00017032782962431205,
      "loss": 0.5185,
      "step": 3560
    },
    {
      "epoch": 0.2562814070351759,
      "grad_norm": 22.18854331970215,
      "learning_rate": 0.00017080641301746828,
      "loss": 0.5858,
      "step": 3570
    },
    {
      "epoch": 0.25699928212491024,
      "grad_norm": 10.752120971679688,
      "learning_rate": 0.00017128499641062456,
      "loss": 0.5115,
      "step": 3580
    },
    {
      "epoch": 0.25771715721464467,
      "grad_norm": 2.6648061275482178,
      "learning_rate": 0.00017176357980378082,
      "loss": 0.55,
      "step": 3590
    },
    {
      "epoch": 0.25843503230437903,
      "grad_norm": 1.0402302742004395,
      "learning_rate": 0.00017224216319693708,
      "loss": 0.4172,
      "step": 3600
    },
    {
      "epoch": 0.25915290739411345,
      "grad_norm": 7.9062628746032715,
      "learning_rate": 0.00017272074659009334,
      "loss": 0.461,
      "step": 3610
    },
    {
      "epoch": 0.2598707824838478,
      "grad_norm": 2.090742826461792,
      "learning_rate": 0.0001731993299832496,
      "loss": 0.2986,
      "step": 3620
    },
    {
      "epoch": 0.2605886575735822,
      "grad_norm": 4.3103437423706055,
      "learning_rate": 0.00017367791337640585,
      "loss": 0.4783,
      "step": 3630
    },
    {
      "epoch": 0.2613065326633166,
      "grad_norm": 18.174556732177734,
      "learning_rate": 0.00017415649676956208,
      "loss": 0.4286,
      "step": 3640
    },
    {
      "epoch": 0.26202440775305097,
      "grad_norm": 7.257902145385742,
      "learning_rate": 0.00017463508016271837,
      "loss": 0.4998,
      "step": 3650
    },
    {
      "epoch": 0.26274228284278534,
      "grad_norm": 7.776939868927002,
      "learning_rate": 0.00017511366355587462,
      "loss": 0.7306,
      "step": 3660
    },
    {
      "epoch": 0.26346015793251976,
      "grad_norm": 12.777061462402344,
      "learning_rate": 0.00017559224694903088,
      "loss": 0.4998,
      "step": 3670
    },
    {
      "epoch": 0.2641780330222541,
      "grad_norm": 11.837723731994629,
      "learning_rate": 0.00017607083034218714,
      "loss": 0.5112,
      "step": 3680
    },
    {
      "epoch": 0.2648959081119885,
      "grad_norm": 11.965012550354004,
      "learning_rate": 0.0001765494137353434,
      "loss": 0.592,
      "step": 3690
    },
    {
      "epoch": 0.2656137832017229,
      "grad_norm": 6.230838775634766,
      "learning_rate": 0.00017702799712849965,
      "loss": 0.6484,
      "step": 3700
    },
    {
      "epoch": 0.2663316582914573,
      "grad_norm": 10.860459327697754,
      "learning_rate": 0.00017750658052165588,
      "loss": 0.4413,
      "step": 3710
    },
    {
      "epoch": 0.2670495333811917,
      "grad_norm": 16.325971603393555,
      "learning_rate": 0.00017798516391481217,
      "loss": 0.7094,
      "step": 3720
    },
    {
      "epoch": 0.26776740847092606,
      "grad_norm": 15.324586868286133,
      "learning_rate": 0.00017846374730796843,
      "loss": 0.5569,
      "step": 3730
    },
    {
      "epoch": 0.2684852835606604,
      "grad_norm": 13.545082092285156,
      "learning_rate": 0.00017894233070112468,
      "loss": 0.4676,
      "step": 3740
    },
    {
      "epoch": 0.26920315865039485,
      "grad_norm": 6.281307220458984,
      "learning_rate": 0.00017942091409428094,
      "loss": 0.3922,
      "step": 3750
    },
    {
      "epoch": 0.2699210337401292,
      "grad_norm": 6.586626052856445,
      "learning_rate": 0.0001798994974874372,
      "loss": 0.693,
      "step": 3760
    },
    {
      "epoch": 0.2706389088298636,
      "grad_norm": 11.07156753540039,
      "learning_rate": 0.00018037808088059346,
      "loss": 0.4992,
      "step": 3770
    },
    {
      "epoch": 0.271356783919598,
      "grad_norm": 9.745314598083496,
      "learning_rate": 0.00018085666427374969,
      "loss": 0.6839,
      "step": 3780
    },
    {
      "epoch": 0.27207465900933236,
      "grad_norm": 4.076261520385742,
      "learning_rate": 0.00018133524766690597,
      "loss": 0.3949,
      "step": 3790
    },
    {
      "epoch": 0.2727925340990668,
      "grad_norm": 9.87498950958252,
      "learning_rate": 0.00018181383106006223,
      "loss": 0.5057,
      "step": 3800
    },
    {
      "epoch": 0.27351040918880115,
      "grad_norm": 10.1710844039917,
      "learning_rate": 0.00018229241445321848,
      "loss": 0.384,
      "step": 3810
    },
    {
      "epoch": 0.2742282842785355,
      "grad_norm": 11.352396011352539,
      "learning_rate": 0.00018277099784637474,
      "loss": 0.6622,
      "step": 3820
    },
    {
      "epoch": 0.27494615936826994,
      "grad_norm": 1.4675006866455078,
      "learning_rate": 0.000183249581239531,
      "loss": 0.545,
      "step": 3830
    },
    {
      "epoch": 0.2756640344580043,
      "grad_norm": 12.436784744262695,
      "learning_rate": 0.00018372816463268726,
      "loss": 0.4749,
      "step": 3840
    },
    {
      "epoch": 0.27638190954773867,
      "grad_norm": 5.627370834350586,
      "learning_rate": 0.00018420674802584351,
      "loss": 0.4601,
      "step": 3850
    },
    {
      "epoch": 0.2770997846374731,
      "grad_norm": 18.027587890625,
      "learning_rate": 0.00018468533141899977,
      "loss": 0.6146,
      "step": 3860
    },
    {
      "epoch": 0.27781765972720746,
      "grad_norm": 6.252625942230225,
      "learning_rate": 0.00018516391481215603,
      "loss": 0.6059,
      "step": 3870
    },
    {
      "epoch": 0.2785355348169419,
      "grad_norm": 2.083648443222046,
      "learning_rate": 0.0001856424982053123,
      "loss": 0.5965,
      "step": 3880
    },
    {
      "epoch": 0.27925340990667624,
      "grad_norm": 7.173253059387207,
      "learning_rate": 0.00018612108159846854,
      "loss": 0.7397,
      "step": 3890
    },
    {
      "epoch": 0.2799712849964106,
      "grad_norm": 1.056167721748352,
      "learning_rate": 0.0001865996649916248,
      "loss": 0.4122,
      "step": 3900
    },
    {
      "epoch": 0.28068916008614503,
      "grad_norm": 12.015295028686523,
      "learning_rate": 0.00018707824838478106,
      "loss": 0.394,
      "step": 3910
    },
    {
      "epoch": 0.2814070351758794,
      "grad_norm": 4.733788967132568,
      "learning_rate": 0.00018755683177793732,
      "loss": 0.6311,
      "step": 3920
    },
    {
      "epoch": 0.28212491026561376,
      "grad_norm": 3.69978666305542,
      "learning_rate": 0.00018803541517109357,
      "loss": 0.4062,
      "step": 3930
    },
    {
      "epoch": 0.2828427853553482,
      "grad_norm": 8.89496898651123,
      "learning_rate": 0.00018851399856424983,
      "loss": 0.4593,
      "step": 3940
    },
    {
      "epoch": 0.28356066044508255,
      "grad_norm": 3.671772003173828,
      "learning_rate": 0.0001889925819574061,
      "loss": 0.5274,
      "step": 3950
    },
    {
      "epoch": 0.28427853553481697,
      "grad_norm": 8.96165657043457,
      "learning_rate": 0.00018947116535056235,
      "loss": 0.7185,
      "step": 3960
    },
    {
      "epoch": 0.28499641062455133,
      "grad_norm": 20.563365936279297,
      "learning_rate": 0.0001899497487437186,
      "loss": 0.6894,
      "step": 3970
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 18.392757415771484,
      "learning_rate": 0.00019042833213687486,
      "loss": 0.6563,
      "step": 3980
    },
    {
      "epoch": 0.2864321608040201,
      "grad_norm": 25.522319793701172,
      "learning_rate": 0.00019090691553003112,
      "loss": 0.5701,
      "step": 3990
    },
    {
      "epoch": 0.2871500358937545,
      "grad_norm": 3.513577699661255,
      "learning_rate": 0.00019138549892318738,
      "loss": 0.3793,
      "step": 4000
    },
    {
      "epoch": 0.28786791098348885,
      "grad_norm": 10.548158645629883,
      "learning_rate": 0.00019186408231634363,
      "loss": 0.4542,
      "step": 4010
    },
    {
      "epoch": 0.28858578607322327,
      "grad_norm": 16.99068260192871,
      "learning_rate": 0.0001923426657094999,
      "loss": 0.5015,
      "step": 4020
    },
    {
      "epoch": 0.28930366116295764,
      "grad_norm": 19.260284423828125,
      "learning_rate": 0.00019282124910265615,
      "loss": 0.4511,
      "step": 4030
    },
    {
      "epoch": 0.29002153625269206,
      "grad_norm": 12.762491226196289,
      "learning_rate": 0.0001932998324958124,
      "loss": 0.8726,
      "step": 4040
    },
    {
      "epoch": 0.2907394113424264,
      "grad_norm": 7.884005069732666,
      "learning_rate": 0.00019377841588896866,
      "loss": 0.4742,
      "step": 4050
    },
    {
      "epoch": 0.2914572864321608,
      "grad_norm": 6.882601737976074,
      "learning_rate": 0.00019425699928212492,
      "loss": 0.5,
      "step": 4060
    },
    {
      "epoch": 0.2921751615218952,
      "grad_norm": 8.216170310974121,
      "learning_rate": 0.00019473558267528118,
      "loss": 0.3462,
      "step": 4070
    },
    {
      "epoch": 0.2928930366116296,
      "grad_norm": 3.567963123321533,
      "learning_rate": 0.00019521416606843744,
      "loss": 0.5323,
      "step": 4080
    },
    {
      "epoch": 0.29361091170136394,
      "grad_norm": 11.320813179016113,
      "learning_rate": 0.0001956927494615937,
      "loss": 0.6787,
      "step": 4090
    },
    {
      "epoch": 0.29432878679109836,
      "grad_norm": 9.663959503173828,
      "learning_rate": 0.00019617133285474995,
      "loss": 0.3647,
      "step": 4100
    },
    {
      "epoch": 0.29504666188083273,
      "grad_norm": 12.001446723937988,
      "learning_rate": 0.0001966499162479062,
      "loss": 0.5981,
      "step": 4110
    },
    {
      "epoch": 0.29576453697056715,
      "grad_norm": 15.260436058044434,
      "learning_rate": 0.00019712849964106247,
      "loss": 0.4666,
      "step": 4120
    },
    {
      "epoch": 0.2964824120603015,
      "grad_norm": 4.417855262756348,
      "learning_rate": 0.00019760708303421872,
      "loss": 0.7709,
      "step": 4130
    },
    {
      "epoch": 0.2972002871500359,
      "grad_norm": 8.791413307189941,
      "learning_rate": 0.00019808566642737498,
      "loss": 0.6106,
      "step": 4140
    },
    {
      "epoch": 0.2979181622397703,
      "grad_norm": 19.40513038635254,
      "learning_rate": 0.00019856424982053124,
      "loss": 0.6537,
      "step": 4150
    },
    {
      "epoch": 0.29863603732950467,
      "grad_norm": 11.812305450439453,
      "learning_rate": 0.0001990428332136875,
      "loss": 0.4014,
      "step": 4160
    },
    {
      "epoch": 0.29935391241923903,
      "grad_norm": 8.936590194702148,
      "learning_rate": 0.00019952141660684375,
      "loss": 0.5279,
      "step": 4170
    },
    {
      "epoch": 0.30007178750897345,
      "grad_norm": 10.349360466003418,
      "learning_rate": 0.0002,
      "loss": 0.491,
      "step": 4180
    },
    {
      "epoch": 0.3007896625987078,
      "grad_norm": 15.231633186340332,
      "learning_rate": 0.0001999468240674271,
      "loss": 0.433,
      "step": 4190
    },
    {
      "epoch": 0.3015075376884422,
      "grad_norm": 8.817174911499023,
      "learning_rate": 0.0001998936481348542,
      "loss": 0.526,
      "step": 4200
    },
    {
      "epoch": 0.3022254127781766,
      "grad_norm": 3.7432806491851807,
      "learning_rate": 0.00019984047220228125,
      "loss": 0.4834,
      "step": 4210
    },
    {
      "epoch": 0.30294328786791097,
      "grad_norm": 11.71324348449707,
      "learning_rate": 0.00019978729626970833,
      "loss": 0.3154,
      "step": 4220
    },
    {
      "epoch": 0.3036611629576454,
      "grad_norm": 1.6932083368301392,
      "learning_rate": 0.00019973412033713543,
      "loss": 0.3894,
      "step": 4230
    },
    {
      "epoch": 0.30437903804737976,
      "grad_norm": 8.231146812438965,
      "learning_rate": 0.00019968094440456251,
      "loss": 0.3465,
      "step": 4240
    },
    {
      "epoch": 0.3050969131371141,
      "grad_norm": 13.177322387695312,
      "learning_rate": 0.0001996277684719896,
      "loss": 0.4788,
      "step": 4250
    },
    {
      "epoch": 0.30581478822684854,
      "grad_norm": 2.885084867477417,
      "learning_rate": 0.00019957459253941667,
      "loss": 0.4858,
      "step": 4260
    },
    {
      "epoch": 0.3065326633165829,
      "grad_norm": 8.836213111877441,
      "learning_rate": 0.00019952141660684375,
      "loss": 0.6456,
      "step": 4270
    },
    {
      "epoch": 0.3072505384063173,
      "grad_norm": 11.01154899597168,
      "learning_rate": 0.00019946824067427083,
      "loss": 0.2946,
      "step": 4280
    },
    {
      "epoch": 0.3079684134960517,
      "grad_norm": 9.641129493713379,
      "learning_rate": 0.0001994150647416979,
      "loss": 0.6268,
      "step": 4290
    },
    {
      "epoch": 0.30868628858578606,
      "grad_norm": 8.154520988464355,
      "learning_rate": 0.00019936188880912502,
      "loss": 0.5708,
      "step": 4300
    },
    {
      "epoch": 0.3094041636755205,
      "grad_norm": 25.057580947875977,
      "learning_rate": 0.0001993087128765521,
      "loss": 0.3294,
      "step": 4310
    },
    {
      "epoch": 0.31012203876525485,
      "grad_norm": 15.723724365234375,
      "learning_rate": 0.00019925553694397915,
      "loss": 0.4448,
      "step": 4320
    },
    {
      "epoch": 0.3108399138549892,
      "grad_norm": 11.27841567993164,
      "learning_rate": 0.00019920236101140626,
      "loss": 0.5049,
      "step": 4330
    },
    {
      "epoch": 0.31155778894472363,
      "grad_norm": 9.43625259399414,
      "learning_rate": 0.00019914918507883334,
      "loss": 0.444,
      "step": 4340
    },
    {
      "epoch": 0.312275664034458,
      "grad_norm": 1.8812917470932007,
      "learning_rate": 0.00019909600914626042,
      "loss": 0.2587,
      "step": 4350
    },
    {
      "epoch": 0.31299353912419237,
      "grad_norm": 9.11155891418457,
      "learning_rate": 0.0001990428332136875,
      "loss": 0.7205,
      "step": 4360
    },
    {
      "epoch": 0.3137114142139268,
      "grad_norm": 6.588705539703369,
      "learning_rate": 0.00019898965728111457,
      "loss": 0.5135,
      "step": 4370
    },
    {
      "epoch": 0.31442928930366115,
      "grad_norm": 9.708564758300781,
      "learning_rate": 0.00019893648134854165,
      "loss": 0.6049,
      "step": 4380
    },
    {
      "epoch": 0.3151471643933956,
      "grad_norm": 13.31852912902832,
      "learning_rate": 0.00019888330541596873,
      "loss": 0.5223,
      "step": 4390
    },
    {
      "epoch": 0.31586503948312994,
      "grad_norm": 5.972749710083008,
      "learning_rate": 0.00019883012948339584,
      "loss": 0.517,
      "step": 4400
    },
    {
      "epoch": 0.3165829145728643,
      "grad_norm": 9.92796802520752,
      "learning_rate": 0.00019877695355082292,
      "loss": 0.4914,
      "step": 4410
    },
    {
      "epoch": 0.3173007896625987,
      "grad_norm": 15.248334884643555,
      "learning_rate": 0.00019872377761824997,
      "loss": 0.3907,
      "step": 4420
    },
    {
      "epoch": 0.3180186647523331,
      "grad_norm": 13.498915672302246,
      "learning_rate": 0.00019867060168567708,
      "loss": 0.3457,
      "step": 4430
    },
    {
      "epoch": 0.31873653984206746,
      "grad_norm": 2.3066651821136475,
      "learning_rate": 0.00019861742575310416,
      "loss": 0.3718,
      "step": 4440
    },
    {
      "epoch": 0.3194544149318019,
      "grad_norm": 5.880839824676514,
      "learning_rate": 0.00019856424982053124,
      "loss": 0.4727,
      "step": 4450
    },
    {
      "epoch": 0.32017229002153624,
      "grad_norm": 9.876163482666016,
      "learning_rate": 0.00019851107388795832,
      "loss": 0.5365,
      "step": 4460
    },
    {
      "epoch": 0.32089016511127066,
      "grad_norm": 15.622401237487793,
      "learning_rate": 0.0001984578979553854,
      "loss": 0.2588,
      "step": 4470
    },
    {
      "epoch": 0.32160804020100503,
      "grad_norm": 13.399739265441895,
      "learning_rate": 0.00019840472202281248,
      "loss": 0.618,
      "step": 4480
    },
    {
      "epoch": 0.3223259152907394,
      "grad_norm": 7.869649410247803,
      "learning_rate": 0.00019835154609023956,
      "loss": 0.3703,
      "step": 4490
    },
    {
      "epoch": 0.3230437903804738,
      "grad_norm": 6.75095796585083,
      "learning_rate": 0.00019829837015766666,
      "loss": 0.6275,
      "step": 4500
    },
    {
      "epoch": 0.3237616654702082,
      "grad_norm": 8.558040618896484,
      "learning_rate": 0.00019824519422509374,
      "loss": 0.4831,
      "step": 4510
    },
    {
      "epoch": 0.32447954055994255,
      "grad_norm": 4.255273342132568,
      "learning_rate": 0.00019819201829252082,
      "loss": 0.6005,
      "step": 4520
    },
    {
      "epoch": 0.32519741564967697,
      "grad_norm": 6.950681686401367,
      "learning_rate": 0.0001981388423599479,
      "loss": 0.3219,
      "step": 4530
    },
    {
      "epoch": 0.32591529073941133,
      "grad_norm": 23.886686325073242,
      "learning_rate": 0.00019808566642737498,
      "loss": 0.4578,
      "step": 4540
    },
    {
      "epoch": 0.32663316582914576,
      "grad_norm": 11.119402885437012,
      "learning_rate": 0.00019803249049480206,
      "loss": 0.6407,
      "step": 4550
    },
    {
      "epoch": 0.3273510409188801,
      "grad_norm": 9.753374099731445,
      "learning_rate": 0.00019797931456222914,
      "loss": 0.3479,
      "step": 4560
    },
    {
      "epoch": 0.3280689160086145,
      "grad_norm": 12.628405570983887,
      "learning_rate": 0.00019792613862965625,
      "loss": 0.5699,
      "step": 4570
    },
    {
      "epoch": 0.3287867910983489,
      "grad_norm": 5.51292610168457,
      "learning_rate": 0.0001978729626970833,
      "loss": 0.5802,
      "step": 4580
    },
    {
      "epoch": 0.3295046661880833,
      "grad_norm": 6.5605010986328125,
      "learning_rate": 0.00019781978676451038,
      "loss": 0.5288,
      "step": 4590
    },
    {
      "epoch": 0.33022254127781764,
      "grad_norm": 0.93780916929245,
      "learning_rate": 0.00019776661083193748,
      "loss": 0.3001,
      "step": 4600
    },
    {
      "epoch": 0.33094041636755206,
      "grad_norm": 7.287946701049805,
      "learning_rate": 0.00019771343489936456,
      "loss": 0.645,
      "step": 4610
    },
    {
      "epoch": 0.3316582914572864,
      "grad_norm": 12.339908599853516,
      "learning_rate": 0.00019766025896679164,
      "loss": 0.5312,
      "step": 4620
    },
    {
      "epoch": 0.33237616654702085,
      "grad_norm": 5.90490198135376,
      "learning_rate": 0.00019760708303421872,
      "loss": 0.3485,
      "step": 4630
    },
    {
      "epoch": 0.3330940416367552,
      "grad_norm": 2.8920788764953613,
      "learning_rate": 0.0001975539071016458,
      "loss": 0.467,
      "step": 4640
    },
    {
      "epoch": 0.3338119167264896,
      "grad_norm": 11.631108283996582,
      "learning_rate": 0.00019750073116907288,
      "loss": 0.5198,
      "step": 4650
    },
    {
      "epoch": 0.334529791816224,
      "grad_norm": 21.798887252807617,
      "learning_rate": 0.00019744755523649996,
      "loss": 0.5797,
      "step": 4660
    },
    {
      "epoch": 0.33524766690595836,
      "grad_norm": 4.397892475128174,
      "learning_rate": 0.00019739437930392707,
      "loss": 0.4747,
      "step": 4670
    },
    {
      "epoch": 0.33596554199569273,
      "grad_norm": 8.400347709655762,
      "learning_rate": 0.00019734120337135412,
      "loss": 0.5269,
      "step": 4680
    },
    {
      "epoch": 0.33668341708542715,
      "grad_norm": 12.330069541931152,
      "learning_rate": 0.0001972880274387812,
      "loss": 0.7241,
      "step": 4690
    },
    {
      "epoch": 0.3374012921751615,
      "grad_norm": 7.536892414093018,
      "learning_rate": 0.0001972348515062083,
      "loss": 0.3498,
      "step": 4700
    },
    {
      "epoch": 0.3381191672648959,
      "grad_norm": 5.63746976852417,
      "learning_rate": 0.00019718167557363539,
      "loss": 0.3424,
      "step": 4710
    },
    {
      "epoch": 0.3388370423546303,
      "grad_norm": 12.748370170593262,
      "learning_rate": 0.00019712849964106247,
      "loss": 0.5958,
      "step": 4720
    },
    {
      "epoch": 0.33955491744436467,
      "grad_norm": 4.763253688812256,
      "learning_rate": 0.00019707532370848954,
      "loss": 0.2449,
      "step": 4730
    },
    {
      "epoch": 0.3402727925340991,
      "grad_norm": 19.1805362701416,
      "learning_rate": 0.00019702214777591662,
      "loss": 0.5567,
      "step": 4740
    },
    {
      "epoch": 0.34099066762383345,
      "grad_norm": 3.5447518825531006,
      "learning_rate": 0.0001969689718433437,
      "loss": 0.2443,
      "step": 4750
    },
    {
      "epoch": 0.3417085427135678,
      "grad_norm": 14.088926315307617,
      "learning_rate": 0.00019691579591077078,
      "loss": 0.3841,
      "step": 4760
    },
    {
      "epoch": 0.34242641780330224,
      "grad_norm": 22.563657760620117,
      "learning_rate": 0.0001968626199781979,
      "loss": 0.4225,
      "step": 4770
    },
    {
      "epoch": 0.3431442928930366,
      "grad_norm": 10.883078575134277,
      "learning_rate": 0.00019680944404562497,
      "loss": 0.4275,
      "step": 4780
    },
    {
      "epoch": 0.34386216798277097,
      "grad_norm": 21.19317054748535,
      "learning_rate": 0.00019675626811305202,
      "loss": 0.4818,
      "step": 4790
    },
    {
      "epoch": 0.3445800430725054,
      "grad_norm": 15.403902053833008,
      "learning_rate": 0.00019670309218047913,
      "loss": 0.5448,
      "step": 4800
    },
    {
      "epoch": 0.34529791816223976,
      "grad_norm": 15.030860900878906,
      "learning_rate": 0.0001966499162479062,
      "loss": 0.4471,
      "step": 4810
    },
    {
      "epoch": 0.3460157932519742,
      "grad_norm": 15.02364730834961,
      "learning_rate": 0.0001965967403153333,
      "loss": 0.3075,
      "step": 4820
    },
    {
      "epoch": 0.34673366834170855,
      "grad_norm": 11.558250427246094,
      "learning_rate": 0.0001965435643827604,
      "loss": 0.44,
      "step": 4830
    },
    {
      "epoch": 0.3474515434314429,
      "grad_norm": 10.083312034606934,
      "learning_rate": 0.00019649038845018745,
      "loss": 0.451,
      "step": 4840
    },
    {
      "epoch": 0.34816941852117733,
      "grad_norm": 4.943320274353027,
      "learning_rate": 0.00019643721251761453,
      "loss": 0.326,
      "step": 4850
    },
    {
      "epoch": 0.3488872936109117,
      "grad_norm": 8.093174934387207,
      "learning_rate": 0.0001963840365850416,
      "loss": 0.5513,
      "step": 4860
    },
    {
      "epoch": 0.34960516870064606,
      "grad_norm": 1.1377662420272827,
      "learning_rate": 0.0001963308606524687,
      "loss": 0.1973,
      "step": 4870
    },
    {
      "epoch": 0.3503230437903805,
      "grad_norm": 10.857267379760742,
      "learning_rate": 0.0001962776847198958,
      "loss": 0.4409,
      "step": 4880
    },
    {
      "epoch": 0.35104091888011485,
      "grad_norm": 10.852984428405762,
      "learning_rate": 0.00019622450878732287,
      "loss": 0.3151,
      "step": 4890
    },
    {
      "epoch": 0.35175879396984927,
      "grad_norm": 8.47148323059082,
      "learning_rate": 0.00019617133285474995,
      "loss": 0.4907,
      "step": 4900
    },
    {
      "epoch": 0.35247666905958364,
      "grad_norm": 7.478244781494141,
      "learning_rate": 0.00019611815692217703,
      "loss": 0.4335,
      "step": 4910
    },
    {
      "epoch": 0.353194544149318,
      "grad_norm": 7.338900089263916,
      "learning_rate": 0.0001960649809896041,
      "loss": 0.7454,
      "step": 4920
    },
    {
      "epoch": 0.3539124192390524,
      "grad_norm": 16.597192764282227,
      "learning_rate": 0.00019601180505703122,
      "loss": 0.3693,
      "step": 4930
    },
    {
      "epoch": 0.3546302943287868,
      "grad_norm": 19.557016372680664,
      "learning_rate": 0.00019595862912445827,
      "loss": 0.397,
      "step": 4940
    },
    {
      "epoch": 0.35534816941852115,
      "grad_norm": 5.940458297729492,
      "learning_rate": 0.00019590545319188535,
      "loss": 0.5849,
      "step": 4950
    },
    {
      "epoch": 0.3560660445082556,
      "grad_norm": 6.183773040771484,
      "learning_rate": 0.00019585227725931245,
      "loss": 0.5309,
      "step": 4960
    },
    {
      "epoch": 0.35678391959798994,
      "grad_norm": 5.606039524078369,
      "learning_rate": 0.00019579910132673953,
      "loss": 0.6263,
      "step": 4970
    },
    {
      "epoch": 0.35750179468772436,
      "grad_norm": 11.279147148132324,
      "learning_rate": 0.0001957459253941666,
      "loss": 0.4145,
      "step": 4980
    },
    {
      "epoch": 0.3582196697774587,
      "grad_norm": 5.077211856842041,
      "learning_rate": 0.0001956927494615937,
      "loss": 0.4006,
      "step": 4990
    },
    {
      "epoch": 0.3589375448671931,
      "grad_norm": 6.7522501945495605,
      "learning_rate": 0.00019563957352902077,
      "loss": 0.4635,
      "step": 5000
    },
    {
      "epoch": 0.3596554199569275,
      "grad_norm": 4.1663737297058105,
      "learning_rate": 0.00019558639759644785,
      "loss": 0.3869,
      "step": 5010
    },
    {
      "epoch": 0.3603732950466619,
      "grad_norm": 11.639829635620117,
      "learning_rate": 0.00019553322166387493,
      "loss": 0.5771,
      "step": 5020
    },
    {
      "epoch": 0.36109117013639624,
      "grad_norm": 4.6015119552612305,
      "learning_rate": 0.00019548004573130204,
      "loss": 0.2189,
      "step": 5030
    },
    {
      "epoch": 0.36180904522613067,
      "grad_norm": 8.544771194458008,
      "learning_rate": 0.00019542686979872912,
      "loss": 0.42,
      "step": 5040
    },
    {
      "epoch": 0.36252692031586503,
      "grad_norm": 12.798040390014648,
      "learning_rate": 0.00019537369386615617,
      "loss": 0.3761,
      "step": 5050
    },
    {
      "epoch": 0.36324479540559945,
      "grad_norm": 16.509723663330078,
      "learning_rate": 0.00019532051793358328,
      "loss": 0.6062,
      "step": 5060
    },
    {
      "epoch": 0.3639626704953338,
      "grad_norm": 9.2608003616333,
      "learning_rate": 0.00019526734200101036,
      "loss": 0.4694,
      "step": 5070
    },
    {
      "epoch": 0.3646805455850682,
      "grad_norm": 8.28105354309082,
      "learning_rate": 0.00019521416606843744,
      "loss": 0.2348,
      "step": 5080
    },
    {
      "epoch": 0.3653984206748026,
      "grad_norm": 13.462971687316895,
      "learning_rate": 0.00019516099013586451,
      "loss": 0.7954,
      "step": 5090
    },
    {
      "epoch": 0.36611629576453697,
      "grad_norm": 12.313349723815918,
      "learning_rate": 0.0001951078142032916,
      "loss": 0.5315,
      "step": 5100
    },
    {
      "epoch": 0.36683417085427134,
      "grad_norm": 8.542134284973145,
      "learning_rate": 0.00019505463827071867,
      "loss": 0.5542,
      "step": 5110
    },
    {
      "epoch": 0.36755204594400576,
      "grad_norm": 12.923853874206543,
      "learning_rate": 0.00019500146233814575,
      "loss": 0.6191,
      "step": 5120
    },
    {
      "epoch": 0.3682699210337401,
      "grad_norm": 20.287561416625977,
      "learning_rate": 0.00019494828640557286,
      "loss": 0.6066,
      "step": 5130
    },
    {
      "epoch": 0.36898779612347454,
      "grad_norm": 14.147100448608398,
      "learning_rate": 0.00019489511047299994,
      "loss": 0.4863,
      "step": 5140
    },
    {
      "epoch": 0.3697056712132089,
      "grad_norm": 5.0502519607543945,
      "learning_rate": 0.00019484193454042702,
      "loss": 0.2242,
      "step": 5150
    },
    {
      "epoch": 0.3704235463029433,
      "grad_norm": 3.1671619415283203,
      "learning_rate": 0.0001947887586078541,
      "loss": 0.321,
      "step": 5160
    },
    {
      "epoch": 0.3711414213926777,
      "grad_norm": 10.874890327453613,
      "learning_rate": 0.00019473558267528118,
      "loss": 0.5395,
      "step": 5170
    },
    {
      "epoch": 0.37185929648241206,
      "grad_norm": 8.386327743530273,
      "learning_rate": 0.00019468240674270826,
      "loss": 0.4615,
      "step": 5180
    },
    {
      "epoch": 0.3725771715721464,
      "grad_norm": 7.35896635055542,
      "learning_rate": 0.00019462923081013534,
      "loss": 0.4152,
      "step": 5190
    },
    {
      "epoch": 0.37329504666188085,
      "grad_norm": 2.0454447269439697,
      "learning_rate": 0.00019457605487756244,
      "loss": 0.1986,
      "step": 5200
    },
    {
      "epoch": 0.3740129217516152,
      "grad_norm": 8.753235816955566,
      "learning_rate": 0.0001945228789449895,
      "loss": 0.2013,
      "step": 5210
    },
    {
      "epoch": 0.3747307968413496,
      "grad_norm": 17.900259017944336,
      "learning_rate": 0.00019446970301241658,
      "loss": 0.6464,
      "step": 5220
    },
    {
      "epoch": 0.375448671931084,
      "grad_norm": 12.577472686767578,
      "learning_rate": 0.00019441652707984368,
      "loss": 0.6416,
      "step": 5230
    },
    {
      "epoch": 0.37616654702081836,
      "grad_norm": 2.650564432144165,
      "learning_rate": 0.00019436335114727076,
      "loss": 0.2329,
      "step": 5240
    },
    {
      "epoch": 0.3768844221105528,
      "grad_norm": 10.979334831237793,
      "learning_rate": 0.00019431017521469784,
      "loss": 0.5854,
      "step": 5250
    },
    {
      "epoch": 0.37760229720028715,
      "grad_norm": 4.514771461486816,
      "learning_rate": 0.00019425699928212492,
      "loss": 0.4185,
      "step": 5260
    },
    {
      "epoch": 0.3783201722900215,
      "grad_norm": 7.794284343719482,
      "learning_rate": 0.000194203823349552,
      "loss": 0.2701,
      "step": 5270
    },
    {
      "epoch": 0.37903804737975594,
      "grad_norm": 8.345793724060059,
      "learning_rate": 0.00019415064741697908,
      "loss": 0.3064,
      "step": 5280
    },
    {
      "epoch": 0.3797559224694903,
      "grad_norm": 3.9765987396240234,
      "learning_rate": 0.00019409747148440616,
      "loss": 0.4509,
      "step": 5290
    },
    {
      "epoch": 0.38047379755922467,
      "grad_norm": 7.38222074508667,
      "learning_rate": 0.00019404429555183327,
      "loss": 0.5488,
      "step": 5300
    },
    {
      "epoch": 0.3811916726489591,
      "grad_norm": 7.677720546722412,
      "learning_rate": 0.00019399111961926032,
      "loss": 0.3234,
      "step": 5310
    },
    {
      "epoch": 0.38190954773869346,
      "grad_norm": 12.818082809448242,
      "learning_rate": 0.0001939379436866874,
      "loss": 0.5429,
      "step": 5320
    },
    {
      "epoch": 0.3826274228284279,
      "grad_norm": 13.303092002868652,
      "learning_rate": 0.0001938847677541145,
      "loss": 0.6036,
      "step": 5330
    },
    {
      "epoch": 0.38334529791816224,
      "grad_norm": 5.107619762420654,
      "learning_rate": 0.00019383159182154158,
      "loss": 0.1822,
      "step": 5340
    },
    {
      "epoch": 0.3840631730078966,
      "grad_norm": 11.150349617004395,
      "learning_rate": 0.00019377841588896866,
      "loss": 0.5202,
      "step": 5350
    },
    {
      "epoch": 0.38478104809763103,
      "grad_norm": 6.818055152893066,
      "learning_rate": 0.00019372523995639574,
      "loss": 0.402,
      "step": 5360
    },
    {
      "epoch": 0.3854989231873654,
      "grad_norm": 8.858460426330566,
      "learning_rate": 0.00019367206402382282,
      "loss": 0.4618,
      "step": 5370
    },
    {
      "epoch": 0.38621679827709976,
      "grad_norm": 12.6859712600708,
      "learning_rate": 0.0001936188880912499,
      "loss": 0.6189,
      "step": 5380
    },
    {
      "epoch": 0.3869346733668342,
      "grad_norm": 13.560842514038086,
      "learning_rate": 0.00019356571215867698,
      "loss": 0.3607,
      "step": 5390
    },
    {
      "epoch": 0.38765254845656855,
      "grad_norm": 9.796453475952148,
      "learning_rate": 0.0001935125362261041,
      "loss": 0.3119,
      "step": 5400
    },
    {
      "epoch": 0.38837042354630297,
      "grad_norm": 0.4368685781955719,
      "learning_rate": 0.00019345936029353117,
      "loss": 0.2479,
      "step": 5410
    },
    {
      "epoch": 0.38908829863603733,
      "grad_norm": 0.3289455771446228,
      "learning_rate": 0.00019340618436095822,
      "loss": 0.3899,
      "step": 5420
    },
    {
      "epoch": 0.3898061737257717,
      "grad_norm": 6.108245849609375,
      "learning_rate": 0.00019335300842838533,
      "loss": 0.1871,
      "step": 5430
    },
    {
      "epoch": 0.3905240488155061,
      "grad_norm": 18.913681030273438,
      "learning_rate": 0.0001932998324958124,
      "loss": 0.4912,
      "step": 5440
    },
    {
      "epoch": 0.3912419239052405,
      "grad_norm": 7.049358367919922,
      "learning_rate": 0.00019324665656323949,
      "loss": 0.4857,
      "step": 5450
    },
    {
      "epoch": 0.39195979899497485,
      "grad_norm": 3.6453819274902344,
      "learning_rate": 0.0001931934806306666,
      "loss": 0.4473,
      "step": 5460
    },
    {
      "epoch": 0.39267767408470927,
      "grad_norm": 18.030048370361328,
      "learning_rate": 0.00019314030469809364,
      "loss": 0.6971,
      "step": 5470
    },
    {
      "epoch": 0.39339554917444364,
      "grad_norm": 3.3645641803741455,
      "learning_rate": 0.00019308712876552072,
      "loss": 0.344,
      "step": 5480
    },
    {
      "epoch": 0.39411342426417806,
      "grad_norm": 0.4566713571548462,
      "learning_rate": 0.0001930339528329478,
      "loss": 0.706,
      "step": 5490
    },
    {
      "epoch": 0.3948312993539124,
      "grad_norm": 10.644160270690918,
      "learning_rate": 0.0001929807769003749,
      "loss": 0.2938,
      "step": 5500
    },
    {
      "epoch": 0.3955491744436468,
      "grad_norm": 15.868194580078125,
      "learning_rate": 0.000192927600967802,
      "loss": 0.3967,
      "step": 5510
    },
    {
      "epoch": 0.3962670495333812,
      "grad_norm": 12.511391639709473,
      "learning_rate": 0.00019287442503522907,
      "loss": 0.2871,
      "step": 5520
    },
    {
      "epoch": 0.3969849246231156,
      "grad_norm": 8.974679946899414,
      "learning_rate": 0.00019282124910265615,
      "loss": 0.287,
      "step": 5530
    },
    {
      "epoch": 0.39770279971284994,
      "grad_norm": 10.916380882263184,
      "learning_rate": 0.00019276807317008323,
      "loss": 0.3222,
      "step": 5540
    },
    {
      "epoch": 0.39842067480258436,
      "grad_norm": 20.757667541503906,
      "learning_rate": 0.0001927148972375103,
      "loss": 0.5902,
      "step": 5550
    },
    {
      "epoch": 0.39913854989231873,
      "grad_norm": 0.08000332117080688,
      "learning_rate": 0.00019266172130493741,
      "loss": 0.3604,
      "step": 5560
    },
    {
      "epoch": 0.39985642498205315,
      "grad_norm": 5.108635425567627,
      "learning_rate": 0.00019260854537236447,
      "loss": 0.179,
      "step": 5570
    },
    {
      "epoch": 0.4005743000717875,
      "grad_norm": 16.281661987304688,
      "learning_rate": 0.00019255536943979155,
      "loss": 0.6113,
      "step": 5580
    },
    {
      "epoch": 0.4012921751615219,
      "grad_norm": 8.11465835571289,
      "learning_rate": 0.00019250219350721863,
      "loss": 0.7441,
      "step": 5590
    },
    {
      "epoch": 0.4020100502512563,
      "grad_norm": 6.220599174499512,
      "learning_rate": 0.00019244901757464573,
      "loss": 0.5548,
      "step": 5600
    },
    {
      "epoch": 0.40272792534099067,
      "grad_norm": 16.756446838378906,
      "learning_rate": 0.0001923958416420728,
      "loss": 0.5356,
      "step": 5610
    },
    {
      "epoch": 0.40344580043072503,
      "grad_norm": 2.5775113105773926,
      "learning_rate": 0.0001923426657094999,
      "loss": 0.3704,
      "step": 5620
    },
    {
      "epoch": 0.40416367552045945,
      "grad_norm": 9.347640037536621,
      "learning_rate": 0.00019228948977692697,
      "loss": 0.6637,
      "step": 5630
    },
    {
      "epoch": 0.4048815506101938,
      "grad_norm": 6.568061828613281,
      "learning_rate": 0.00019223631384435405,
      "loss": 0.4897,
      "step": 5640
    },
    {
      "epoch": 0.40559942569992824,
      "grad_norm": 11.5574951171875,
      "learning_rate": 0.00019218313791178113,
      "loss": 0.4035,
      "step": 5650
    },
    {
      "epoch": 0.4063173007896626,
      "grad_norm": 5.570053577423096,
      "learning_rate": 0.00019212996197920824,
      "loss": 0.2762,
      "step": 5660
    },
    {
      "epoch": 0.40703517587939697,
      "grad_norm": 5.441007614135742,
      "learning_rate": 0.00019207678604663532,
      "loss": 0.5357,
      "step": 5670
    },
    {
      "epoch": 0.4077530509691314,
      "grad_norm": 3.05793833732605,
      "learning_rate": 0.00019202361011406237,
      "loss": 0.3223,
      "step": 5680
    },
    {
      "epoch": 0.40847092605886576,
      "grad_norm": 9.944670677185059,
      "learning_rate": 0.00019197043418148947,
      "loss": 0.6236,
      "step": 5690
    },
    {
      "epoch": 0.4091888011486001,
      "grad_norm": 7.713029861450195,
      "learning_rate": 0.00019191725824891655,
      "loss": 0.601,
      "step": 5700
    },
    {
      "epoch": 0.40990667623833454,
      "grad_norm": 5.449270725250244,
      "learning_rate": 0.00019186408231634363,
      "loss": 0.2632,
      "step": 5710
    },
    {
      "epoch": 0.4106245513280689,
      "grad_norm": 5.400418758392334,
      "learning_rate": 0.0001918109063837707,
      "loss": 0.4408,
      "step": 5720
    },
    {
      "epoch": 0.4113424264178033,
      "grad_norm": 18.909055709838867,
      "learning_rate": 0.0001917577304511978,
      "loss": 0.4743,
      "step": 5730
    },
    {
      "epoch": 0.4120603015075377,
      "grad_norm": 2.8091673851013184,
      "learning_rate": 0.00019170455451862487,
      "loss": 0.7182,
      "step": 5740
    },
    {
      "epoch": 0.41277817659727206,
      "grad_norm": 20.4910831451416,
      "learning_rate": 0.00019165137858605195,
      "loss": 0.339,
      "step": 5750
    },
    {
      "epoch": 0.4134960516870065,
      "grad_norm": 5.457846164703369,
      "learning_rate": 0.00019159820265347906,
      "loss": 0.3596,
      "step": 5760
    },
    {
      "epoch": 0.41421392677674085,
      "grad_norm": 6.282767295837402,
      "learning_rate": 0.00019154502672090614,
      "loss": 0.5098,
      "step": 5770
    },
    {
      "epoch": 0.4149318018664752,
      "grad_norm": 0.5734180808067322,
      "learning_rate": 0.00019149185078833322,
      "loss": 0.2693,
      "step": 5780
    },
    {
      "epoch": 0.41564967695620963,
      "grad_norm": 6.3380208015441895,
      "learning_rate": 0.0001914386748557603,
      "loss": 0.3557,
      "step": 5790
    },
    {
      "epoch": 0.416367552045944,
      "grad_norm": 2.1359846591949463,
      "learning_rate": 0.00019138549892318738,
      "loss": 0.3991,
      "step": 5800
    },
    {
      "epoch": 0.41708542713567837,
      "grad_norm": 6.145281791687012,
      "learning_rate": 0.00019133232299061446,
      "loss": 0.4695,
      "step": 5810
    },
    {
      "epoch": 0.4178033022254128,
      "grad_norm": 1.0400123596191406,
      "learning_rate": 0.00019127914705804153,
      "loss": 0.2105,
      "step": 5820
    },
    {
      "epoch": 0.41852117731514715,
      "grad_norm": 4.196476459503174,
      "learning_rate": 0.00019122597112546864,
      "loss": 0.4844,
      "step": 5830
    },
    {
      "epoch": 0.4192390524048816,
      "grad_norm": 14.732769966125488,
      "learning_rate": 0.0001911727951928957,
      "loss": 0.3686,
      "step": 5840
    },
    {
      "epoch": 0.41995692749461594,
      "grad_norm": 9.705808639526367,
      "learning_rate": 0.00019111961926032277,
      "loss": 0.3445,
      "step": 5850
    },
    {
      "epoch": 0.4206748025843503,
      "grad_norm": 8.968667030334473,
      "learning_rate": 0.00019106644332774988,
      "loss": 0.3547,
      "step": 5860
    },
    {
      "epoch": 0.4213926776740847,
      "grad_norm": 11.756317138671875,
      "learning_rate": 0.00019101326739517696,
      "loss": 0.1988,
      "step": 5870
    },
    {
      "epoch": 0.4221105527638191,
      "grad_norm": 3.2433321475982666,
      "learning_rate": 0.00019096009146260404,
      "loss": 0.4383,
      "step": 5880
    },
    {
      "epoch": 0.42282842785355346,
      "grad_norm": 10.101899147033691,
      "learning_rate": 0.00019090691553003112,
      "loss": 0.486,
      "step": 5890
    },
    {
      "epoch": 0.4235463029432879,
      "grad_norm": 7.738649368286133,
      "learning_rate": 0.0001908537395974582,
      "loss": 0.3654,
      "step": 5900
    },
    {
      "epoch": 0.42426417803302224,
      "grad_norm": 4.316162109375,
      "learning_rate": 0.00019080056366488528,
      "loss": 0.1999,
      "step": 5910
    },
    {
      "epoch": 0.42498205312275666,
      "grad_norm": 18.913829803466797,
      "learning_rate": 0.00019074738773231236,
      "loss": 0.6624,
      "step": 5920
    },
    {
      "epoch": 0.42569992821249103,
      "grad_norm": 12.339452743530273,
      "learning_rate": 0.00019069421179973946,
      "loss": 0.4171,
      "step": 5930
    },
    {
      "epoch": 0.4264178033022254,
      "grad_norm": 1.5401605367660522,
      "learning_rate": 0.00019064103586716652,
      "loss": 0.3809,
      "step": 5940
    },
    {
      "epoch": 0.4271356783919598,
      "grad_norm": 12.726974487304688,
      "learning_rate": 0.0001905878599345936,
      "loss": 0.2511,
      "step": 5950
    },
    {
      "epoch": 0.4278535534816942,
      "grad_norm": 1.6857913732528687,
      "learning_rate": 0.0001905346840020207,
      "loss": 0.2439,
      "step": 5960
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.9145579934120178,
      "learning_rate": 0.00019048150806944778,
      "loss": 0.3648,
      "step": 5970
    },
    {
      "epoch": 0.42928930366116297,
      "grad_norm": 0.48263072967529297,
      "learning_rate": 0.00019042833213687486,
      "loss": 0.3856,
      "step": 5980
    },
    {
      "epoch": 0.43000717875089733,
      "grad_norm": 7.279073238372803,
      "learning_rate": 0.00019037515620430194,
      "loss": 0.3278,
      "step": 5990
    },
    {
      "epoch": 0.43072505384063176,
      "grad_norm": 12.230714797973633,
      "learning_rate": 0.00019032198027172902,
      "loss": 0.5808,
      "step": 6000
    },
    {
      "epoch": 0.4314429289303661,
      "grad_norm": 18.369977951049805,
      "learning_rate": 0.0001902688043391561,
      "loss": 0.4693,
      "step": 6010
    },
    {
      "epoch": 0.4321608040201005,
      "grad_norm": 10.01547622680664,
      "learning_rate": 0.00019021562840658318,
      "loss": 0.3026,
      "step": 6020
    },
    {
      "epoch": 0.4328786791098349,
      "grad_norm": 1.9660993814468384,
      "learning_rate": 0.00019016245247401029,
      "loss": 0.4154,
      "step": 6030
    },
    {
      "epoch": 0.4335965541995693,
      "grad_norm": 15.829273223876953,
      "learning_rate": 0.00019010927654143737,
      "loss": 0.2193,
      "step": 6040
    },
    {
      "epoch": 0.43431442928930364,
      "grad_norm": 10.128904342651367,
      "learning_rate": 0.00019005610060886442,
      "loss": 0.4371,
      "step": 6050
    },
    {
      "epoch": 0.43503230437903806,
      "grad_norm": 8.871116638183594,
      "learning_rate": 0.00019000292467629152,
      "loss": 0.2934,
      "step": 6060
    },
    {
      "epoch": 0.4357501794687724,
      "grad_norm": 12.289554595947266,
      "learning_rate": 0.0001899497487437186,
      "loss": 0.3466,
      "step": 6070
    },
    {
      "epoch": 0.43646805455850685,
      "grad_norm": 12.905600547790527,
      "learning_rate": 0.00018989657281114568,
      "loss": 0.3259,
      "step": 6080
    },
    {
      "epoch": 0.4371859296482412,
      "grad_norm": 4.699131965637207,
      "learning_rate": 0.0001898433968785728,
      "loss": 0.3896,
      "step": 6090
    },
    {
      "epoch": 0.4379038047379756,
      "grad_norm": 12.362549781799316,
      "learning_rate": 0.00018979022094599984,
      "loss": 0.2261,
      "step": 6100
    },
    {
      "epoch": 0.43862167982771,
      "grad_norm": 9.608431816101074,
      "learning_rate": 0.00018973704501342692,
      "loss": 0.6053,
      "step": 6110
    },
    {
      "epoch": 0.43933955491744436,
      "grad_norm": 4.108154296875,
      "learning_rate": 0.000189683869080854,
      "loss": 0.286,
      "step": 6120
    },
    {
      "epoch": 0.44005743000717873,
      "grad_norm": 10.134041786193848,
      "learning_rate": 0.0001896306931482811,
      "loss": 0.4266,
      "step": 6130
    },
    {
      "epoch": 0.44077530509691315,
      "grad_norm": 10.39553451538086,
      "learning_rate": 0.0001895775172157082,
      "loss": 0.4247,
      "step": 6140
    },
    {
      "epoch": 0.4414931801866475,
      "grad_norm": 0.886890172958374,
      "learning_rate": 0.00018952434128313527,
      "loss": 0.3649,
      "step": 6150
    },
    {
      "epoch": 0.44221105527638194,
      "grad_norm": 19.1607666015625,
      "learning_rate": 0.00018947116535056235,
      "loss": 0.3209,
      "step": 6160
    },
    {
      "epoch": 0.4429289303661163,
      "grad_norm": 3.6741297245025635,
      "learning_rate": 0.00018941798941798943,
      "loss": 0.6046,
      "step": 6170
    },
    {
      "epoch": 0.44364680545585067,
      "grad_norm": 9.068282127380371,
      "learning_rate": 0.0001893648134854165,
      "loss": 0.1751,
      "step": 6180
    },
    {
      "epoch": 0.4443646805455851,
      "grad_norm": 14.862722396850586,
      "learning_rate": 0.0001893116375528436,
      "loss": 0.4676,
      "step": 6190
    },
    {
      "epoch": 0.44508255563531945,
      "grad_norm": 3.279519557952881,
      "learning_rate": 0.00018925846162027066,
      "loss": 0.3579,
      "step": 6200
    },
    {
      "epoch": 0.4458004307250538,
      "grad_norm": 6.97314453125,
      "learning_rate": 0.00018920528568769774,
      "loss": 0.305,
      "step": 6210
    },
    {
      "epoch": 0.44651830581478824,
      "grad_norm": 0.4417423903942108,
      "learning_rate": 0.00018915210975512482,
      "loss": 0.3203,
      "step": 6220
    },
    {
      "epoch": 0.4472361809045226,
      "grad_norm": 7.092375755310059,
      "learning_rate": 0.00018909893382255193,
      "loss": 0.5458,
      "step": 6230
    },
    {
      "epoch": 0.44795405599425697,
      "grad_norm": 1.6025340557098389,
      "learning_rate": 0.000189045757889979,
      "loss": 0.3566,
      "step": 6240
    },
    {
      "epoch": 0.4486719310839914,
      "grad_norm": 2.5099899768829346,
      "learning_rate": 0.0001889925819574061,
      "loss": 0.4303,
      "step": 6250
    },
    {
      "epoch": 0.44938980617372576,
      "grad_norm": 9.029745101928711,
      "learning_rate": 0.00018893940602483317,
      "loss": 0.4401,
      "step": 6260
    },
    {
      "epoch": 0.4501076812634602,
      "grad_norm": 7.850565433502197,
      "learning_rate": 0.00018888623009226025,
      "loss": 0.3198,
      "step": 6270
    },
    {
      "epoch": 0.45082555635319455,
      "grad_norm": 20.868484497070312,
      "learning_rate": 0.00018883305415968733,
      "loss": 0.4384,
      "step": 6280
    },
    {
      "epoch": 0.4515434314429289,
      "grad_norm": 7.925074100494385,
      "learning_rate": 0.00018877987822711443,
      "loss": 0.3184,
      "step": 6290
    },
    {
      "epoch": 0.45226130653266333,
      "grad_norm": 11.883323669433594,
      "learning_rate": 0.0001887267022945415,
      "loss": 0.2464,
      "step": 6300
    },
    {
      "epoch": 0.4529791816223977,
      "grad_norm": 16.936016082763672,
      "learning_rate": 0.00018867352636196857,
      "loss": 0.2517,
      "step": 6310
    },
    {
      "epoch": 0.45369705671213206,
      "grad_norm": 9.792603492736816,
      "learning_rate": 0.00018862035042939565,
      "loss": 0.3038,
      "step": 6320
    },
    {
      "epoch": 0.4544149318018665,
      "grad_norm": 6.019199371337891,
      "learning_rate": 0.00018856717449682275,
      "loss": 0.2826,
      "step": 6330
    },
    {
      "epoch": 0.45513280689160085,
      "grad_norm": 0.16951590776443481,
      "learning_rate": 0.00018851399856424983,
      "loss": 0.4083,
      "step": 6340
    },
    {
      "epoch": 0.45585068198133527,
      "grad_norm": 9.582266807556152,
      "learning_rate": 0.0001884608226316769,
      "loss": 0.4131,
      "step": 6350
    },
    {
      "epoch": 0.45656855707106964,
      "grad_norm": 4.012490749359131,
      "learning_rate": 0.000188407646699104,
      "loss": 0.51,
      "step": 6360
    },
    {
      "epoch": 0.457286432160804,
      "grad_norm": 13.282256126403809,
      "learning_rate": 0.00018835447076653107,
      "loss": 0.3406,
      "step": 6370
    },
    {
      "epoch": 0.4580043072505384,
      "grad_norm": 4.134006500244141,
      "learning_rate": 0.00018830129483395815,
      "loss": 0.3022,
      "step": 6380
    },
    {
      "epoch": 0.4587221823402728,
      "grad_norm": 15.983673095703125,
      "learning_rate": 0.00018824811890138526,
      "loss": 0.3505,
      "step": 6390
    },
    {
      "epoch": 0.45944005743000715,
      "grad_norm": 4.695316314697266,
      "learning_rate": 0.00018819494296881234,
      "loss": 0.3607,
      "step": 6400
    },
    {
      "epoch": 0.4601579325197416,
      "grad_norm": 10.677056312561035,
      "learning_rate": 0.00018814176703623942,
      "loss": 0.3803,
      "step": 6410
    },
    {
      "epoch": 0.46087580760947594,
      "grad_norm": 7.203023910522461,
      "learning_rate": 0.0001880885911036665,
      "loss": 0.3697,
      "step": 6420
    },
    {
      "epoch": 0.46159368269921036,
      "grad_norm": 1.926374077796936,
      "learning_rate": 0.00018803541517109357,
      "loss": 0.2925,
      "step": 6430
    },
    {
      "epoch": 0.4623115577889447,
      "grad_norm": 3.7185091972351074,
      "learning_rate": 0.00018798223923852065,
      "loss": 0.2662,
      "step": 6440
    },
    {
      "epoch": 0.4630294328786791,
      "grad_norm": 8.342915534973145,
      "learning_rate": 0.00018792906330594773,
      "loss": 0.4234,
      "step": 6450
    },
    {
      "epoch": 0.4637473079684135,
      "grad_norm": 10.45308780670166,
      "learning_rate": 0.0001878758873733748,
      "loss": 0.2766,
      "step": 6460
    },
    {
      "epoch": 0.4644651830581479,
      "grad_norm": 20.496828079223633,
      "learning_rate": 0.0001878227114408019,
      "loss": 0.2704,
      "step": 6470
    },
    {
      "epoch": 0.46518305814788224,
      "grad_norm": 9.836297988891602,
      "learning_rate": 0.00018776953550822897,
      "loss": 0.6412,
      "step": 6480
    },
    {
      "epoch": 0.46590093323761667,
      "grad_norm": 1.7075589895248413,
      "learning_rate": 0.00018771635957565608,
      "loss": 0.5082,
      "step": 6490
    },
    {
      "epoch": 0.46661880832735103,
      "grad_norm": 6.057934284210205,
      "learning_rate": 0.00018766318364308316,
      "loss": 0.3769,
      "step": 6500
    },
    {
      "epoch": 0.46733668341708545,
      "grad_norm": 14.791902542114258,
      "learning_rate": 0.00018761000771051024,
      "loss": 0.3393,
      "step": 6510
    },
    {
      "epoch": 0.4680545585068198,
      "grad_norm": 10.102235794067383,
      "learning_rate": 0.00018755683177793732,
      "loss": 0.417,
      "step": 6520
    },
    {
      "epoch": 0.4687724335965542,
      "grad_norm": 1.2361880540847778,
      "learning_rate": 0.0001875036558453644,
      "loss": 0.3966,
      "step": 6530
    },
    {
      "epoch": 0.4694903086862886,
      "grad_norm": 2.5716307163238525,
      "learning_rate": 0.00018745047991279148,
      "loss": 0.5458,
      "step": 6540
    },
    {
      "epoch": 0.47020818377602297,
      "grad_norm": 3.456779956817627,
      "learning_rate": 0.00018739730398021856,
      "loss": 0.339,
      "step": 6550
    },
    {
      "epoch": 0.47092605886575734,
      "grad_norm": 10.23364543914795,
      "learning_rate": 0.00018734412804764566,
      "loss": 0.5789,
      "step": 6560
    },
    {
      "epoch": 0.47164393395549176,
      "grad_norm": 1.9407559633255005,
      "learning_rate": 0.00018729095211507271,
      "loss": 0.3717,
      "step": 6570
    },
    {
      "epoch": 0.4723618090452261,
      "grad_norm": 24.503944396972656,
      "learning_rate": 0.0001872377761824998,
      "loss": 0.6088,
      "step": 6580
    },
    {
      "epoch": 0.47307968413496054,
      "grad_norm": 26.721452713012695,
      "learning_rate": 0.0001871846002499269,
      "loss": 0.6956,
      "step": 6590
    },
    {
      "epoch": 0.4737975592246949,
      "grad_norm": 8.180331230163574,
      "learning_rate": 0.00018713142431735398,
      "loss": 0.5237,
      "step": 6600
    },
    {
      "epoch": 0.4745154343144293,
      "grad_norm": 7.865305423736572,
      "learning_rate": 0.00018707824838478106,
      "loss": 0.1864,
      "step": 6610
    },
    {
      "epoch": 0.4752333094041637,
      "grad_norm": 5.323720932006836,
      "learning_rate": 0.00018702507245220814,
      "loss": 0.4388,
      "step": 6620
    },
    {
      "epoch": 0.47595118449389806,
      "grad_norm": 12.890048027038574,
      "learning_rate": 0.00018697189651963522,
      "loss": 0.2993,
      "step": 6630
    },
    {
      "epoch": 0.4766690595836324,
      "grad_norm": 15.062538146972656,
      "learning_rate": 0.0001869187205870623,
      "loss": 0.3374,
      "step": 6640
    },
    {
      "epoch": 0.47738693467336685,
      "grad_norm": 0.2666378319263458,
      "learning_rate": 0.00018686554465448938,
      "loss": 0.2852,
      "step": 6650
    },
    {
      "epoch": 0.4781048097631012,
      "grad_norm": 8.282364845275879,
      "learning_rate": 0.00018681236872191648,
      "loss": 0.1746,
      "step": 6660
    },
    {
      "epoch": 0.47882268485283563,
      "grad_norm": 11.403924942016602,
      "learning_rate": 0.00018675919278934356,
      "loss": 0.2399,
      "step": 6670
    },
    {
      "epoch": 0.47954055994257,
      "grad_norm": 20.959558486938477,
      "learning_rate": 0.00018670601685677062,
      "loss": 0.2952,
      "step": 6680
    },
    {
      "epoch": 0.48025843503230436,
      "grad_norm": 13.743422508239746,
      "learning_rate": 0.00018665284092419772,
      "loss": 0.3306,
      "step": 6690
    },
    {
      "epoch": 0.4809763101220388,
      "grad_norm": 6.597092628479004,
      "learning_rate": 0.0001865996649916248,
      "loss": 0.2929,
      "step": 6700
    },
    {
      "epoch": 0.48169418521177315,
      "grad_norm": 8.648802757263184,
      "learning_rate": 0.00018654648905905188,
      "loss": 0.3724,
      "step": 6710
    },
    {
      "epoch": 0.4824120603015075,
      "grad_norm": 11.164175033569336,
      "learning_rate": 0.000186493313126479,
      "loss": 0.3619,
      "step": 6720
    },
    {
      "epoch": 0.48312993539124194,
      "grad_norm": 17.181758880615234,
      "learning_rate": 0.00018644013719390604,
      "loss": 0.6988,
      "step": 6730
    },
    {
      "epoch": 0.4838478104809763,
      "grad_norm": 6.895853042602539,
      "learning_rate": 0.00018638696126133312,
      "loss": 0.2001,
      "step": 6740
    },
    {
      "epoch": 0.48456568557071067,
      "grad_norm": 13.067466735839844,
      "learning_rate": 0.0001863337853287602,
      "loss": 0.3467,
      "step": 6750
    },
    {
      "epoch": 0.4852835606604451,
      "grad_norm": 4.970389366149902,
      "learning_rate": 0.0001862806093961873,
      "loss": 0.2434,
      "step": 6760
    },
    {
      "epoch": 0.48600143575017946,
      "grad_norm": 2.2819292545318604,
      "learning_rate": 0.00018622743346361439,
      "loss": 0.4447,
      "step": 6770
    },
    {
      "epoch": 0.4867193108399139,
      "grad_norm": 7.270702362060547,
      "learning_rate": 0.00018617425753104146,
      "loss": 0.2147,
      "step": 6780
    },
    {
      "epoch": 0.48743718592964824,
      "grad_norm": 9.517415046691895,
      "learning_rate": 0.00018612108159846854,
      "loss": 0.4747,
      "step": 6790
    },
    {
      "epoch": 0.4881550610193826,
      "grad_norm": 21.30824851989746,
      "learning_rate": 0.00018606790566589562,
      "loss": 0.2918,
      "step": 6800
    },
    {
      "epoch": 0.48887293610911703,
      "grad_norm": 20.677383422851562,
      "learning_rate": 0.0001860147297333227,
      "loss": 0.5441,
      "step": 6810
    },
    {
      "epoch": 0.4895908111988514,
      "grad_norm": 14.380184173583984,
      "learning_rate": 0.0001859615538007498,
      "loss": 0.3911,
      "step": 6820
    },
    {
      "epoch": 0.49030868628858576,
      "grad_norm": 10.205326080322266,
      "learning_rate": 0.00018590837786817686,
      "loss": 0.3759,
      "step": 6830
    },
    {
      "epoch": 0.4910265613783202,
      "grad_norm": 7.931687831878662,
      "learning_rate": 0.00018585520193560394,
      "loss": 0.4387,
      "step": 6840
    },
    {
      "epoch": 0.49174443646805455,
      "grad_norm": 1.5726876258850098,
      "learning_rate": 0.00018580202600303102,
      "loss": 0.4125,
      "step": 6850
    },
    {
      "epoch": 0.49246231155778897,
      "grad_norm": 31.060827255249023,
      "learning_rate": 0.00018574885007045813,
      "loss": 0.5584,
      "step": 6860
    },
    {
      "epoch": 0.49318018664752333,
      "grad_norm": 1.9922171831130981,
      "learning_rate": 0.0001856956741378852,
      "loss": 0.3247,
      "step": 6870
    },
    {
      "epoch": 0.4938980617372577,
      "grad_norm": 21.219234466552734,
      "learning_rate": 0.0001856424982053123,
      "loss": 0.5046,
      "step": 6880
    },
    {
      "epoch": 0.4946159368269921,
      "grad_norm": 4.6312665939331055,
      "learning_rate": 0.00018558932227273937,
      "loss": 0.2325,
      "step": 6890
    },
    {
      "epoch": 0.4953338119167265,
      "grad_norm": 0.0615130253136158,
      "learning_rate": 0.00018553614634016645,
      "loss": 0.2838,
      "step": 6900
    },
    {
      "epoch": 0.49605168700646085,
      "grad_norm": 6.883524417877197,
      "learning_rate": 0.00018548297040759353,
      "loss": 0.5049,
      "step": 6910
    },
    {
      "epoch": 0.49676956209619527,
      "grad_norm": 15.710931777954102,
      "learning_rate": 0.00018542979447502063,
      "loss": 0.328,
      "step": 6920
    },
    {
      "epoch": 0.49748743718592964,
      "grad_norm": 11.216903686523438,
      "learning_rate": 0.0001853766185424477,
      "loss": 0.5538,
      "step": 6930
    },
    {
      "epoch": 0.49820531227566406,
      "grad_norm": 6.303172588348389,
      "learning_rate": 0.00018532344260987476,
      "loss": 0.3978,
      "step": 6940
    },
    {
      "epoch": 0.4989231873653984,
      "grad_norm": 4.211390972137451,
      "learning_rate": 0.00018527026667730184,
      "loss": 0.2876,
      "step": 6950
    },
    {
      "epoch": 0.4996410624551328,
      "grad_norm": 1.815003752708435,
      "learning_rate": 0.00018521709074472895,
      "loss": 0.4304,
      "step": 6960
    },
    {
      "epoch": 0.5003589375448672,
      "grad_norm": 8.757102012634277,
      "learning_rate": 0.00018516391481215603,
      "loss": 0.5127,
      "step": 6970
    },
    {
      "epoch": 0.5010768126346016,
      "grad_norm": 0.879818320274353,
      "learning_rate": 0.0001851107388795831,
      "loss": 0.5065,
      "step": 6980
    },
    {
      "epoch": 0.501794687724336,
      "grad_norm": 24.910625457763672,
      "learning_rate": 0.0001850575629470102,
      "loss": 0.4826,
      "step": 6990
    },
    {
      "epoch": 0.5025125628140703,
      "grad_norm": 18.188308715820312,
      "learning_rate": 0.00018500438701443727,
      "loss": 0.2213,
      "step": 7000
    },
    {
      "epoch": 0.5032304379038047,
      "grad_norm": 0.8715868592262268,
      "learning_rate": 0.00018495121108186435,
      "loss": 0.394,
      "step": 7010
    },
    {
      "epoch": 0.5039483129935391,
      "grad_norm": 0.7334087491035461,
      "learning_rate": 0.00018489803514929145,
      "loss": 0.3829,
      "step": 7020
    },
    {
      "epoch": 0.5046661880832735,
      "grad_norm": 8.50461483001709,
      "learning_rate": 0.00018484485921671853,
      "loss": 0.4576,
      "step": 7030
    },
    {
      "epoch": 0.5053840631730079,
      "grad_norm": 1.1896220445632935,
      "learning_rate": 0.0001847916832841456,
      "loss": 0.1344,
      "step": 7040
    },
    {
      "epoch": 0.5061019382627423,
      "grad_norm": 13.271541595458984,
      "learning_rate": 0.00018473850735157267,
      "loss": 0.4457,
      "step": 7050
    },
    {
      "epoch": 0.5068198133524767,
      "grad_norm": 6.624181747436523,
      "learning_rate": 0.00018468533141899977,
      "loss": 0.3256,
      "step": 7060
    },
    {
      "epoch": 0.507537688442211,
      "grad_norm": 0.4745083749294281,
      "learning_rate": 0.00018463215548642685,
      "loss": 0.1436,
      "step": 7070
    },
    {
      "epoch": 0.5082555635319455,
      "grad_norm": 11.775962829589844,
      "learning_rate": 0.00018457897955385393,
      "loss": 0.5311,
      "step": 7080
    },
    {
      "epoch": 0.5089734386216799,
      "grad_norm": 11.129064559936523,
      "learning_rate": 0.000184525803621281,
      "loss": 0.3,
      "step": 7090
    },
    {
      "epoch": 0.5096913137114142,
      "grad_norm": 7.835327625274658,
      "learning_rate": 0.0001844726276887081,
      "loss": 0.2847,
      "step": 7100
    },
    {
      "epoch": 0.5104091888011486,
      "grad_norm": 3.8714513778686523,
      "learning_rate": 0.00018441945175613517,
      "loss": 0.5969,
      "step": 7110
    },
    {
      "epoch": 0.511127063890883,
      "grad_norm": 5.74752140045166,
      "learning_rate": 0.00018436627582356228,
      "loss": 0.2994,
      "step": 7120
    },
    {
      "epoch": 0.5118449389806173,
      "grad_norm": 5.958890438079834,
      "learning_rate": 0.00018431309989098936,
      "loss": 0.2083,
      "step": 7130
    },
    {
      "epoch": 0.5125628140703518,
      "grad_norm": 1.980952262878418,
      "learning_rate": 0.00018425992395841644,
      "loss": 0.2068,
      "step": 7140
    },
    {
      "epoch": 0.5132806891600862,
      "grad_norm": 12.921666145324707,
      "learning_rate": 0.00018420674802584351,
      "loss": 0.4833,
      "step": 7150
    },
    {
      "epoch": 0.5139985642498205,
      "grad_norm": 4.4624223709106445,
      "learning_rate": 0.0001841535720932706,
      "loss": 0.2048,
      "step": 7160
    },
    {
      "epoch": 0.5147164393395549,
      "grad_norm": 0.41596418619155884,
      "learning_rate": 0.00018410039616069767,
      "loss": 0.2531,
      "step": 7170
    },
    {
      "epoch": 0.5154343144292893,
      "grad_norm": 1.4481117725372314,
      "learning_rate": 0.00018404722022812475,
      "loss": 0.3136,
      "step": 7180
    },
    {
      "epoch": 0.5161521895190236,
      "grad_norm": 30.065052032470703,
      "learning_rate": 0.00018399404429555186,
      "loss": 0.5978,
      "step": 7190
    },
    {
      "epoch": 0.5168700646087581,
      "grad_norm": 13.085323333740234,
      "learning_rate": 0.0001839408683629789,
      "loss": 0.3051,
      "step": 7200
    },
    {
      "epoch": 0.5175879396984925,
      "grad_norm": 6.692070960998535,
      "learning_rate": 0.000183887692430406,
      "loss": 0.3455,
      "step": 7210
    },
    {
      "epoch": 0.5183058147882269,
      "grad_norm": 11.379188537597656,
      "learning_rate": 0.0001838345164978331,
      "loss": 0.4828,
      "step": 7220
    },
    {
      "epoch": 0.5190236898779612,
      "grad_norm": 14.25040340423584,
      "learning_rate": 0.00018378134056526018,
      "loss": 0.3991,
      "step": 7230
    },
    {
      "epoch": 0.5197415649676956,
      "grad_norm": 15.598342895507812,
      "learning_rate": 0.00018372816463268726,
      "loss": 0.3284,
      "step": 7240
    },
    {
      "epoch": 0.5204594400574301,
      "grad_norm": 5.514676094055176,
      "learning_rate": 0.00018367498870011434,
      "loss": 0.3179,
      "step": 7250
    },
    {
      "epoch": 0.5211773151471644,
      "grad_norm": 7.509927749633789,
      "learning_rate": 0.00018362181276754142,
      "loss": 0.2004,
      "step": 7260
    },
    {
      "epoch": 0.5218951902368988,
      "grad_norm": 23.074337005615234,
      "learning_rate": 0.0001835686368349685,
      "loss": 0.5071,
      "step": 7270
    },
    {
      "epoch": 0.5226130653266332,
      "grad_norm": 0.17698577046394348,
      "learning_rate": 0.00018351546090239558,
      "loss": 0.3183,
      "step": 7280
    },
    {
      "epoch": 0.5233309404163675,
      "grad_norm": 11.557934761047363,
      "learning_rate": 0.00018346228496982268,
      "loss": 0.1521,
      "step": 7290
    },
    {
      "epoch": 0.5240488155061019,
      "grad_norm": 1.0473084449768066,
      "learning_rate": 0.00018340910903724976,
      "loss": 0.1187,
      "step": 7300
    },
    {
      "epoch": 0.5247666905958364,
      "grad_norm": 5.703590393066406,
      "learning_rate": 0.00018335593310467681,
      "loss": 0.3093,
      "step": 7310
    },
    {
      "epoch": 0.5254845656855707,
      "grad_norm": 4.205304145812988,
      "learning_rate": 0.00018330275717210392,
      "loss": 0.2332,
      "step": 7320
    },
    {
      "epoch": 0.5262024407753051,
      "grad_norm": 0.11484265327453613,
      "learning_rate": 0.000183249581239531,
      "loss": 0.2384,
      "step": 7330
    },
    {
      "epoch": 0.5269203158650395,
      "grad_norm": 18.661115646362305,
      "learning_rate": 0.00018319640530695808,
      "loss": 0.1277,
      "step": 7340
    },
    {
      "epoch": 0.5276381909547738,
      "grad_norm": 0.32066431641578674,
      "learning_rate": 0.00018314322937438519,
      "loss": 0.2611,
      "step": 7350
    },
    {
      "epoch": 0.5283560660445082,
      "grad_norm": 10.60971736907959,
      "learning_rate": 0.00018309005344181224,
      "loss": 0.3656,
      "step": 7360
    },
    {
      "epoch": 0.5290739411342427,
      "grad_norm": 15.253334999084473,
      "learning_rate": 0.00018303687750923932,
      "loss": 0.3565,
      "step": 7370
    },
    {
      "epoch": 0.529791816223977,
      "grad_norm": 1.7197651863098145,
      "learning_rate": 0.0001829837015766664,
      "loss": 0.1936,
      "step": 7380
    },
    {
      "epoch": 0.5305096913137114,
      "grad_norm": 9.170820236206055,
      "learning_rate": 0.0001829305256440935,
      "loss": 0.4309,
      "step": 7390
    },
    {
      "epoch": 0.5312275664034458,
      "grad_norm": 16.604373931884766,
      "learning_rate": 0.00018287734971152058,
      "loss": 0.5063,
      "step": 7400
    },
    {
      "epoch": 0.5319454414931802,
      "grad_norm": 0.08176188915967941,
      "learning_rate": 0.00018282417377894766,
      "loss": 0.1979,
      "step": 7410
    },
    {
      "epoch": 0.5326633165829145,
      "grad_norm": 1.5371818542480469,
      "learning_rate": 0.00018277099784637474,
      "loss": 0.3311,
      "step": 7420
    },
    {
      "epoch": 0.533381191672649,
      "grad_norm": 7.0558671951293945,
      "learning_rate": 0.00018271782191380182,
      "loss": 0.3402,
      "step": 7430
    },
    {
      "epoch": 0.5340990667623834,
      "grad_norm": 11.027749061584473,
      "learning_rate": 0.0001826646459812289,
      "loss": 0.3103,
      "step": 7440
    },
    {
      "epoch": 0.5348169418521177,
      "grad_norm": 9.483819961547852,
      "learning_rate": 0.000182611470048656,
      "loss": 0.2057,
      "step": 7450
    },
    {
      "epoch": 0.5355348169418521,
      "grad_norm": 14.773197174072266,
      "learning_rate": 0.00018255829411608306,
      "loss": 0.3617,
      "step": 7460
    },
    {
      "epoch": 0.5362526920315865,
      "grad_norm": 1.5820244550704956,
      "learning_rate": 0.00018250511818351014,
      "loss": 0.3039,
      "step": 7470
    },
    {
      "epoch": 0.5369705671213209,
      "grad_norm": 1.9285969734191895,
      "learning_rate": 0.00018245194225093722,
      "loss": 0.4605,
      "step": 7480
    },
    {
      "epoch": 0.5376884422110553,
      "grad_norm": 9.404413223266602,
      "learning_rate": 0.00018239876631836433,
      "loss": 0.3484,
      "step": 7490
    },
    {
      "epoch": 0.5384063173007897,
      "grad_norm": 2.5934054851531982,
      "learning_rate": 0.0001823455903857914,
      "loss": 0.1943,
      "step": 7500
    },
    {
      "epoch": 0.539124192390524,
      "grad_norm": 9.121219635009766,
      "learning_rate": 0.00018229241445321848,
      "loss": 0.3659,
      "step": 7510
    },
    {
      "epoch": 0.5398420674802584,
      "grad_norm": 3.8526062965393066,
      "learning_rate": 0.00018223923852064556,
      "loss": 0.243,
      "step": 7520
    },
    {
      "epoch": 0.5405599425699928,
      "grad_norm": 31.86806297302246,
      "learning_rate": 0.00018218606258807264,
      "loss": 0.3456,
      "step": 7530
    },
    {
      "epoch": 0.5412778176597272,
      "grad_norm": 0.2135343998670578,
      "learning_rate": 0.00018213288665549972,
      "loss": 0.4311,
      "step": 7540
    },
    {
      "epoch": 0.5419956927494616,
      "grad_norm": 2.9055802822113037,
      "learning_rate": 0.00018207971072292683,
      "loss": 0.3157,
      "step": 7550
    },
    {
      "epoch": 0.542713567839196,
      "grad_norm": 4.288837909698486,
      "learning_rate": 0.0001820265347903539,
      "loss": 0.3029,
      "step": 7560
    },
    {
      "epoch": 0.5434314429289304,
      "grad_norm": 3.124582529067993,
      "learning_rate": 0.00018197335885778096,
      "loss": 0.2368,
      "step": 7570
    },
    {
      "epoch": 0.5441493180186647,
      "grad_norm": 1.670087456703186,
      "learning_rate": 0.00018192018292520804,
      "loss": 0.2675,
      "step": 7580
    },
    {
      "epoch": 0.5448671931083992,
      "grad_norm": 4.106883525848389,
      "learning_rate": 0.00018186700699263515,
      "loss": 0.3752,
      "step": 7590
    },
    {
      "epoch": 0.5455850681981336,
      "grad_norm": 15.55656623840332,
      "learning_rate": 0.00018181383106006223,
      "loss": 0.0716,
      "step": 7600
    },
    {
      "epoch": 0.5463029432878679,
      "grad_norm": 10.070992469787598,
      "learning_rate": 0.0001817606551274893,
      "loss": 0.4364,
      "step": 7610
    },
    {
      "epoch": 0.5470208183776023,
      "grad_norm": 41.96803283691406,
      "learning_rate": 0.0001817074791949164,
      "loss": 0.2467,
      "step": 7620
    },
    {
      "epoch": 0.5477386934673367,
      "grad_norm": 22.236003875732422,
      "learning_rate": 0.00018165430326234347,
      "loss": 0.5083,
      "step": 7630
    },
    {
      "epoch": 0.548456568557071,
      "grad_norm": 11.634846687316895,
      "learning_rate": 0.00018160112732977055,
      "loss": 0.3455,
      "step": 7640
    },
    {
      "epoch": 0.5491744436468055,
      "grad_norm": 27.386991500854492,
      "learning_rate": 0.00018154795139719765,
      "loss": 0.172,
      "step": 7650
    },
    {
      "epoch": 0.5498923187365399,
      "grad_norm": 18.939231872558594,
      "learning_rate": 0.00018149477546462473,
      "loss": 0.3113,
      "step": 7660
    },
    {
      "epoch": 0.5506101938262742,
      "grad_norm": 14.583816528320312,
      "learning_rate": 0.0001814415995320518,
      "loss": 0.2336,
      "step": 7670
    },
    {
      "epoch": 0.5513280689160086,
      "grad_norm": 0.6548125743865967,
      "learning_rate": 0.00018138842359947886,
      "loss": 0.4457,
      "step": 7680
    },
    {
      "epoch": 0.552045944005743,
      "grad_norm": 8.74632740020752,
      "learning_rate": 0.00018133524766690597,
      "loss": 0.2332,
      "step": 7690
    },
    {
      "epoch": 0.5527638190954773,
      "grad_norm": 6.0458221435546875,
      "learning_rate": 0.00018128207173433305,
      "loss": 0.4842,
      "step": 7700
    },
    {
      "epoch": 0.5534816941852118,
      "grad_norm": 1.0713298320770264,
      "learning_rate": 0.00018122889580176013,
      "loss": 0.3515,
      "step": 7710
    },
    {
      "epoch": 0.5541995692749462,
      "grad_norm": 8.86851692199707,
      "learning_rate": 0.0001811757198691872,
      "loss": 0.2326,
      "step": 7720
    },
    {
      "epoch": 0.5549174443646806,
      "grad_norm": 18.453617095947266,
      "learning_rate": 0.0001811225439366143,
      "loss": 0.3455,
      "step": 7730
    },
    {
      "epoch": 0.5556353194544149,
      "grad_norm": 8.18680191040039,
      "learning_rate": 0.00018106936800404137,
      "loss": 0.4657,
      "step": 7740
    },
    {
      "epoch": 0.5563531945441493,
      "grad_norm": 8.105086326599121,
      "learning_rate": 0.00018101619207146847,
      "loss": 0.25,
      "step": 7750
    },
    {
      "epoch": 0.5570710696338838,
      "grad_norm": 13.787933349609375,
      "learning_rate": 0.00018096301613889555,
      "loss": 0.3634,
      "step": 7760
    },
    {
      "epoch": 0.5577889447236181,
      "grad_norm": 4.5936455726623535,
      "learning_rate": 0.00018090984020632263,
      "loss": 0.2315,
      "step": 7770
    },
    {
      "epoch": 0.5585068198133525,
      "grad_norm": 3.137397289276123,
      "learning_rate": 0.00018085666427374969,
      "loss": 0.225,
      "step": 7780
    },
    {
      "epoch": 0.5592246949030869,
      "grad_norm": 0.7156670093536377,
      "learning_rate": 0.0001808034883411768,
      "loss": 0.6384,
      "step": 7790
    },
    {
      "epoch": 0.5599425699928212,
      "grad_norm": 13.14498233795166,
      "learning_rate": 0.00018075031240860387,
      "loss": 0.3145,
      "step": 7800
    },
    {
      "epoch": 0.5606604450825556,
      "grad_norm": 9.264433860778809,
      "learning_rate": 0.00018069713647603095,
      "loss": 0.2142,
      "step": 7810
    },
    {
      "epoch": 0.5613783201722901,
      "grad_norm": 6.438668251037598,
      "learning_rate": 0.00018064396054345806,
      "loss": 0.5307,
      "step": 7820
    },
    {
      "epoch": 0.5620961952620244,
      "grad_norm": 1.3125633001327515,
      "learning_rate": 0.0001805907846108851,
      "loss": 0.2506,
      "step": 7830
    },
    {
      "epoch": 0.5628140703517588,
      "grad_norm": 16.184833526611328,
      "learning_rate": 0.0001805376086783122,
      "loss": 0.3108,
      "step": 7840
    },
    {
      "epoch": 0.5635319454414932,
      "grad_norm": 1.100014090538025,
      "learning_rate": 0.0001804844327457393,
      "loss": 0.4637,
      "step": 7850
    },
    {
      "epoch": 0.5642498205312275,
      "grad_norm": 4.357879161834717,
      "learning_rate": 0.00018043125681316638,
      "loss": 0.4155,
      "step": 7860
    },
    {
      "epoch": 0.5649676956209619,
      "grad_norm": 0.6675605177879333,
      "learning_rate": 0.00018037808088059346,
      "loss": 0.4099,
      "step": 7870
    },
    {
      "epoch": 0.5656855707106964,
      "grad_norm": 6.183744430541992,
      "learning_rate": 0.00018032490494802053,
      "loss": 0.4011,
      "step": 7880
    },
    {
      "epoch": 0.5664034458004307,
      "grad_norm": 11.524213790893555,
      "learning_rate": 0.00018027172901544761,
      "loss": 0.425,
      "step": 7890
    },
    {
      "epoch": 0.5671213208901651,
      "grad_norm": 10.025464057922363,
      "learning_rate": 0.0001802185530828747,
      "loss": 0.1541,
      "step": 7900
    },
    {
      "epoch": 0.5678391959798995,
      "grad_norm": 17.208826065063477,
      "learning_rate": 0.00018016537715030177,
      "loss": 0.4327,
      "step": 7910
    },
    {
      "epoch": 0.5685570710696339,
      "grad_norm": 5.269631385803223,
      "learning_rate": 0.00018011220121772888,
      "loss": 0.2951,
      "step": 7920
    },
    {
      "epoch": 0.5692749461593682,
      "grad_norm": 0.3604435920715332,
      "learning_rate": 0.00018005902528515596,
      "loss": 0.3043,
      "step": 7930
    },
    {
      "epoch": 0.5699928212491027,
      "grad_norm": 9.375688552856445,
      "learning_rate": 0.000180005849352583,
      "loss": 0.2502,
      "step": 7940
    },
    {
      "epoch": 0.5707106963388371,
      "grad_norm": 9.251527786254883,
      "learning_rate": 0.00017995267342001012,
      "loss": 0.445,
      "step": 7950
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 8.096343994140625,
      "learning_rate": 0.0001798994974874372,
      "loss": 0.4091,
      "step": 7960
    },
    {
      "epoch": 0.5721464465183058,
      "grad_norm": 8.195535659790039,
      "learning_rate": 0.00017984632155486428,
      "loss": 0.1693,
      "step": 7970
    },
    {
      "epoch": 0.5728643216080402,
      "grad_norm": 13.120677947998047,
      "learning_rate": 0.00017979314562229136,
      "loss": 0.2568,
      "step": 7980
    },
    {
      "epoch": 0.5735821966977745,
      "grad_norm": 0.5129302144050598,
      "learning_rate": 0.00017973996968971844,
      "loss": 0.134,
      "step": 7990
    },
    {
      "epoch": 0.574300071787509,
      "grad_norm": 1.3797030448913574,
      "learning_rate": 0.00017968679375714552,
      "loss": 0.2443,
      "step": 8000
    },
    {
      "epoch": 0.5750179468772434,
      "grad_norm": 19.18665313720703,
      "learning_rate": 0.0001796336178245726,
      "loss": 0.4918,
      "step": 8010
    },
    {
      "epoch": 0.5757358219669777,
      "grad_norm": 17.05031394958496,
      "learning_rate": 0.0001795804418919997,
      "loss": 0.3707,
      "step": 8020
    },
    {
      "epoch": 0.5764536970567121,
      "grad_norm": 26.0999755859375,
      "learning_rate": 0.00017952726595942678,
      "loss": 0.4534,
      "step": 8030
    },
    {
      "epoch": 0.5771715721464465,
      "grad_norm": 8.066903114318848,
      "learning_rate": 0.00017947409002685383,
      "loss": 0.4345,
      "step": 8040
    },
    {
      "epoch": 0.5778894472361809,
      "grad_norm": 3.015598773956299,
      "learning_rate": 0.00017942091409428094,
      "loss": 0.311,
      "step": 8050
    },
    {
      "epoch": 0.5786073223259153,
      "grad_norm": 0.5249974131584167,
      "learning_rate": 0.00017936773816170802,
      "loss": 0.2022,
      "step": 8060
    },
    {
      "epoch": 0.5793251974156497,
      "grad_norm": 3.914290428161621,
      "learning_rate": 0.0001793145622291351,
      "loss": 0.1583,
      "step": 8070
    },
    {
      "epoch": 0.5800430725053841,
      "grad_norm": 0.7158316969871521,
      "learning_rate": 0.0001792613862965622,
      "loss": 0.2986,
      "step": 8080
    },
    {
      "epoch": 0.5807609475951184,
      "grad_norm": 7.268983840942383,
      "learning_rate": 0.00017920821036398926,
      "loss": 0.2811,
      "step": 8090
    },
    {
      "epoch": 0.5814788226848528,
      "grad_norm": 6.891279220581055,
      "learning_rate": 0.00017915503443141634,
      "loss": 0.3467,
      "step": 8100
    },
    {
      "epoch": 0.5821966977745873,
      "grad_norm": 1.3878010511398315,
      "learning_rate": 0.00017910185849884342,
      "loss": 0.4691,
      "step": 8110
    },
    {
      "epoch": 0.5829145728643216,
      "grad_norm": 4.4973320960998535,
      "learning_rate": 0.00017904868256627052,
      "loss": 0.2404,
      "step": 8120
    },
    {
      "epoch": 0.583632447954056,
      "grad_norm": 13.150846481323242,
      "learning_rate": 0.0001789955066336976,
      "loss": 0.2201,
      "step": 8130
    },
    {
      "epoch": 0.5843503230437904,
      "grad_norm": 14.04060173034668,
      "learning_rate": 0.00017894233070112468,
      "loss": 0.3139,
      "step": 8140
    },
    {
      "epoch": 0.5850681981335247,
      "grad_norm": 0.3661705553531647,
      "learning_rate": 0.00017888915476855176,
      "loss": 0.3424,
      "step": 8150
    },
    {
      "epoch": 0.5857860732232592,
      "grad_norm": 6.090330600738525,
      "learning_rate": 0.00017883597883597884,
      "loss": 0.2535,
      "step": 8160
    },
    {
      "epoch": 0.5865039483129936,
      "grad_norm": 12.859427452087402,
      "learning_rate": 0.00017878280290340592,
      "loss": 0.2114,
      "step": 8170
    },
    {
      "epoch": 0.5872218234027279,
      "grad_norm": 2.551968812942505,
      "learning_rate": 0.00017872962697083303,
      "loss": 0.1728,
      "step": 8180
    },
    {
      "epoch": 0.5879396984924623,
      "grad_norm": 0.12885107100009918,
      "learning_rate": 0.0001786764510382601,
      "loss": 0.1497,
      "step": 8190
    },
    {
      "epoch": 0.5886575735821967,
      "grad_norm": 0.4709925353527069,
      "learning_rate": 0.00017862327510568716,
      "loss": 0.3319,
      "step": 8200
    },
    {
      "epoch": 0.589375448671931,
      "grad_norm": 10.837961196899414,
      "learning_rate": 0.00017857009917311424,
      "loss": 0.137,
      "step": 8210
    },
    {
      "epoch": 0.5900933237616655,
      "grad_norm": 5.722709655761719,
      "learning_rate": 0.00017851692324054135,
      "loss": 0.4754,
      "step": 8220
    },
    {
      "epoch": 0.5908111988513999,
      "grad_norm": 0.058360811322927475,
      "learning_rate": 0.00017846374730796843,
      "loss": 0.0975,
      "step": 8230
    },
    {
      "epoch": 0.5915290739411343,
      "grad_norm": 0.19297169148921967,
      "learning_rate": 0.0001784105713753955,
      "loss": 0.2717,
      "step": 8240
    },
    {
      "epoch": 0.5922469490308686,
      "grad_norm": 4.407927989959717,
      "learning_rate": 0.00017835739544282258,
      "loss": 0.3045,
      "step": 8250
    },
    {
      "epoch": 0.592964824120603,
      "grad_norm": 10.905023574829102,
      "learning_rate": 0.00017830421951024966,
      "loss": 0.2611,
      "step": 8260
    },
    {
      "epoch": 0.5936826992103375,
      "grad_norm": 6.120312690734863,
      "learning_rate": 0.00017825104357767674,
      "loss": 0.4377,
      "step": 8270
    },
    {
      "epoch": 0.5944005743000718,
      "grad_norm": 0.7145756483078003,
      "learning_rate": 0.00017819786764510385,
      "loss": 0.2969,
      "step": 8280
    },
    {
      "epoch": 0.5951184493898062,
      "grad_norm": 16.68655014038086,
      "learning_rate": 0.00017814469171253093,
      "loss": 0.3383,
      "step": 8290
    },
    {
      "epoch": 0.5958363244795406,
      "grad_norm": 2.368863582611084,
      "learning_rate": 0.000178091515779958,
      "loss": 0.4603,
      "step": 8300
    },
    {
      "epoch": 0.5965541995692749,
      "grad_norm": 14.671781539916992,
      "learning_rate": 0.00017803833984738506,
      "loss": 0.3097,
      "step": 8310
    },
    {
      "epoch": 0.5972720746590093,
      "grad_norm": 6.921037197113037,
      "learning_rate": 0.00017798516391481217,
      "loss": 0.3012,
      "step": 8320
    },
    {
      "epoch": 0.5979899497487438,
      "grad_norm": 11.324048042297363,
      "learning_rate": 0.00017793198798223925,
      "loss": 0.2807,
      "step": 8330
    },
    {
      "epoch": 0.5987078248384781,
      "grad_norm": 15.03763198852539,
      "learning_rate": 0.00017787881204966633,
      "loss": 0.5466,
      "step": 8340
    },
    {
      "epoch": 0.5994256999282125,
      "grad_norm": 20.074153900146484,
      "learning_rate": 0.0001778256361170934,
      "loss": 0.3262,
      "step": 8350
    },
    {
      "epoch": 0.6001435750179469,
      "grad_norm": 0.4855998456478119,
      "learning_rate": 0.00017777246018452049,
      "loss": 0.3952,
      "step": 8360
    },
    {
      "epoch": 0.6008614501076812,
      "grad_norm": 14.143453598022461,
      "learning_rate": 0.00017771928425194757,
      "loss": 0.2807,
      "step": 8370
    },
    {
      "epoch": 0.6015793251974156,
      "grad_norm": 18.772689819335938,
      "learning_rate": 0.00017766610831937467,
      "loss": 0.2736,
      "step": 8380
    },
    {
      "epoch": 0.6022972002871501,
      "grad_norm": 0.09592466801404953,
      "learning_rate": 0.00017761293238680175,
      "loss": 0.3176,
      "step": 8390
    },
    {
      "epoch": 0.6030150753768844,
      "grad_norm": 19.15901756286621,
      "learning_rate": 0.00017755975645422883,
      "loss": 0.4507,
      "step": 8400
    },
    {
      "epoch": 0.6037329504666188,
      "grad_norm": 1.8654391765594482,
      "learning_rate": 0.00017750658052165588,
      "loss": 0.3119,
      "step": 8410
    },
    {
      "epoch": 0.6044508255563532,
      "grad_norm": 8.797539710998535,
      "learning_rate": 0.000177453404589083,
      "loss": 0.1979,
      "step": 8420
    },
    {
      "epoch": 0.6051687006460876,
      "grad_norm": 16.402692794799805,
      "learning_rate": 0.00017740022865651007,
      "loss": 0.334,
      "step": 8430
    },
    {
      "epoch": 0.6058865757358219,
      "grad_norm": 14.092290878295898,
      "learning_rate": 0.00017734705272393715,
      "loss": 0.2646,
      "step": 8440
    },
    {
      "epoch": 0.6066044508255564,
      "grad_norm": 5.128342151641846,
      "learning_rate": 0.00017729387679136426,
      "loss": 0.2797,
      "step": 8450
    },
    {
      "epoch": 0.6073223259152908,
      "grad_norm": 19.49176597595215,
      "learning_rate": 0.0001772407008587913,
      "loss": 0.2832,
      "step": 8460
    },
    {
      "epoch": 0.6080402010050251,
      "grad_norm": 3.5231618881225586,
      "learning_rate": 0.0001771875249262184,
      "loss": 0.4528,
      "step": 8470
    },
    {
      "epoch": 0.6087580760947595,
      "grad_norm": 5.990913391113281,
      "learning_rate": 0.0001771343489936455,
      "loss": 0.3571,
      "step": 8480
    },
    {
      "epoch": 0.6094759511844939,
      "grad_norm": 5.7816925048828125,
      "learning_rate": 0.00017708117306107257,
      "loss": 0.5214,
      "step": 8490
    },
    {
      "epoch": 0.6101938262742282,
      "grad_norm": 3.8822970390319824,
      "learning_rate": 0.00017702799712849965,
      "loss": 0.4173,
      "step": 8500
    },
    {
      "epoch": 0.6109117013639627,
      "grad_norm": 3.531802177429199,
      "learning_rate": 0.00017697482119592673,
      "loss": 0.1232,
      "step": 8510
    },
    {
      "epoch": 0.6116295764536971,
      "grad_norm": 15.26954174041748,
      "learning_rate": 0.0001769216452633538,
      "loss": 0.3826,
      "step": 8520
    },
    {
      "epoch": 0.6123474515434314,
      "grad_norm": 0.0872708112001419,
      "learning_rate": 0.0001768684693307809,
      "loss": 0.2799,
      "step": 8530
    },
    {
      "epoch": 0.6130653266331658,
      "grad_norm": 21.2775936126709,
      "learning_rate": 0.00017681529339820797,
      "loss": 0.4618,
      "step": 8540
    },
    {
      "epoch": 0.6137832017229002,
      "grad_norm": 1.7872984409332275,
      "learning_rate": 0.00017676211746563508,
      "loss": 0.3589,
      "step": 8550
    },
    {
      "epoch": 0.6145010768126346,
      "grad_norm": 2.4370806217193604,
      "learning_rate": 0.00017670894153306216,
      "loss": 0.2573,
      "step": 8560
    },
    {
      "epoch": 0.615218951902369,
      "grad_norm": 2.8897061347961426,
      "learning_rate": 0.0001766557656004892,
      "loss": 0.1819,
      "step": 8570
    },
    {
      "epoch": 0.6159368269921034,
      "grad_norm": 20.965139389038086,
      "learning_rate": 0.00017660258966791632,
      "loss": 0.2635,
      "step": 8580
    },
    {
      "epoch": 0.6166547020818378,
      "grad_norm": 1.378088116645813,
      "learning_rate": 0.0001765494137353434,
      "loss": 0.1021,
      "step": 8590
    },
    {
      "epoch": 0.6173725771715721,
      "grad_norm": 9.429272651672363,
      "learning_rate": 0.00017649623780277048,
      "loss": 0.5774,
      "step": 8600
    },
    {
      "epoch": 0.6180904522613065,
      "grad_norm": 8.38917064666748,
      "learning_rate": 0.00017644306187019755,
      "loss": 0.1957,
      "step": 8610
    },
    {
      "epoch": 0.618808327351041,
      "grad_norm": 13.814610481262207,
      "learning_rate": 0.00017638988593762463,
      "loss": 0.5308,
      "step": 8620
    },
    {
      "epoch": 0.6195262024407753,
      "grad_norm": 0.2344159632921219,
      "learning_rate": 0.00017633671000505171,
      "loss": 0.0539,
      "step": 8630
    },
    {
      "epoch": 0.6202440775305097,
      "grad_norm": 20.101890563964844,
      "learning_rate": 0.0001762835340724788,
      "loss": 0.4559,
      "step": 8640
    },
    {
      "epoch": 0.6209619526202441,
      "grad_norm": 4.236398696899414,
      "learning_rate": 0.0001762303581399059,
      "loss": 0.4035,
      "step": 8650
    },
    {
      "epoch": 0.6216798277099784,
      "grad_norm": 0.09215864539146423,
      "learning_rate": 0.00017617718220733298,
      "loss": 0.4344,
      "step": 8660
    },
    {
      "epoch": 0.6223977027997128,
      "grad_norm": 7.352196216583252,
      "learning_rate": 0.00017612400627476003,
      "loss": 0.1986,
      "step": 8670
    },
    {
      "epoch": 0.6231155778894473,
      "grad_norm": 9.239477157592773,
      "learning_rate": 0.00017607083034218714,
      "loss": 0.4006,
      "step": 8680
    },
    {
      "epoch": 0.6238334529791816,
      "grad_norm": 13.957605361938477,
      "learning_rate": 0.00017601765440961422,
      "loss": 0.2249,
      "step": 8690
    },
    {
      "epoch": 0.624551328068916,
      "grad_norm": 3.895437717437744,
      "learning_rate": 0.0001759644784770413,
      "loss": 0.5093,
      "step": 8700
    },
    {
      "epoch": 0.6252692031586504,
      "grad_norm": 0.627009928226471,
      "learning_rate": 0.0001759113025444684,
      "loss": 0.1144,
      "step": 8710
    },
    {
      "epoch": 0.6259870782483847,
      "grad_norm": 1.301298975944519,
      "learning_rate": 0.00017585812661189546,
      "loss": 0.174,
      "step": 8720
    },
    {
      "epoch": 0.6267049533381192,
      "grad_norm": 2.6652510166168213,
      "learning_rate": 0.00017580495067932254,
      "loss": 0.1827,
      "step": 8730
    },
    {
      "epoch": 0.6274228284278536,
      "grad_norm": 16.146312713623047,
      "learning_rate": 0.00017575177474674962,
      "loss": 0.2767,
      "step": 8740
    },
    {
      "epoch": 0.628140703517588,
      "grad_norm": 15.8358736038208,
      "learning_rate": 0.00017569859881417672,
      "loss": 0.1878,
      "step": 8750
    },
    {
      "epoch": 0.6288585786073223,
      "grad_norm": 0.05401540547609329,
      "learning_rate": 0.0001756454228816038,
      "loss": 0.2466,
      "step": 8760
    },
    {
      "epoch": 0.6295764536970567,
      "grad_norm": 0.6656820178031921,
      "learning_rate": 0.00017559224694903088,
      "loss": 0.2937,
      "step": 8770
    },
    {
      "epoch": 0.6302943287867911,
      "grad_norm": 2.74638295173645,
      "learning_rate": 0.00017553907101645796,
      "loss": 0.4342,
      "step": 8780
    },
    {
      "epoch": 0.6310122038765255,
      "grad_norm": 12.01982307434082,
      "learning_rate": 0.00017548589508388504,
      "loss": 0.5275,
      "step": 8790
    },
    {
      "epoch": 0.6317300789662599,
      "grad_norm": 3.6108109951019287,
      "learning_rate": 0.00017543271915131212,
      "loss": 0.3576,
      "step": 8800
    },
    {
      "epoch": 0.6324479540559943,
      "grad_norm": 3.881333112716675,
      "learning_rate": 0.00017537954321873923,
      "loss": 0.5669,
      "step": 8810
    },
    {
      "epoch": 0.6331658291457286,
      "grad_norm": 0.2610112428665161,
      "learning_rate": 0.0001753263672861663,
      "loss": 0.2433,
      "step": 8820
    },
    {
      "epoch": 0.633883704235463,
      "grad_norm": 5.344810485839844,
      "learning_rate": 0.00017527319135359336,
      "loss": 0.2854,
      "step": 8830
    },
    {
      "epoch": 0.6346015793251975,
      "grad_norm": 2.7879090309143066,
      "learning_rate": 0.00017522001542102044,
      "loss": 0.311,
      "step": 8840
    },
    {
      "epoch": 0.6353194544149318,
      "grad_norm": 1.6339541673660278,
      "learning_rate": 0.00017516683948844754,
      "loss": 0.2121,
      "step": 8850
    },
    {
      "epoch": 0.6360373295046662,
      "grad_norm": 26.39142417907715,
      "learning_rate": 0.00017511366355587462,
      "loss": 0.3328,
      "step": 8860
    },
    {
      "epoch": 0.6367552045944006,
      "grad_norm": 8.974946975708008,
      "learning_rate": 0.0001750604876233017,
      "loss": 0.2577,
      "step": 8870
    },
    {
      "epoch": 0.6374730796841349,
      "grad_norm": 11.471726417541504,
      "learning_rate": 0.00017500731169072878,
      "loss": 0.2782,
      "step": 8880
    },
    {
      "epoch": 0.6381909547738693,
      "grad_norm": 10.824934959411621,
      "learning_rate": 0.00017495413575815586,
      "loss": 0.1885,
      "step": 8890
    },
    {
      "epoch": 0.6389088298636038,
      "grad_norm": 19.080371856689453,
      "learning_rate": 0.00017490095982558294,
      "loss": 0.2191,
      "step": 8900
    },
    {
      "epoch": 0.6396267049533381,
      "grad_norm": 11.358440399169922,
      "learning_rate": 0.00017484778389301005,
      "loss": 0.2705,
      "step": 8910
    },
    {
      "epoch": 0.6403445800430725,
      "grad_norm": 11.22589111328125,
      "learning_rate": 0.00017479460796043713,
      "loss": 0.2882,
      "step": 8920
    },
    {
      "epoch": 0.6410624551328069,
      "grad_norm": 8.899341583251953,
      "learning_rate": 0.0001747414320278642,
      "loss": 0.4708,
      "step": 8930
    },
    {
      "epoch": 0.6417803302225413,
      "grad_norm": 0.7948843240737915,
      "learning_rate": 0.00017468825609529126,
      "loss": 0.2256,
      "step": 8940
    },
    {
      "epoch": 0.6424982053122756,
      "grad_norm": 12.29107666015625,
      "learning_rate": 0.00017463508016271837,
      "loss": 0.2229,
      "step": 8950
    },
    {
      "epoch": 0.6432160804020101,
      "grad_norm": 0.3914202153682709,
      "learning_rate": 0.00017458190423014545,
      "loss": 0.2854,
      "step": 8960
    },
    {
      "epoch": 0.6439339554917445,
      "grad_norm": 0.23056747019290924,
      "learning_rate": 0.00017452872829757253,
      "loss": 0.3485,
      "step": 8970
    },
    {
      "epoch": 0.6446518305814788,
      "grad_norm": 11.435912132263184,
      "learning_rate": 0.0001744755523649996,
      "loss": 0.4086,
      "step": 8980
    },
    {
      "epoch": 0.6453697056712132,
      "grad_norm": 11.007671356201172,
      "learning_rate": 0.00017442237643242668,
      "loss": 0.4609,
      "step": 8990
    },
    {
      "epoch": 0.6460875807609476,
      "grad_norm": 21.965883255004883,
      "learning_rate": 0.00017436920049985376,
      "loss": 0.2957,
      "step": 9000
    },
    {
      "epoch": 0.6468054558506819,
      "grad_norm": 18.765323638916016,
      "learning_rate": 0.00017431602456728087,
      "loss": 0.1758,
      "step": 9010
    },
    {
      "epoch": 0.6475233309404164,
      "grad_norm": 14.802559852600098,
      "learning_rate": 0.00017426284863470795,
      "loss": 0.3224,
      "step": 9020
    },
    {
      "epoch": 0.6482412060301508,
      "grad_norm": 15.835829734802246,
      "learning_rate": 0.00017420967270213503,
      "loss": 0.3578,
      "step": 9030
    },
    {
      "epoch": 0.6489590811198851,
      "grad_norm": 13.060470581054688,
      "learning_rate": 0.00017415649676956208,
      "loss": 0.3731,
      "step": 9040
    },
    {
      "epoch": 0.6496769562096195,
      "grad_norm": 4.752431869506836,
      "learning_rate": 0.0001741033208369892,
      "loss": 0.1508,
      "step": 9050
    },
    {
      "epoch": 0.6503948312993539,
      "grad_norm": 0.06818567961454391,
      "learning_rate": 0.00017405014490441627,
      "loss": 0.2563,
      "step": 9060
    },
    {
      "epoch": 0.6511127063890882,
      "grad_norm": 14.850653648376465,
      "learning_rate": 0.00017399696897184335,
      "loss": 0.3196,
      "step": 9070
    },
    {
      "epoch": 0.6518305814788227,
      "grad_norm": 8.61403751373291,
      "learning_rate": 0.00017394379303927045,
      "loss": 0.5296,
      "step": 9080
    },
    {
      "epoch": 0.6525484565685571,
      "grad_norm": 2.675006628036499,
      "learning_rate": 0.0001738906171066975,
      "loss": 0.4305,
      "step": 9090
    },
    {
      "epoch": 0.6532663316582915,
      "grad_norm": 4.083374500274658,
      "learning_rate": 0.00017383744117412459,
      "loss": 0.1122,
      "step": 9100
    },
    {
      "epoch": 0.6539842067480258,
      "grad_norm": 5.363386154174805,
      "learning_rate": 0.0001737842652415517,
      "loss": 0.3324,
      "step": 9110
    },
    {
      "epoch": 0.6547020818377602,
      "grad_norm": 2.5468688011169434,
      "learning_rate": 0.00017373108930897877,
      "loss": 0.2472,
      "step": 9120
    },
    {
      "epoch": 0.6554199569274947,
      "grad_norm": 10.452987670898438,
      "learning_rate": 0.00017367791337640585,
      "loss": 0.2694,
      "step": 9130
    },
    {
      "epoch": 0.656137832017229,
      "grad_norm": 23.29120445251465,
      "learning_rate": 0.00017362473744383293,
      "loss": 0.5791,
      "step": 9140
    },
    {
      "epoch": 0.6568557071069634,
      "grad_norm": 12.81174087524414,
      "learning_rate": 0.00017357156151126,
      "loss": 0.7999,
      "step": 9150
    },
    {
      "epoch": 0.6575735821966978,
      "grad_norm": 2.074673891067505,
      "learning_rate": 0.0001735183855786871,
      "loss": 0.3818,
      "step": 9160
    },
    {
      "epoch": 0.6582914572864321,
      "grad_norm": 6.238882541656494,
      "learning_rate": 0.00017346520964611417,
      "loss": 0.3501,
      "step": 9170
    },
    {
      "epoch": 0.6590093323761665,
      "grad_norm": 2.1450181007385254,
      "learning_rate": 0.00017341203371354128,
      "loss": 0.2246,
      "step": 9180
    },
    {
      "epoch": 0.659727207465901,
      "grad_norm": 4.229192733764648,
      "learning_rate": 0.00017335885778096836,
      "loss": 0.5727,
      "step": 9190
    },
    {
      "epoch": 0.6604450825556353,
      "grad_norm": 1.4049838781356812,
      "learning_rate": 0.0001733056818483954,
      "loss": 0.2672,
      "step": 9200
    },
    {
      "epoch": 0.6611629576453697,
      "grad_norm": 14.79945182800293,
      "learning_rate": 0.00017325250591582251,
      "loss": 0.3411,
      "step": 9210
    },
    {
      "epoch": 0.6618808327351041,
      "grad_norm": 0.026502300053834915,
      "learning_rate": 0.0001731993299832496,
      "loss": 0.2323,
      "step": 9220
    },
    {
      "epoch": 0.6625987078248384,
      "grad_norm": 19.3157958984375,
      "learning_rate": 0.00017314615405067667,
      "loss": 0.5089,
      "step": 9230
    },
    {
      "epoch": 0.6633165829145728,
      "grad_norm": 10.986977577209473,
      "learning_rate": 0.00017309297811810375,
      "loss": 0.3236,
      "step": 9240
    },
    {
      "epoch": 0.6640344580043073,
      "grad_norm": 0.16828089952468872,
      "learning_rate": 0.00017303980218553083,
      "loss": 0.4155,
      "step": 9250
    },
    {
      "epoch": 0.6647523330940417,
      "grad_norm": 12.03813362121582,
      "learning_rate": 0.0001729866262529579,
      "loss": 0.0628,
      "step": 9260
    },
    {
      "epoch": 0.665470208183776,
      "grad_norm": 23.05914878845215,
      "learning_rate": 0.000172933450320385,
      "loss": 0.4246,
      "step": 9270
    },
    {
      "epoch": 0.6661880832735104,
      "grad_norm": 7.084920883178711,
      "learning_rate": 0.0001728802743878121,
      "loss": 0.4404,
      "step": 9280
    },
    {
      "epoch": 0.6669059583632448,
      "grad_norm": 9.730473518371582,
      "learning_rate": 0.00017282709845523918,
      "loss": 0.2492,
      "step": 9290
    },
    {
      "epoch": 0.6676238334529792,
      "grad_norm": 0.41035106778144836,
      "learning_rate": 0.00017277392252266623,
      "loss": 0.2321,
      "step": 9300
    },
    {
      "epoch": 0.6683417085427136,
      "grad_norm": 7.662316799163818,
      "learning_rate": 0.00017272074659009334,
      "loss": 0.2501,
      "step": 9310
    },
    {
      "epoch": 0.669059583632448,
      "grad_norm": 17.971271514892578,
      "learning_rate": 0.00017266757065752042,
      "loss": 0.4066,
      "step": 9320
    },
    {
      "epoch": 0.6697774587221823,
      "grad_norm": 13.041863441467285,
      "learning_rate": 0.0001726143947249475,
      "loss": 0.3341,
      "step": 9330
    },
    {
      "epoch": 0.6704953338119167,
      "grad_norm": 1.6810587644577026,
      "learning_rate": 0.0001725612187923746,
      "loss": 0.4079,
      "step": 9340
    },
    {
      "epoch": 0.6712132089016511,
      "grad_norm": 1.6203476190567017,
      "learning_rate": 0.00017250804285980165,
      "loss": 0.1534,
      "step": 9350
    },
    {
      "epoch": 0.6719310839913855,
      "grad_norm": 8.109549522399902,
      "learning_rate": 0.00017245486692722873,
      "loss": 0.5256,
      "step": 9360
    },
    {
      "epoch": 0.6726489590811199,
      "grad_norm": 4.645214557647705,
      "learning_rate": 0.0001724016909946558,
      "loss": 0.2891,
      "step": 9370
    },
    {
      "epoch": 0.6733668341708543,
      "grad_norm": 9.033782958984375,
      "learning_rate": 0.00017234851506208292,
      "loss": 0.2584,
      "step": 9380
    },
    {
      "epoch": 0.6740847092605886,
      "grad_norm": 4.921586513519287,
      "learning_rate": 0.00017229533912951,
      "loss": 0.1369,
      "step": 9390
    },
    {
      "epoch": 0.674802584350323,
      "grad_norm": 13.187591552734375,
      "learning_rate": 0.00017224216319693708,
      "loss": 0.6204,
      "step": 9400
    },
    {
      "epoch": 0.6755204594400575,
      "grad_norm": 6.703019142150879,
      "learning_rate": 0.00017218898726436416,
      "loss": 0.1844,
      "step": 9410
    },
    {
      "epoch": 0.6762383345297918,
      "grad_norm": 9.190531730651855,
      "learning_rate": 0.00017213581133179124,
      "loss": 0.4002,
      "step": 9420
    },
    {
      "epoch": 0.6769562096195262,
      "grad_norm": 2.5559751987457275,
      "learning_rate": 0.00017208263539921832,
      "loss": 0.2541,
      "step": 9430
    },
    {
      "epoch": 0.6776740847092606,
      "grad_norm": 11.00485610961914,
      "learning_rate": 0.00017202945946664542,
      "loss": 0.2789,
      "step": 9440
    },
    {
      "epoch": 0.678391959798995,
      "grad_norm": 15.722553253173828,
      "learning_rate": 0.0001719762835340725,
      "loss": 0.4058,
      "step": 9450
    },
    {
      "epoch": 0.6791098348887293,
      "grad_norm": 0.06710057705640793,
      "learning_rate": 0.00017192310760149956,
      "loss": 0.1885,
      "step": 9460
    },
    {
      "epoch": 0.6798277099784638,
      "grad_norm": 15.078625679016113,
      "learning_rate": 0.00017186993166892664,
      "loss": 0.3522,
      "step": 9470
    },
    {
      "epoch": 0.6805455850681982,
      "grad_norm": 6.6453423500061035,
      "learning_rate": 0.00017181675573635374,
      "loss": 0.5285,
      "step": 9480
    },
    {
      "epoch": 0.6812634601579325,
      "grad_norm": 0.9451792240142822,
      "learning_rate": 0.00017176357980378082,
      "loss": 0.3903,
      "step": 9490
    },
    {
      "epoch": 0.6819813352476669,
      "grad_norm": 0.6865790486335754,
      "learning_rate": 0.0001717104038712079,
      "loss": 0.2564,
      "step": 9500
    },
    {
      "epoch": 0.6826992103374013,
      "grad_norm": 4.5422539710998535,
      "learning_rate": 0.00017165722793863498,
      "loss": 0.1851,
      "step": 9510
    },
    {
      "epoch": 0.6834170854271356,
      "grad_norm": 16.847719192504883,
      "learning_rate": 0.00017160405200606206,
      "loss": 0.3203,
      "step": 9520
    },
    {
      "epoch": 0.6841349605168701,
      "grad_norm": 6.745655536651611,
      "learning_rate": 0.00017155087607348914,
      "loss": 0.1283,
      "step": 9530
    },
    {
      "epoch": 0.6848528356066045,
      "grad_norm": 14.901955604553223,
      "learning_rate": 0.00017149770014091625,
      "loss": 0.4156,
      "step": 9540
    },
    {
      "epoch": 0.6855707106963388,
      "grad_norm": 15.179346084594727,
      "learning_rate": 0.00017144452420834333,
      "loss": 0.554,
      "step": 9550
    },
    {
      "epoch": 0.6862885857860732,
      "grad_norm": 4.059324264526367,
      "learning_rate": 0.00017139134827577038,
      "loss": 0.5139,
      "step": 9560
    },
    {
      "epoch": 0.6870064608758076,
      "grad_norm": 33.413963317871094,
      "learning_rate": 0.00017133817234319746,
      "loss": 0.2614,
      "step": 9570
    },
    {
      "epoch": 0.6877243359655419,
      "grad_norm": 23.94222640991211,
      "learning_rate": 0.00017128499641062456,
      "loss": 0.2431,
      "step": 9580
    },
    {
      "epoch": 0.6884422110552764,
      "grad_norm": 0.1288120448589325,
      "learning_rate": 0.00017123182047805164,
      "loss": 0.301,
      "step": 9590
    },
    {
      "epoch": 0.6891600861450108,
      "grad_norm": 4.691328525543213,
      "learning_rate": 0.00017117864454547872,
      "loss": 0.2936,
      "step": 9600
    },
    {
      "epoch": 0.6898779612347452,
      "grad_norm": 2.1188066005706787,
      "learning_rate": 0.0001711254686129058,
      "loss": 0.4494,
      "step": 9610
    },
    {
      "epoch": 0.6905958363244795,
      "grad_norm": 0.46536195278167725,
      "learning_rate": 0.00017107229268033288,
      "loss": 0.42,
      "step": 9620
    },
    {
      "epoch": 0.6913137114142139,
      "grad_norm": 1.6674171686172485,
      "learning_rate": 0.00017101911674775996,
      "loss": 0.4924,
      "step": 9630
    },
    {
      "epoch": 0.6920315865039484,
      "grad_norm": 20.113502502441406,
      "learning_rate": 0.00017096594081518707,
      "loss": 0.4594,
      "step": 9640
    },
    {
      "epoch": 0.6927494615936827,
      "grad_norm": 0.12894511222839355,
      "learning_rate": 0.00017091276488261415,
      "loss": 0.2618,
      "step": 9650
    },
    {
      "epoch": 0.6934673366834171,
      "grad_norm": 0.839422345161438,
      "learning_rate": 0.00017085958895004123,
      "loss": 0.2651,
      "step": 9660
    },
    {
      "epoch": 0.6941852117731515,
      "grad_norm": 12.465484619140625,
      "learning_rate": 0.00017080641301746828,
      "loss": 0.3123,
      "step": 9670
    },
    {
      "epoch": 0.6949030868628858,
      "grad_norm": 0.12127242237329483,
      "learning_rate": 0.00017075323708489539,
      "loss": 0.0838,
      "step": 9680
    },
    {
      "epoch": 0.6956209619526202,
      "grad_norm": 4.888281345367432,
      "learning_rate": 0.00017070006115232247,
      "loss": 0.0755,
      "step": 9690
    },
    {
      "epoch": 0.6963388370423547,
      "grad_norm": 15.558061599731445,
      "learning_rate": 0.00017064688521974955,
      "loss": 0.5429,
      "step": 9700
    },
    {
      "epoch": 0.697056712132089,
      "grad_norm": 15.277076721191406,
      "learning_rate": 0.00017059370928717665,
      "loss": 0.427,
      "step": 9710
    },
    {
      "epoch": 0.6977745872218234,
      "grad_norm": 7.524124622344971,
      "learning_rate": 0.0001705405333546037,
      "loss": 0.346,
      "step": 9720
    },
    {
      "epoch": 0.6984924623115578,
      "grad_norm": 17.153005599975586,
      "learning_rate": 0.00017048735742203078,
      "loss": 0.2243,
      "step": 9730
    },
    {
      "epoch": 0.6992103374012921,
      "grad_norm": 2.3564720153808594,
      "learning_rate": 0.0001704341814894579,
      "loss": 0.5925,
      "step": 9740
    },
    {
      "epoch": 0.6999282124910265,
      "grad_norm": 14.127296447753906,
      "learning_rate": 0.00017038100555688497,
      "loss": 0.3387,
      "step": 9750
    },
    {
      "epoch": 0.700646087580761,
      "grad_norm": 12.44150161743164,
      "learning_rate": 0.00017032782962431205,
      "loss": 0.2613,
      "step": 9760
    },
    {
      "epoch": 0.7013639626704954,
      "grad_norm": 22.353017807006836,
      "learning_rate": 0.00017027465369173913,
      "loss": 0.2157,
      "step": 9770
    },
    {
      "epoch": 0.7020818377602297,
      "grad_norm": 12.9564790725708,
      "learning_rate": 0.0001702214777591662,
      "loss": 0.2956,
      "step": 9780
    },
    {
      "epoch": 0.7027997128499641,
      "grad_norm": 10.202808380126953,
      "learning_rate": 0.0001701683018265933,
      "loss": 0.2218,
      "step": 9790
    },
    {
      "epoch": 0.7035175879396985,
      "grad_norm": 1.9493613243103027,
      "learning_rate": 0.00017011512589402037,
      "loss": 0.2596,
      "step": 9800
    },
    {
      "epoch": 0.7042354630294329,
      "grad_norm": 1.459073543548584,
      "learning_rate": 0.00017006194996144747,
      "loss": 0.1115,
      "step": 9810
    },
    {
      "epoch": 0.7049533381191673,
      "grad_norm": 12.76464557647705,
      "learning_rate": 0.00017000877402887455,
      "loss": 0.1781,
      "step": 9820
    },
    {
      "epoch": 0.7056712132089017,
      "grad_norm": 7.408548355102539,
      "learning_rate": 0.0001699555980963016,
      "loss": 0.2085,
      "step": 9830
    },
    {
      "epoch": 0.706389088298636,
      "grad_norm": 0.5971412062644958,
      "learning_rate": 0.0001699024221637287,
      "loss": 0.0933,
      "step": 9840
    },
    {
      "epoch": 0.7071069633883704,
      "grad_norm": 5.762805461883545,
      "learning_rate": 0.0001698492462311558,
      "loss": 0.2357,
      "step": 9850
    },
    {
      "epoch": 0.7078248384781048,
      "grad_norm": 10.723072052001953,
      "learning_rate": 0.00016979607029858287,
      "loss": 0.1923,
      "step": 9860
    },
    {
      "epoch": 0.7085427135678392,
      "grad_norm": 11.057768821716309,
      "learning_rate": 0.00016974289436600995,
      "loss": 0.2975,
      "step": 9870
    },
    {
      "epoch": 0.7092605886575736,
      "grad_norm": 2.699977397918701,
      "learning_rate": 0.00016968971843343703,
      "loss": 0.1278,
      "step": 9880
    },
    {
      "epoch": 0.709978463747308,
      "grad_norm": 5.196922302246094,
      "learning_rate": 0.0001696365425008641,
      "loss": 0.131,
      "step": 9890
    },
    {
      "epoch": 0.7106963388370423,
      "grad_norm": 9.06864070892334,
      "learning_rate": 0.0001695833665682912,
      "loss": 0.4398,
      "step": 9900
    },
    {
      "epoch": 0.7114142139267767,
      "grad_norm": 0.56918865442276,
      "learning_rate": 0.0001695301906357183,
      "loss": 0.2827,
      "step": 9910
    },
    {
      "epoch": 0.7121320890165111,
      "grad_norm": 11.488222122192383,
      "learning_rate": 0.00016947701470314538,
      "loss": 0.3381,
      "step": 9920
    },
    {
      "epoch": 0.7128499641062455,
      "grad_norm": 11.085681915283203,
      "learning_rate": 0.00016942383877057243,
      "loss": 0.1728,
      "step": 9930
    },
    {
      "epoch": 0.7135678391959799,
      "grad_norm": 1.680679202079773,
      "learning_rate": 0.00016937066283799953,
      "loss": 0.2366,
      "step": 9940
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 4.357662200927734,
      "learning_rate": 0.00016931748690542661,
      "loss": 0.3023,
      "step": 9950
    },
    {
      "epoch": 0.7150035893754487,
      "grad_norm": 1.1868274211883545,
      "learning_rate": 0.0001692643109728537,
      "loss": 0.2842,
      "step": 9960
    },
    {
      "epoch": 0.715721464465183,
      "grad_norm": 0.8082106113433838,
      "learning_rate": 0.00016921113504028077,
      "loss": 0.1947,
      "step": 9970
    },
    {
      "epoch": 0.7164393395549175,
      "grad_norm": 17.863683700561523,
      "learning_rate": 0.00016915795910770785,
      "loss": 0.4781,
      "step": 9980
    },
    {
      "epoch": 0.7171572146446519,
      "grad_norm": 0.3033179044723511,
      "learning_rate": 0.00016910478317513493,
      "loss": 0.1708,
      "step": 9990
    },
    {
      "epoch": 0.7178750897343862,
      "grad_norm": 17.916526794433594,
      "learning_rate": 0.000169051607242562,
      "loss": 0.4133,
      "step": 10000
    },
    {
      "epoch": 0.7185929648241206,
      "grad_norm": 4.150047779083252,
      "learning_rate": 0.00016899843130998912,
      "loss": 0.2604,
      "step": 10010
    },
    {
      "epoch": 0.719310839913855,
      "grad_norm": 12.317670822143555,
      "learning_rate": 0.0001689452553774162,
      "loss": 0.119,
      "step": 10020
    },
    {
      "epoch": 0.7200287150035893,
      "grad_norm": 5.361465930938721,
      "learning_rate": 0.00016889207944484328,
      "loss": 0.1905,
      "step": 10030
    },
    {
      "epoch": 0.7207465900933238,
      "grad_norm": 5.370115756988525,
      "learning_rate": 0.00016883890351227036,
      "loss": 0.1183,
      "step": 10040
    },
    {
      "epoch": 0.7214644651830582,
      "grad_norm": 3.569206714630127,
      "learning_rate": 0.00016878572757969744,
      "loss": 0.258,
      "step": 10050
    },
    {
      "epoch": 0.7221823402727925,
      "grad_norm": 8.965453147888184,
      "learning_rate": 0.00016873255164712452,
      "loss": 0.2644,
      "step": 10060
    },
    {
      "epoch": 0.7229002153625269,
      "grad_norm": 15.652944564819336,
      "learning_rate": 0.00016867937571455162,
      "loss": 0.1926,
      "step": 10070
    },
    {
      "epoch": 0.7236180904522613,
      "grad_norm": 0.3885261118412018,
      "learning_rate": 0.0001686261997819787,
      "loss": 0.3527,
      "step": 10080
    },
    {
      "epoch": 0.7243359655419956,
      "grad_norm": 4.314914226531982,
      "learning_rate": 0.00016857302384940575,
      "loss": 0.2357,
      "step": 10090
    },
    {
      "epoch": 0.7250538406317301,
      "grad_norm": 7.0363569259643555,
      "learning_rate": 0.00016851984791683283,
      "loss": 0.4186,
      "step": 10100
    },
    {
      "epoch": 0.7257717157214645,
      "grad_norm": 7.2662739753723145,
      "learning_rate": 0.00016846667198425994,
      "loss": 0.3843,
      "step": 10110
    },
    {
      "epoch": 0.7264895908111989,
      "grad_norm": 0.02892650105059147,
      "learning_rate": 0.00016841349605168702,
      "loss": 0.2342,
      "step": 10120
    },
    {
      "epoch": 0.7272074659009332,
      "grad_norm": 20.868518829345703,
      "learning_rate": 0.0001683603201191141,
      "loss": 0.3611,
      "step": 10130
    },
    {
      "epoch": 0.7279253409906676,
      "grad_norm": 19.57709503173828,
      "learning_rate": 0.00016830714418654118,
      "loss": 0.2097,
      "step": 10140
    },
    {
      "epoch": 0.7286432160804021,
      "grad_norm": 20.6524715423584,
      "learning_rate": 0.00016825396825396826,
      "loss": 0.5084,
      "step": 10150
    },
    {
      "epoch": 0.7293610911701364,
      "grad_norm": 8.146682739257812,
      "learning_rate": 0.00016820079232139534,
      "loss": 0.2497,
      "step": 10160
    },
    {
      "epoch": 0.7300789662598708,
      "grad_norm": 5.258018493652344,
      "learning_rate": 0.00016814761638882244,
      "loss": 0.397,
      "step": 10170
    },
    {
      "epoch": 0.7307968413496052,
      "grad_norm": 0.6661662459373474,
      "learning_rate": 0.00016809444045624952,
      "loss": 0.2193,
      "step": 10180
    },
    {
      "epoch": 0.7315147164393395,
      "grad_norm": 0.4245293140411377,
      "learning_rate": 0.00016804126452367658,
      "loss": 0.2963,
      "step": 10190
    },
    {
      "epoch": 0.7322325915290739,
      "grad_norm": 7.470398902893066,
      "learning_rate": 0.00016798808859110366,
      "loss": 0.2497,
      "step": 10200
    },
    {
      "epoch": 0.7329504666188084,
      "grad_norm": 0.04798496887087822,
      "learning_rate": 0.00016793491265853076,
      "loss": 0.3504,
      "step": 10210
    },
    {
      "epoch": 0.7336683417085427,
      "grad_norm": 1.9325610399246216,
      "learning_rate": 0.00016788173672595784,
      "loss": 0.1088,
      "step": 10220
    },
    {
      "epoch": 0.7343862167982771,
      "grad_norm": 1.0514167547225952,
      "learning_rate": 0.00016782856079338492,
      "loss": 0.1653,
      "step": 10230
    },
    {
      "epoch": 0.7351040918880115,
      "grad_norm": 11.179141998291016,
      "learning_rate": 0.000167775384860812,
      "loss": 0.5685,
      "step": 10240
    },
    {
      "epoch": 0.7358219669777458,
      "grad_norm": 0.026947498321533203,
      "learning_rate": 0.00016772220892823908,
      "loss": 0.3112,
      "step": 10250
    },
    {
      "epoch": 0.7365398420674802,
      "grad_norm": 16.75489044189453,
      "learning_rate": 0.00016766903299566616,
      "loss": 0.2339,
      "step": 10260
    },
    {
      "epoch": 0.7372577171572147,
      "grad_norm": 0.3937341868877411,
      "learning_rate": 0.00016761585706309327,
      "loss": 0.0946,
      "step": 10270
    },
    {
      "epoch": 0.7379755922469491,
      "grad_norm": 8.581510543823242,
      "learning_rate": 0.00016756268113052035,
      "loss": 0.2093,
      "step": 10280
    },
    {
      "epoch": 0.7386934673366834,
      "grad_norm": 9.7059965133667,
      "learning_rate": 0.00016750950519794743,
      "loss": 0.169,
      "step": 10290
    },
    {
      "epoch": 0.7394113424264178,
      "grad_norm": 23.468055725097656,
      "learning_rate": 0.00016745632926537448,
      "loss": 0.4583,
      "step": 10300
    },
    {
      "epoch": 0.7401292175161522,
      "grad_norm": 4.011721611022949,
      "learning_rate": 0.00016740315333280158,
      "loss": 0.2465,
      "step": 10310
    },
    {
      "epoch": 0.7408470926058865,
      "grad_norm": 17.61904525756836,
      "learning_rate": 0.00016734997740022866,
      "loss": 0.2794,
      "step": 10320
    },
    {
      "epoch": 0.741564967695621,
      "grad_norm": 20.052608489990234,
      "learning_rate": 0.00016729680146765574,
      "loss": 0.2064,
      "step": 10330
    },
    {
      "epoch": 0.7422828427853554,
      "grad_norm": 25.37265396118164,
      "learning_rate": 0.00016724362553508285,
      "loss": 0.5499,
      "step": 10340
    },
    {
      "epoch": 0.7430007178750897,
      "grad_norm": 25.916501998901367,
      "learning_rate": 0.0001671904496025099,
      "loss": 0.4715,
      "step": 10350
    },
    {
      "epoch": 0.7437185929648241,
      "grad_norm": 22.182435989379883,
      "learning_rate": 0.00016713727366993698,
      "loss": 0.465,
      "step": 10360
    },
    {
      "epoch": 0.7444364680545585,
      "grad_norm": 0.13431422412395477,
      "learning_rate": 0.0001670840977373641,
      "loss": 0.0782,
      "step": 10370
    },
    {
      "epoch": 0.7451543431442929,
      "grad_norm": 13.701704978942871,
      "learning_rate": 0.00016703092180479117,
      "loss": 0.4703,
      "step": 10380
    },
    {
      "epoch": 0.7458722182340273,
      "grad_norm": 0.8262835144996643,
      "learning_rate": 0.00016697774587221825,
      "loss": 0.5642,
      "step": 10390
    },
    {
      "epoch": 0.7465900933237617,
      "grad_norm": 13.975122451782227,
      "learning_rate": 0.00016692456993964533,
      "loss": 0.2477,
      "step": 10400
    },
    {
      "epoch": 0.747307968413496,
      "grad_norm": 1.3679217100143433,
      "learning_rate": 0.0001668713940070724,
      "loss": 0.2274,
      "step": 10410
    },
    {
      "epoch": 0.7480258435032304,
      "grad_norm": 9.82023811340332,
      "learning_rate": 0.00016681821807449949,
      "loss": 0.2084,
      "step": 10420
    },
    {
      "epoch": 0.7487437185929648,
      "grad_norm": 11.815079689025879,
      "learning_rate": 0.00016676504214192657,
      "loss": 0.2365,
      "step": 10430
    },
    {
      "epoch": 0.7494615936826992,
      "grad_norm": 6.432394027709961,
      "learning_rate": 0.00016671186620935367,
      "loss": 0.185,
      "step": 10440
    },
    {
      "epoch": 0.7501794687724336,
      "grad_norm": 1.448486566543579,
      "learning_rate": 0.00016665869027678072,
      "loss": 0.192,
      "step": 10450
    },
    {
      "epoch": 0.750897343862168,
      "grad_norm": 2.3427894115448,
      "learning_rate": 0.0001666055143442078,
      "loss": 0.2742,
      "step": 10460
    },
    {
      "epoch": 0.7516152189519024,
      "grad_norm": 3.1842308044433594,
      "learning_rate": 0.0001665523384116349,
      "loss": 0.2815,
      "step": 10470
    },
    {
      "epoch": 0.7523330940416367,
      "grad_norm": 7.577178478240967,
      "learning_rate": 0.000166499162479062,
      "loss": 0.0523,
      "step": 10480
    },
    {
      "epoch": 0.7530509691313712,
      "grad_norm": 13.25500774383545,
      "learning_rate": 0.00016644598654648907,
      "loss": 0.2026,
      "step": 10490
    },
    {
      "epoch": 0.7537688442211056,
      "grad_norm": 10.54909896850586,
      "learning_rate": 0.00016639281061391615,
      "loss": 0.4853,
      "step": 10500
    },
    {
      "epoch": 0.7544867193108399,
      "grad_norm": 1.1639049053192139,
      "learning_rate": 0.00016633963468134323,
      "loss": 0.3975,
      "step": 10510
    },
    {
      "epoch": 0.7552045944005743,
      "grad_norm": 1.181279182434082,
      "learning_rate": 0.0001662864587487703,
      "loss": 0.2391,
      "step": 10520
    },
    {
      "epoch": 0.7559224694903087,
      "grad_norm": 1.8679962158203125,
      "learning_rate": 0.0001662332828161974,
      "loss": 0.4084,
      "step": 10530
    },
    {
      "epoch": 0.756640344580043,
      "grad_norm": 22.061614990234375,
      "learning_rate": 0.0001661801068836245,
      "loss": 0.3736,
      "step": 10540
    },
    {
      "epoch": 0.7573582196697775,
      "grad_norm": 0.05299094319343567,
      "learning_rate": 0.00016612693095105157,
      "loss": 0.28,
      "step": 10550
    },
    {
      "epoch": 0.7580760947595119,
      "grad_norm": 19.594947814941406,
      "learning_rate": 0.00016607375501847863,
      "loss": 0.4271,
      "step": 10560
    },
    {
      "epoch": 0.7587939698492462,
      "grad_norm": 6.35526704788208,
      "learning_rate": 0.00016602057908590573,
      "loss": 0.2011,
      "step": 10570
    },
    {
      "epoch": 0.7595118449389806,
      "grad_norm": 3.8330681324005127,
      "learning_rate": 0.0001659674031533328,
      "loss": 0.1771,
      "step": 10580
    },
    {
      "epoch": 0.760229720028715,
      "grad_norm": 21.09161949157715,
      "learning_rate": 0.0001659142272207599,
      "loss": 0.2717,
      "step": 10590
    },
    {
      "epoch": 0.7609475951184493,
      "grad_norm": 4.571539402008057,
      "learning_rate": 0.00016586105128818697,
      "loss": 0.4205,
      "step": 10600
    },
    {
      "epoch": 0.7616654702081838,
      "grad_norm": 11.47598648071289,
      "learning_rate": 0.00016580787535561405,
      "loss": 0.2247,
      "step": 10610
    },
    {
      "epoch": 0.7623833452979182,
      "grad_norm": 18.29061508178711,
      "learning_rate": 0.00016575469942304113,
      "loss": 0.3489,
      "step": 10620
    },
    {
      "epoch": 0.7631012203876526,
      "grad_norm": 13.345130920410156,
      "learning_rate": 0.0001657015234904682,
      "loss": 0.3142,
      "step": 10630
    },
    {
      "epoch": 0.7638190954773869,
      "grad_norm": 15.90650749206543,
      "learning_rate": 0.00016564834755789532,
      "loss": 0.4686,
      "step": 10640
    },
    {
      "epoch": 0.7645369705671213,
      "grad_norm": 1.2461796998977661,
      "learning_rate": 0.0001655951716253224,
      "loss": 0.2874,
      "step": 10650
    },
    {
      "epoch": 0.7652548456568558,
      "grad_norm": 15.703997611999512,
      "learning_rate": 0.00016554199569274948,
      "loss": 0.2398,
      "step": 10660
    },
    {
      "epoch": 0.7659727207465901,
      "grad_norm": 0.14044752717018127,
      "learning_rate": 0.00016548881976017655,
      "loss": 0.1602,
      "step": 10670
    },
    {
      "epoch": 0.7666905958363245,
      "grad_norm": 6.092559337615967,
      "learning_rate": 0.00016543564382760363,
      "loss": 0.3304,
      "step": 10680
    },
    {
      "epoch": 0.7674084709260589,
      "grad_norm": 10.849255561828613,
      "learning_rate": 0.00016538246789503071,
      "loss": 0.2,
      "step": 10690
    },
    {
      "epoch": 0.7681263460157932,
      "grad_norm": 1.5749748945236206,
      "learning_rate": 0.0001653292919624578,
      "loss": 0.2471,
      "step": 10700
    },
    {
      "epoch": 0.7688442211055276,
      "grad_norm": 2.8925998210906982,
      "learning_rate": 0.0001652761160298849,
      "loss": 0.1599,
      "step": 10710
    },
    {
      "epoch": 0.7695620961952621,
      "grad_norm": 18.72650718688965,
      "learning_rate": 0.00016522294009731195,
      "loss": 0.4801,
      "step": 10720
    },
    {
      "epoch": 0.7702799712849964,
      "grad_norm": 18.013147354125977,
      "learning_rate": 0.00016516976416473903,
      "loss": 0.384,
      "step": 10730
    },
    {
      "epoch": 0.7709978463747308,
      "grad_norm": 7.621912002563477,
      "learning_rate": 0.00016511658823216614,
      "loss": 0.3685,
      "step": 10740
    },
    {
      "epoch": 0.7717157214644652,
      "grad_norm": 8.621289253234863,
      "learning_rate": 0.00016506341229959322,
      "loss": 0.3937,
      "step": 10750
    },
    {
      "epoch": 0.7724335965541995,
      "grad_norm": 0.0820804089307785,
      "learning_rate": 0.0001650102363670203,
      "loss": 0.2307,
      "step": 10760
    },
    {
      "epoch": 0.7731514716439339,
      "grad_norm": 8.137399673461914,
      "learning_rate": 0.00016495706043444738,
      "loss": 0.2685,
      "step": 10770
    },
    {
      "epoch": 0.7738693467336684,
      "grad_norm": 1.9682326316833496,
      "learning_rate": 0.00016490388450187446,
      "loss": 0.2809,
      "step": 10780
    },
    {
      "epoch": 0.7745872218234028,
      "grad_norm": 16.352052688598633,
      "learning_rate": 0.00016485070856930154,
      "loss": 0.1958,
      "step": 10790
    },
    {
      "epoch": 0.7753050969131371,
      "grad_norm": 0.24092881381511688,
      "learning_rate": 0.00016479753263672864,
      "loss": 0.3523,
      "step": 10800
    },
    {
      "epoch": 0.7760229720028715,
      "grad_norm": 1.2339781522750854,
      "learning_rate": 0.00016474435670415572,
      "loss": 0.2407,
      "step": 10810
    },
    {
      "epoch": 0.7767408470926059,
      "grad_norm": 3.2274460792541504,
      "learning_rate": 0.00016469118077158277,
      "loss": 0.1793,
      "step": 10820
    },
    {
      "epoch": 0.7774587221823402,
      "grad_norm": 7.367012977600098,
      "learning_rate": 0.00016463800483900985,
      "loss": 0.2584,
      "step": 10830
    },
    {
      "epoch": 0.7781765972720747,
      "grad_norm": 2.2549569606781006,
      "learning_rate": 0.00016458482890643696,
      "loss": 0.092,
      "step": 10840
    },
    {
      "epoch": 0.7788944723618091,
      "grad_norm": 0.25810474157333374,
      "learning_rate": 0.00016453165297386404,
      "loss": 0.3445,
      "step": 10850
    },
    {
      "epoch": 0.7796123474515434,
      "grad_norm": 0.5634281039237976,
      "learning_rate": 0.00016447847704129112,
      "loss": 0.4593,
      "step": 10860
    },
    {
      "epoch": 0.7803302225412778,
      "grad_norm": 6.859597206115723,
      "learning_rate": 0.0001644253011087182,
      "loss": 0.1634,
      "step": 10870
    },
    {
      "epoch": 0.7810480976310122,
      "grad_norm": 3.12161922454834,
      "learning_rate": 0.00016437212517614528,
      "loss": 0.3308,
      "step": 10880
    },
    {
      "epoch": 0.7817659727207465,
      "grad_norm": 0.024017170071601868,
      "learning_rate": 0.00016431894924357236,
      "loss": 0.1617,
      "step": 10890
    },
    {
      "epoch": 0.782483847810481,
      "grad_norm": 0.15585844218730927,
      "learning_rate": 0.00016426577331099946,
      "loss": 0.1804,
      "step": 10900
    },
    {
      "epoch": 0.7832017229002154,
      "grad_norm": 14.199904441833496,
      "learning_rate": 0.00016421259737842654,
      "loss": 0.2509,
      "step": 10910
    },
    {
      "epoch": 0.7839195979899497,
      "grad_norm": 5.502984046936035,
      "learning_rate": 0.00016415942144585362,
      "loss": 0.2737,
      "step": 10920
    },
    {
      "epoch": 0.7846374730796841,
      "grad_norm": 0.698007345199585,
      "learning_rate": 0.00016410624551328068,
      "loss": 0.1363,
      "step": 10930
    },
    {
      "epoch": 0.7853553481694185,
      "grad_norm": 0.6868225932121277,
      "learning_rate": 0.00016405306958070778,
      "loss": 0.0882,
      "step": 10940
    },
    {
      "epoch": 0.7860732232591529,
      "grad_norm": 8.08154582977295,
      "learning_rate": 0.00016399989364813486,
      "loss": 0.3848,
      "step": 10950
    },
    {
      "epoch": 0.7867910983488873,
      "grad_norm": 2.802954912185669,
      "learning_rate": 0.00016394671771556194,
      "loss": 0.1974,
      "step": 10960
    },
    {
      "epoch": 0.7875089734386217,
      "grad_norm": 8.592987060546875,
      "learning_rate": 0.00016389354178298905,
      "loss": 0.2793,
      "step": 10970
    },
    {
      "epoch": 0.7882268485283561,
      "grad_norm": 13.749303817749023,
      "learning_rate": 0.0001638403658504161,
      "loss": 0.2487,
      "step": 10980
    },
    {
      "epoch": 0.7889447236180904,
      "grad_norm": 10.063469886779785,
      "learning_rate": 0.00016378718991784318,
      "loss": 0.3332,
      "step": 10990
    },
    {
      "epoch": 0.7896625987078248,
      "grad_norm": 14.109420776367188,
      "learning_rate": 0.00016373401398527029,
      "loss": 0.299,
      "step": 11000
    },
    {
      "epoch": 0.7903804737975593,
      "grad_norm": 3.1694798469543457,
      "learning_rate": 0.00016368083805269737,
      "loss": 0.1805,
      "step": 11010
    },
    {
      "epoch": 0.7910983488872936,
      "grad_norm": 7.970658302307129,
      "learning_rate": 0.00016362766212012445,
      "loss": 0.1234,
      "step": 11020
    },
    {
      "epoch": 0.791816223977028,
      "grad_norm": 4.790167808532715,
      "learning_rate": 0.00016357448618755152,
      "loss": 0.0817,
      "step": 11030
    },
    {
      "epoch": 0.7925340990667624,
      "grad_norm": 0.18514958024024963,
      "learning_rate": 0.0001635213102549786,
      "loss": 0.2973,
      "step": 11040
    },
    {
      "epoch": 0.7932519741564967,
      "grad_norm": 17.626632690429688,
      "learning_rate": 0.00016346813432240568,
      "loss": 0.3665,
      "step": 11050
    },
    {
      "epoch": 0.7939698492462312,
      "grad_norm": 11.218132019042969,
      "learning_rate": 0.00016341495838983276,
      "loss": 0.515,
      "step": 11060
    },
    {
      "epoch": 0.7946877243359656,
      "grad_norm": 0.2175390124320984,
      "learning_rate": 0.00016336178245725987,
      "loss": 0.1883,
      "step": 11070
    },
    {
      "epoch": 0.7954055994256999,
      "grad_norm": 1.2895249128341675,
      "learning_rate": 0.00016330860652468692,
      "loss": 0.2212,
      "step": 11080
    },
    {
      "epoch": 0.7961234745154343,
      "grad_norm": 14.033848762512207,
      "learning_rate": 0.000163255430592114,
      "loss": 0.5284,
      "step": 11090
    },
    {
      "epoch": 0.7968413496051687,
      "grad_norm": 0.23150186240673065,
      "learning_rate": 0.0001632022546595411,
      "loss": 0.1669,
      "step": 11100
    },
    {
      "epoch": 0.797559224694903,
      "grad_norm": 0.5888727307319641,
      "learning_rate": 0.0001631490787269682,
      "loss": 0.1973,
      "step": 11110
    },
    {
      "epoch": 0.7982770997846375,
      "grad_norm": 0.8723751902580261,
      "learning_rate": 0.00016309590279439527,
      "loss": 0.4633,
      "step": 11120
    },
    {
      "epoch": 0.7989949748743719,
      "grad_norm": 0.18442149460315704,
      "learning_rate": 0.00016304272686182235,
      "loss": 0.1735,
      "step": 11130
    },
    {
      "epoch": 0.7997128499641063,
      "grad_norm": 0.45273593068122864,
      "learning_rate": 0.00016298955092924943,
      "loss": 0.281,
      "step": 11140
    },
    {
      "epoch": 0.8004307250538406,
      "grad_norm": 25.96979331970215,
      "learning_rate": 0.0001629363749966765,
      "loss": 0.4318,
      "step": 11150
    },
    {
      "epoch": 0.801148600143575,
      "grad_norm": 0.026830662041902542,
      "learning_rate": 0.00016288319906410359,
      "loss": 0.2248,
      "step": 11160
    },
    {
      "epoch": 0.8018664752333095,
      "grad_norm": 6.611185550689697,
      "learning_rate": 0.0001628300231315307,
      "loss": 0.1253,
      "step": 11170
    },
    {
      "epoch": 0.8025843503230438,
      "grad_norm": 0.5214711427688599,
      "learning_rate": 0.00016277684719895777,
      "loss": 0.2632,
      "step": 11180
    },
    {
      "epoch": 0.8033022254127782,
      "grad_norm": 0.024716263636946678,
      "learning_rate": 0.00016272367126638482,
      "loss": 0.3292,
      "step": 11190
    },
    {
      "epoch": 0.8040201005025126,
      "grad_norm": 2.2459542751312256,
      "learning_rate": 0.00016267049533381193,
      "loss": 0.0614,
      "step": 11200
    },
    {
      "epoch": 0.8047379755922469,
      "grad_norm": 14.77238941192627,
      "learning_rate": 0.000162617319401239,
      "loss": 0.3564,
      "step": 11210
    },
    {
      "epoch": 0.8054558506819813,
      "grad_norm": 1.7949235439300537,
      "learning_rate": 0.0001625641434686661,
      "loss": 0.104,
      "step": 11220
    },
    {
      "epoch": 0.8061737257717158,
      "grad_norm": 3.6622047424316406,
      "learning_rate": 0.00016251096753609317,
      "loss": 0.2851,
      "step": 11230
    },
    {
      "epoch": 0.8068916008614501,
      "grad_norm": 0.7249214053153992,
      "learning_rate": 0.00016245779160352025,
      "loss": 0.2418,
      "step": 11240
    },
    {
      "epoch": 0.8076094759511845,
      "grad_norm": 3.0705268383026123,
      "learning_rate": 0.00016240461567094733,
      "loss": 0.2784,
      "step": 11250
    },
    {
      "epoch": 0.8083273510409189,
      "grad_norm": 5.124979019165039,
      "learning_rate": 0.0001623514397383744,
      "loss": 0.2431,
      "step": 11260
    },
    {
      "epoch": 0.8090452261306532,
      "grad_norm": 0.05743919685482979,
      "learning_rate": 0.00016229826380580151,
      "loss": 0.2773,
      "step": 11270
    },
    {
      "epoch": 0.8097631012203876,
      "grad_norm": 1.2258236408233643,
      "learning_rate": 0.0001622450878732286,
      "loss": 0.4774,
      "step": 11280
    },
    {
      "epoch": 0.8104809763101221,
      "grad_norm": 9.755674362182617,
      "learning_rate": 0.00016219191194065567,
      "loss": 0.4152,
      "step": 11290
    },
    {
      "epoch": 0.8111988513998565,
      "grad_norm": 10.119333267211914,
      "learning_rate": 0.00016213873600808275,
      "loss": 0.1549,
      "step": 11300
    },
    {
      "epoch": 0.8119167264895908,
      "grad_norm": 7.014317512512207,
      "learning_rate": 0.00016208556007550983,
      "loss": 0.4031,
      "step": 11310
    },
    {
      "epoch": 0.8126346015793252,
      "grad_norm": 0.22776834666728973,
      "learning_rate": 0.0001620323841429369,
      "loss": 0.2484,
      "step": 11320
    },
    {
      "epoch": 0.8133524766690596,
      "grad_norm": 9.50921630859375,
      "learning_rate": 0.000161979208210364,
      "loss": 0.5116,
      "step": 11330
    },
    {
      "epoch": 0.8140703517587939,
      "grad_norm": 2.803264856338501,
      "learning_rate": 0.0001619260322777911,
      "loss": 0.1218,
      "step": 11340
    },
    {
      "epoch": 0.8147882268485284,
      "grad_norm": 27.730182647705078,
      "learning_rate": 0.00016187285634521815,
      "loss": 0.2423,
      "step": 11350
    },
    {
      "epoch": 0.8155061019382628,
      "grad_norm": 13.712970733642578,
      "learning_rate": 0.00016181968041264523,
      "loss": 0.4112,
      "step": 11360
    },
    {
      "epoch": 0.8162239770279971,
      "grad_norm": 6.918055534362793,
      "learning_rate": 0.00016176650448007234,
      "loss": 0.6016,
      "step": 11370
    },
    {
      "epoch": 0.8169418521177315,
      "grad_norm": 1.4553163051605225,
      "learning_rate": 0.00016171332854749942,
      "loss": 0.2111,
      "step": 11380
    },
    {
      "epoch": 0.8176597272074659,
      "grad_norm": 6.617661952972412,
      "learning_rate": 0.0001616601526149265,
      "loss": 0.1172,
      "step": 11390
    },
    {
      "epoch": 0.8183776022972002,
      "grad_norm": 2.2786550521850586,
      "learning_rate": 0.00016160697668235357,
      "loss": 0.2736,
      "step": 11400
    },
    {
      "epoch": 0.8190954773869347,
      "grad_norm": 10.946818351745605,
      "learning_rate": 0.00016155380074978065,
      "loss": 0.2317,
      "step": 11410
    },
    {
      "epoch": 0.8198133524766691,
      "grad_norm": 10.043144226074219,
      "learning_rate": 0.00016150062481720773,
      "loss": 0.1186,
      "step": 11420
    },
    {
      "epoch": 0.8205312275664034,
      "grad_norm": 1.5419844388961792,
      "learning_rate": 0.0001614474488846348,
      "loss": 0.1433,
      "step": 11430
    },
    {
      "epoch": 0.8212491026561378,
      "grad_norm": 9.066306114196777,
      "learning_rate": 0.00016139427295206192,
      "loss": 0.2285,
      "step": 11440
    },
    {
      "epoch": 0.8219669777458722,
      "grad_norm": 11.722107887268066,
      "learning_rate": 0.00016134109701948897,
      "loss": 0.2838,
      "step": 11450
    },
    {
      "epoch": 0.8226848528356066,
      "grad_norm": 0.301734983921051,
      "learning_rate": 0.00016128792108691605,
      "loss": 0.0415,
      "step": 11460
    },
    {
      "epoch": 0.823402727925341,
      "grad_norm": 10.248827934265137,
      "learning_rate": 0.00016123474515434316,
      "loss": 0.2617,
      "step": 11470
    },
    {
      "epoch": 0.8241206030150754,
      "grad_norm": 26.560466766357422,
      "learning_rate": 0.00016118156922177024,
      "loss": 0.2314,
      "step": 11480
    },
    {
      "epoch": 0.8248384781048098,
      "grad_norm": 0.10874493420124054,
      "learning_rate": 0.00016112839328919732,
      "loss": 0.2612,
      "step": 11490
    },
    {
      "epoch": 0.8255563531945441,
      "grad_norm": 3.242734909057617,
      "learning_rate": 0.0001610752173566244,
      "loss": 0.2034,
      "step": 11500
    },
    {
      "epoch": 0.8262742282842785,
      "grad_norm": 6.7151923179626465,
      "learning_rate": 0.00016102204142405148,
      "loss": 0.2265,
      "step": 11510
    },
    {
      "epoch": 0.826992103374013,
      "grad_norm": 0.18686506152153015,
      "learning_rate": 0.00016096886549147856,
      "loss": 0.2008,
      "step": 11520
    },
    {
      "epoch": 0.8277099784637473,
      "grad_norm": 0.10114151239395142,
      "learning_rate": 0.00016091568955890566,
      "loss": 0.2208,
      "step": 11530
    },
    {
      "epoch": 0.8284278535534817,
      "grad_norm": 1.9978865385055542,
      "learning_rate": 0.00016086251362633274,
      "loss": 0.2181,
      "step": 11540
    },
    {
      "epoch": 0.8291457286432161,
      "grad_norm": 13.975104331970215,
      "learning_rate": 0.00016080933769375982,
      "loss": 0.1103,
      "step": 11550
    },
    {
      "epoch": 0.8298636037329504,
      "grad_norm": 0.46009257435798645,
      "learning_rate": 0.00016075616176118687,
      "loss": 0.1055,
      "step": 11560
    },
    {
      "epoch": 0.8305814788226848,
      "grad_norm": 5.5142741203308105,
      "learning_rate": 0.00016070298582861398,
      "loss": 0.3242,
      "step": 11570
    },
    {
      "epoch": 0.8312993539124193,
      "grad_norm": 0.4117339253425598,
      "learning_rate": 0.00016064980989604106,
      "loss": 0.5249,
      "step": 11580
    },
    {
      "epoch": 0.8320172290021536,
      "grad_norm": 22.825572967529297,
      "learning_rate": 0.00016059663396346814,
      "loss": 0.2499,
      "step": 11590
    },
    {
      "epoch": 0.832735104091888,
      "grad_norm": 6.210501670837402,
      "learning_rate": 0.00016054345803089525,
      "loss": 0.1709,
      "step": 11600
    },
    {
      "epoch": 0.8334529791816224,
      "grad_norm": 2.4839158058166504,
      "learning_rate": 0.0001604902820983223,
      "loss": 0.2107,
      "step": 11610
    },
    {
      "epoch": 0.8341708542713567,
      "grad_norm": 3.1770131587982178,
      "learning_rate": 0.00016043710616574938,
      "loss": 0.156,
      "step": 11620
    },
    {
      "epoch": 0.8348887293610912,
      "grad_norm": 0.0721876323223114,
      "learning_rate": 0.00016038393023317648,
      "loss": 0.2147,
      "step": 11630
    },
    {
      "epoch": 0.8356066044508256,
      "grad_norm": 19.443510055541992,
      "learning_rate": 0.00016033075430060356,
      "loss": 0.2107,
      "step": 11640
    },
    {
      "epoch": 0.83632447954056,
      "grad_norm": 0.7423526644706726,
      "learning_rate": 0.00016027757836803064,
      "loss": 0.3403,
      "step": 11650
    },
    {
      "epoch": 0.8370423546302943,
      "grad_norm": 0.6757634282112122,
      "learning_rate": 0.00016022440243545772,
      "loss": 0.1844,
      "step": 11660
    },
    {
      "epoch": 0.8377602297200287,
      "grad_norm": 18.081626892089844,
      "learning_rate": 0.0001601712265028848,
      "loss": 0.2105,
      "step": 11670
    },
    {
      "epoch": 0.8384781048097631,
      "grad_norm": 4.770555019378662,
      "learning_rate": 0.00016011805057031188,
      "loss": 0.1991,
      "step": 11680
    },
    {
      "epoch": 0.8391959798994975,
      "grad_norm": 0.5983889698982239,
      "learning_rate": 0.00016006487463773896,
      "loss": 0.3871,
      "step": 11690
    },
    {
      "epoch": 0.8399138549892319,
      "grad_norm": 16.959489822387695,
      "learning_rate": 0.00016001169870516607,
      "loss": 0.194,
      "step": 11700
    },
    {
      "epoch": 0.8406317300789663,
      "grad_norm": 0.1347326636314392,
      "learning_rate": 0.00015995852277259312,
      "loss": 0.3286,
      "step": 11710
    },
    {
      "epoch": 0.8413496051687006,
      "grad_norm": 24.327144622802734,
      "learning_rate": 0.0001599053468400202,
      "loss": 0.4358,
      "step": 11720
    },
    {
      "epoch": 0.842067480258435,
      "grad_norm": 5.822369575500488,
      "learning_rate": 0.0001598521709074473,
      "loss": 0.2334,
      "step": 11730
    },
    {
      "epoch": 0.8427853553481695,
      "grad_norm": 10.726471900939941,
      "learning_rate": 0.00015979899497487439,
      "loss": 0.2653,
      "step": 11740
    },
    {
      "epoch": 0.8435032304379038,
      "grad_norm": 4.534488677978516,
      "learning_rate": 0.00015974581904230147,
      "loss": 0.3509,
      "step": 11750
    },
    {
      "epoch": 0.8442211055276382,
      "grad_norm": 1.4983329772949219,
      "learning_rate": 0.00015969264310972855,
      "loss": 0.2405,
      "step": 11760
    },
    {
      "epoch": 0.8449389806173726,
      "grad_norm": 14.283467292785645,
      "learning_rate": 0.00015963946717715562,
      "loss": 0.1389,
      "step": 11770
    },
    {
      "epoch": 0.8456568557071069,
      "grad_norm": 0.038082435727119446,
      "learning_rate": 0.0001595862912445827,
      "loss": 0.1098,
      "step": 11780
    },
    {
      "epoch": 0.8463747307968413,
      "grad_norm": 0.04640957713127136,
      "learning_rate": 0.00015953311531200978,
      "loss": 0.2841,
      "step": 11790
    },
    {
      "epoch": 0.8470926058865758,
      "grad_norm": 6.483451843261719,
      "learning_rate": 0.0001594799393794369,
      "loss": 0.2566,
      "step": 11800
    },
    {
      "epoch": 0.8478104809763102,
      "grad_norm": 8.3869047164917,
      "learning_rate": 0.00015942676344686397,
      "loss": 0.2818,
      "step": 11810
    },
    {
      "epoch": 0.8485283560660445,
      "grad_norm": 0.14692984521389008,
      "learning_rate": 0.00015937358751429102,
      "loss": 0.2687,
      "step": 11820
    },
    {
      "epoch": 0.8492462311557789,
      "grad_norm": 0.126887708902359,
      "learning_rate": 0.00015932041158171813,
      "loss": 0.212,
      "step": 11830
    },
    {
      "epoch": 0.8499641062455133,
      "grad_norm": 6.900025367736816,
      "learning_rate": 0.0001592672356491452,
      "loss": 0.3236,
      "step": 11840
    },
    {
      "epoch": 0.8506819813352476,
      "grad_norm": 9.059415817260742,
      "learning_rate": 0.0001592140597165723,
      "loss": 0.2338,
      "step": 11850
    },
    {
      "epoch": 0.8513998564249821,
      "grad_norm": 4.460964202880859,
      "learning_rate": 0.00015916088378399937,
      "loss": 0.1276,
      "step": 11860
    },
    {
      "epoch": 0.8521177315147165,
      "grad_norm": 0.56159907579422,
      "learning_rate": 0.00015910770785142645,
      "loss": 0.1438,
      "step": 11870
    },
    {
      "epoch": 0.8528356066044508,
      "grad_norm": 2.1708147525787354,
      "learning_rate": 0.00015905453191885353,
      "loss": 0.207,
      "step": 11880
    },
    {
      "epoch": 0.8535534816941852,
      "grad_norm": 3.922240972518921,
      "learning_rate": 0.0001590013559862806,
      "loss": 0.2588,
      "step": 11890
    },
    {
      "epoch": 0.8542713567839196,
      "grad_norm": 4.001335620880127,
      "learning_rate": 0.0001589481800537077,
      "loss": 0.2344,
      "step": 11900
    },
    {
      "epoch": 0.8549892318736539,
      "grad_norm": 3.9213919639587402,
      "learning_rate": 0.0001588950041211348,
      "loss": 0.1643,
      "step": 11910
    },
    {
      "epoch": 0.8557071069633884,
      "grad_norm": 16.65751075744629,
      "learning_rate": 0.00015884182818856187,
      "loss": 0.5648,
      "step": 11920
    },
    {
      "epoch": 0.8564249820531228,
      "grad_norm": 0.23605124652385712,
      "learning_rate": 0.00015878865225598895,
      "loss": 0.212,
      "step": 11930
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 6.953709602355957,
      "learning_rate": 0.00015873547632341603,
      "loss": 0.2489,
      "step": 11940
    },
    {
      "epoch": 0.8578607322325915,
      "grad_norm": 0.9419875741004944,
      "learning_rate": 0.0001586823003908431,
      "loss": 0.3126,
      "step": 11950
    },
    {
      "epoch": 0.8585786073223259,
      "grad_norm": 2.233118772506714,
      "learning_rate": 0.0001586291244582702,
      "loss": 0.1446,
      "step": 11960
    },
    {
      "epoch": 0.8592964824120602,
      "grad_norm": 0.48885729908943176,
      "learning_rate": 0.00015857594852569727,
      "loss": 0.0574,
      "step": 11970
    },
    {
      "epoch": 0.8600143575017947,
      "grad_norm": 6.186768054962158,
      "learning_rate": 0.00015852277259312435,
      "loss": 0.1673,
      "step": 11980
    },
    {
      "epoch": 0.8607322325915291,
      "grad_norm": 15.419243812561035,
      "learning_rate": 0.00015846959666055143,
      "loss": 0.1871,
      "step": 11990
    },
    {
      "epoch": 0.8614501076812635,
      "grad_norm": 16.163686752319336,
      "learning_rate": 0.00015841642072797853,
      "loss": 0.0616,
      "step": 12000
    },
    {
      "epoch": 0.8621679827709978,
      "grad_norm": 1.690240740776062,
      "learning_rate": 0.00015836324479540561,
      "loss": 0.2922,
      "step": 12010
    },
    {
      "epoch": 0.8628858578607322,
      "grad_norm": 24.54299545288086,
      "learning_rate": 0.0001583100688628327,
      "loss": 0.5818,
      "step": 12020
    },
    {
      "epoch": 0.8636037329504667,
      "grad_norm": 13.631690979003906,
      "learning_rate": 0.00015825689293025977,
      "loss": 0.2756,
      "step": 12030
    },
    {
      "epoch": 0.864321608040201,
      "grad_norm": 0.21452505886554718,
      "learning_rate": 0.00015820371699768685,
      "loss": 0.2031,
      "step": 12040
    },
    {
      "epoch": 0.8650394831299354,
      "grad_norm": 9.448668479919434,
      "learning_rate": 0.00015815054106511393,
      "loss": 0.1734,
      "step": 12050
    },
    {
      "epoch": 0.8657573582196698,
      "grad_norm": 0.05986086651682854,
      "learning_rate": 0.000158097365132541,
      "loss": 0.2327,
      "step": 12060
    },
    {
      "epoch": 0.8664752333094041,
      "grad_norm": 12.921780586242676,
      "learning_rate": 0.00015804418919996812,
      "loss": 0.3027,
      "step": 12070
    },
    {
      "epoch": 0.8671931083991385,
      "grad_norm": 0.14392034709453583,
      "learning_rate": 0.00015799101326739517,
      "loss": 0.2953,
      "step": 12080
    },
    {
      "epoch": 0.867910983488873,
      "grad_norm": 0.21796093881130219,
      "learning_rate": 0.00015793783733482225,
      "loss": 0.1931,
      "step": 12090
    },
    {
      "epoch": 0.8686288585786073,
      "grad_norm": 10.924053192138672,
      "learning_rate": 0.00015788466140224936,
      "loss": 0.5477,
      "step": 12100
    },
    {
      "epoch": 0.8693467336683417,
      "grad_norm": 6.958584308624268,
      "learning_rate": 0.00015783148546967644,
      "loss": 0.1262,
      "step": 12110
    },
    {
      "epoch": 0.8700646087580761,
      "grad_norm": 0.29190361499786377,
      "learning_rate": 0.00015777830953710352,
      "loss": 0.2456,
      "step": 12120
    },
    {
      "epoch": 0.8707824838478104,
      "grad_norm": 7.83608341217041,
      "learning_rate": 0.0001577251336045306,
      "loss": 0.2981,
      "step": 12130
    },
    {
      "epoch": 0.8715003589375449,
      "grad_norm": 11.02092170715332,
      "learning_rate": 0.00015767195767195767,
      "loss": 0.216,
      "step": 12140
    },
    {
      "epoch": 0.8722182340272793,
      "grad_norm": 3.2723774909973145,
      "learning_rate": 0.00015761878173938475,
      "loss": 0.1508,
      "step": 12150
    },
    {
      "epoch": 0.8729361091170137,
      "grad_norm": 9.222883224487305,
      "learning_rate": 0.00015756560580681183,
      "loss": 0.1989,
      "step": 12160
    },
    {
      "epoch": 0.873653984206748,
      "grad_norm": 5.307323455810547,
      "learning_rate": 0.00015751242987423894,
      "loss": 0.2837,
      "step": 12170
    },
    {
      "epoch": 0.8743718592964824,
      "grad_norm": 0.15995025634765625,
      "learning_rate": 0.00015745925394166602,
      "loss": 0.2535,
      "step": 12180
    },
    {
      "epoch": 0.8750897343862168,
      "grad_norm": 8.186609268188477,
      "learning_rate": 0.00015740607800909307,
      "loss": 0.1606,
      "step": 12190
    },
    {
      "epoch": 0.8758076094759512,
      "grad_norm": 0.06273337453603745,
      "learning_rate": 0.00015735290207652018,
      "loss": 0.3139,
      "step": 12200
    },
    {
      "epoch": 0.8765254845656856,
      "grad_norm": 1.2890137434005737,
      "learning_rate": 0.00015729972614394726,
      "loss": 0.1989,
      "step": 12210
    },
    {
      "epoch": 0.87724335965542,
      "grad_norm": 6.5065202713012695,
      "learning_rate": 0.00015724655021137434,
      "loss": 0.3744,
      "step": 12220
    },
    {
      "epoch": 0.8779612347451543,
      "grad_norm": 25.77219009399414,
      "learning_rate": 0.00015719337427880144,
      "loss": 0.2356,
      "step": 12230
    },
    {
      "epoch": 0.8786791098348887,
      "grad_norm": 20.291015625,
      "learning_rate": 0.0001571401983462285,
      "loss": 0.3455,
      "step": 12240
    },
    {
      "epoch": 0.8793969849246231,
      "grad_norm": 8.562131881713867,
      "learning_rate": 0.00015708702241365558,
      "loss": 0.231,
      "step": 12250
    },
    {
      "epoch": 0.8801148600143575,
      "grad_norm": 2.1889102458953857,
      "learning_rate": 0.00015703384648108268,
      "loss": 0.3575,
      "step": 12260
    },
    {
      "epoch": 0.8808327351040919,
      "grad_norm": 0.6942485570907593,
      "learning_rate": 0.00015698067054850976,
      "loss": 0.2537,
      "step": 12270
    },
    {
      "epoch": 0.8815506101938263,
      "grad_norm": 0.8226823806762695,
      "learning_rate": 0.00015692749461593684,
      "loss": 0.2873,
      "step": 12280
    },
    {
      "epoch": 0.8822684852835606,
      "grad_norm": 6.848333358764648,
      "learning_rate": 0.00015687431868336392,
      "loss": 0.3095,
      "step": 12290
    },
    {
      "epoch": 0.882986360373295,
      "grad_norm": 3.273402214050293,
      "learning_rate": 0.000156821142750791,
      "loss": 0.2749,
      "step": 12300
    },
    {
      "epoch": 0.8837042354630295,
      "grad_norm": 6.658769607543945,
      "learning_rate": 0.00015676796681821808,
      "loss": 0.2387,
      "step": 12310
    },
    {
      "epoch": 0.8844221105527639,
      "grad_norm": 15.53836441040039,
      "learning_rate": 0.00015671479088564516,
      "loss": 0.2856,
      "step": 12320
    },
    {
      "epoch": 0.8851399856424982,
      "grad_norm": 0.07248211652040482,
      "learning_rate": 0.00015666161495307227,
      "loss": 0.0219,
      "step": 12330
    },
    {
      "epoch": 0.8858578607322326,
      "grad_norm": 0.263200044631958,
      "learning_rate": 0.00015660843902049932,
      "loss": 0.2159,
      "step": 12340
    },
    {
      "epoch": 0.886575735821967,
      "grad_norm": 14.868254661560059,
      "learning_rate": 0.0001565552630879264,
      "loss": 0.2931,
      "step": 12350
    },
    {
      "epoch": 0.8872936109117013,
      "grad_norm": 1.4930493831634521,
      "learning_rate": 0.0001565020871553535,
      "loss": 0.0933,
      "step": 12360
    },
    {
      "epoch": 0.8880114860014358,
      "grad_norm": 16.115215301513672,
      "learning_rate": 0.00015644891122278058,
      "loss": 0.3359,
      "step": 12370
    },
    {
      "epoch": 0.8887293610911702,
      "grad_norm": 16.07269859313965,
      "learning_rate": 0.00015639573529020766,
      "loss": 0.2579,
      "step": 12380
    },
    {
      "epoch": 0.8894472361809045,
      "grad_norm": 5.805087089538574,
      "learning_rate": 0.00015634255935763474,
      "loss": 0.3849,
      "step": 12390
    },
    {
      "epoch": 0.8901651112706389,
      "grad_norm": 0.126991868019104,
      "learning_rate": 0.00015628938342506182,
      "loss": 0.1487,
      "step": 12400
    },
    {
      "epoch": 0.8908829863603733,
      "grad_norm": 0.022886594757437706,
      "learning_rate": 0.0001562362074924889,
      "loss": 0.3494,
      "step": 12410
    },
    {
      "epoch": 0.8916008614501076,
      "grad_norm": 13.008905410766602,
      "learning_rate": 0.00015618303155991598,
      "loss": 0.2098,
      "step": 12420
    },
    {
      "epoch": 0.8923187365398421,
      "grad_norm": 11.28788948059082,
      "learning_rate": 0.0001561298556273431,
      "loss": 0.2601,
      "step": 12430
    },
    {
      "epoch": 0.8930366116295765,
      "grad_norm": 9.147527694702148,
      "learning_rate": 0.00015607667969477017,
      "loss": 0.3378,
      "step": 12440
    },
    {
      "epoch": 0.8937544867193108,
      "grad_norm": 18.794004440307617,
      "learning_rate": 0.00015602350376219722,
      "loss": 0.1294,
      "step": 12450
    },
    {
      "epoch": 0.8944723618090452,
      "grad_norm": 11.528300285339355,
      "learning_rate": 0.00015597032782962433,
      "loss": 0.143,
      "step": 12460
    },
    {
      "epoch": 0.8951902368987796,
      "grad_norm": 1.9133341312408447,
      "learning_rate": 0.0001559171518970514,
      "loss": 0.3493,
      "step": 12470
    },
    {
      "epoch": 0.8959081119885139,
      "grad_norm": 12.008835792541504,
      "learning_rate": 0.00015586397596447849,
      "loss": 0.4669,
      "step": 12480
    },
    {
      "epoch": 0.8966259870782484,
      "grad_norm": 1.1481444835662842,
      "learning_rate": 0.00015581080003190557,
      "loss": 0.2223,
      "step": 12490
    },
    {
      "epoch": 0.8973438621679828,
      "grad_norm": 6.518469333648682,
      "learning_rate": 0.00015575762409933264,
      "loss": 0.1459,
      "step": 12500
    },
    {
      "epoch": 0.8980617372577172,
      "grad_norm": 1.3949527740478516,
      "learning_rate": 0.00015570444816675972,
      "loss": 0.4421,
      "step": 12510
    },
    {
      "epoch": 0.8987796123474515,
      "grad_norm": 26.9881649017334,
      "learning_rate": 0.0001556512722341868,
      "loss": 0.3624,
      "step": 12520
    },
    {
      "epoch": 0.8994974874371859,
      "grad_norm": 0.5950194001197815,
      "learning_rate": 0.0001555980963016139,
      "loss": 0.42,
      "step": 12530
    },
    {
      "epoch": 0.9002153625269204,
      "grad_norm": 15.944051742553711,
      "learning_rate": 0.000155544920369041,
      "loss": 0.3113,
      "step": 12540
    },
    {
      "epoch": 0.9009332376166547,
      "grad_norm": 16.04977035522461,
      "learning_rate": 0.00015549174443646807,
      "loss": 0.4127,
      "step": 12550
    },
    {
      "epoch": 0.9016511127063891,
      "grad_norm": 6.672685623168945,
      "learning_rate": 0.00015543856850389515,
      "loss": 0.3488,
      "step": 12560
    },
    {
      "epoch": 0.9023689877961235,
      "grad_norm": 0.5652123093605042,
      "learning_rate": 0.00015538539257132223,
      "loss": 0.3069,
      "step": 12570
    },
    {
      "epoch": 0.9030868628858578,
      "grad_norm": 9.009113311767578,
      "learning_rate": 0.0001553322166387493,
      "loss": 0.3809,
      "step": 12580
    },
    {
      "epoch": 0.9038047379755922,
      "grad_norm": 1.083612084388733,
      "learning_rate": 0.0001552790407061764,
      "loss": 0.2558,
      "step": 12590
    },
    {
      "epoch": 0.9045226130653267,
      "grad_norm": 0.9650187492370605,
      "learning_rate": 0.00015522586477360347,
      "loss": 0.2953,
      "step": 12600
    },
    {
      "epoch": 0.905240488155061,
      "grad_norm": 19.790279388427734,
      "learning_rate": 0.00015517268884103055,
      "loss": 0.2399,
      "step": 12610
    },
    {
      "epoch": 0.9059583632447954,
      "grad_norm": 14.829329490661621,
      "learning_rate": 0.00015511951290845763,
      "loss": 0.1126,
      "step": 12620
    },
    {
      "epoch": 0.9066762383345298,
      "grad_norm": 7.789073944091797,
      "learning_rate": 0.00015506633697588473,
      "loss": 0.236,
      "step": 12630
    },
    {
      "epoch": 0.9073941134242641,
      "grad_norm": 3.294743061065674,
      "learning_rate": 0.0001550131610433118,
      "loss": 0.2454,
      "step": 12640
    },
    {
      "epoch": 0.9081119885139985,
      "grad_norm": 9.836190223693848,
      "learning_rate": 0.0001549599851107389,
      "loss": 0.2145,
      "step": 12650
    },
    {
      "epoch": 0.908829863603733,
      "grad_norm": 18.526569366455078,
      "learning_rate": 0.00015490680917816597,
      "loss": 0.2649,
      "step": 12660
    },
    {
      "epoch": 0.9095477386934674,
      "grad_norm": 0.16923323273658752,
      "learning_rate": 0.00015485363324559305,
      "loss": 0.3588,
      "step": 12670
    },
    {
      "epoch": 0.9102656137832017,
      "grad_norm": 1.5355262756347656,
      "learning_rate": 0.00015480045731302013,
      "loss": 0.303,
      "step": 12680
    },
    {
      "epoch": 0.9109834888729361,
      "grad_norm": 0.17437708377838135,
      "learning_rate": 0.0001547472813804472,
      "loss": 0.1026,
      "step": 12690
    },
    {
      "epoch": 0.9117013639626705,
      "grad_norm": 25.248672485351562,
      "learning_rate": 0.00015469410544787432,
      "loss": 0.3138,
      "step": 12700
    },
    {
      "epoch": 0.9124192390524049,
      "grad_norm": 14.756101608276367,
      "learning_rate": 0.00015464092951530137,
      "loss": 0.5165,
      "step": 12710
    },
    {
      "epoch": 0.9131371141421393,
      "grad_norm": 1.3411636352539062,
      "learning_rate": 0.00015458775358272845,
      "loss": 0.5228,
      "step": 12720
    },
    {
      "epoch": 0.9138549892318737,
      "grad_norm": 0.031182849779725075,
      "learning_rate": 0.00015453457765015555,
      "loss": 0.2215,
      "step": 12730
    },
    {
      "epoch": 0.914572864321608,
      "grad_norm": 3.9149045944213867,
      "learning_rate": 0.00015448140171758263,
      "loss": 0.1086,
      "step": 12740
    },
    {
      "epoch": 0.9152907394113424,
      "grad_norm": 2.181964635848999,
      "learning_rate": 0.0001544282257850097,
      "loss": 0.1598,
      "step": 12750
    },
    {
      "epoch": 0.9160086145010768,
      "grad_norm": 20.561464309692383,
      "learning_rate": 0.0001543750498524368,
      "loss": 0.2171,
      "step": 12760
    },
    {
      "epoch": 0.9167264895908112,
      "grad_norm": 0.19785882532596588,
      "learning_rate": 0.00015432187391986387,
      "loss": 0.3617,
      "step": 12770
    },
    {
      "epoch": 0.9174443646805456,
      "grad_norm": 14.910714149475098,
      "learning_rate": 0.00015426869798729095,
      "loss": 0.2052,
      "step": 12780
    },
    {
      "epoch": 0.91816223977028,
      "grad_norm": 0.7213811278343201,
      "learning_rate": 0.00015421552205471803,
      "loss": 0.3946,
      "step": 12790
    },
    {
      "epoch": 0.9188801148600143,
      "grad_norm": 14.22236156463623,
      "learning_rate": 0.00015416234612214514,
      "loss": 0.0588,
      "step": 12800
    },
    {
      "epoch": 0.9195979899497487,
      "grad_norm": 10.493487358093262,
      "learning_rate": 0.00015410917018957222,
      "loss": 0.1973,
      "step": 12810
    },
    {
      "epoch": 0.9203158650394831,
      "grad_norm": 9.975790977478027,
      "learning_rate": 0.00015405599425699927,
      "loss": 0.1992,
      "step": 12820
    },
    {
      "epoch": 0.9210337401292176,
      "grad_norm": 0.7803455591201782,
      "learning_rate": 0.00015400281832442638,
      "loss": 0.1041,
      "step": 12830
    },
    {
      "epoch": 0.9217516152189519,
      "grad_norm": 11.704242706298828,
      "learning_rate": 0.00015394964239185346,
      "loss": 0.0813,
      "step": 12840
    },
    {
      "epoch": 0.9224694903086863,
      "grad_norm": 0.12353529781103134,
      "learning_rate": 0.00015389646645928054,
      "loss": 0.0987,
      "step": 12850
    },
    {
      "epoch": 0.9231873653984207,
      "grad_norm": 2.7082090377807617,
      "learning_rate": 0.00015384329052670764,
      "loss": 0.3356,
      "step": 12860
    },
    {
      "epoch": 0.923905240488155,
      "grad_norm": 6.397132396697998,
      "learning_rate": 0.0001537901145941347,
      "loss": 0.1241,
      "step": 12870
    },
    {
      "epoch": 0.9246231155778895,
      "grad_norm": 4.90989875793457,
      "learning_rate": 0.00015373693866156177,
      "loss": 0.4149,
      "step": 12880
    },
    {
      "epoch": 0.9253409906676239,
      "grad_norm": 0.6315723657608032,
      "learning_rate": 0.00015368376272898885,
      "loss": 0.2387,
      "step": 12890
    },
    {
      "epoch": 0.9260588657573582,
      "grad_norm": 10.03077220916748,
      "learning_rate": 0.00015363058679641596,
      "loss": 0.1363,
      "step": 12900
    },
    {
      "epoch": 0.9267767408470926,
      "grad_norm": 12.315449714660645,
      "learning_rate": 0.00015357741086384304,
      "loss": 0.4947,
      "step": 12910
    },
    {
      "epoch": 0.927494615936827,
      "grad_norm": 11.521056175231934,
      "learning_rate": 0.00015352423493127012,
      "loss": 0.2922,
      "step": 12920
    },
    {
      "epoch": 0.9282124910265613,
      "grad_norm": 7.273861408233643,
      "learning_rate": 0.0001534710589986972,
      "loss": 0.2255,
      "step": 12930
    },
    {
      "epoch": 0.9289303661162958,
      "grad_norm": 18.266746520996094,
      "learning_rate": 0.00015341788306612428,
      "loss": 0.3359,
      "step": 12940
    },
    {
      "epoch": 0.9296482412060302,
      "grad_norm": 17.350767135620117,
      "learning_rate": 0.00015336470713355136,
      "loss": 0.257,
      "step": 12950
    },
    {
      "epoch": 0.9303661162957645,
      "grad_norm": 20.402925491333008,
      "learning_rate": 0.00015331153120097846,
      "loss": 0.173,
      "step": 12960
    },
    {
      "epoch": 0.9310839913854989,
      "grad_norm": 6.693394660949707,
      "learning_rate": 0.00015325835526840552,
      "loss": 0.1618,
      "step": 12970
    },
    {
      "epoch": 0.9318018664752333,
      "grad_norm": 0.08472821116447449,
      "learning_rate": 0.0001532051793358326,
      "loss": 0.3581,
      "step": 12980
    },
    {
      "epoch": 0.9325197415649676,
      "grad_norm": 1.4329651594161987,
      "learning_rate": 0.0001531520034032597,
      "loss": 0.16,
      "step": 12990
    },
    {
      "epoch": 0.9332376166547021,
      "grad_norm": 0.1476219892501831,
      "learning_rate": 0.00015309882747068678,
      "loss": 0.1576,
      "step": 13000
    },
    {
      "epoch": 0.9339554917444365,
      "grad_norm": 11.423112869262695,
      "learning_rate": 0.00015304565153811386,
      "loss": 0.2951,
      "step": 13010
    },
    {
      "epoch": 0.9346733668341709,
      "grad_norm": 10.599957466125488,
      "learning_rate": 0.00015299247560554094,
      "loss": 0.5053,
      "step": 13020
    },
    {
      "epoch": 0.9353912419239052,
      "grad_norm": 0.27338847517967224,
      "learning_rate": 0.00015293929967296802,
      "loss": 0.0395,
      "step": 13030
    },
    {
      "epoch": 0.9361091170136396,
      "grad_norm": 1.4776841402053833,
      "learning_rate": 0.0001528861237403951,
      "loss": 0.0951,
      "step": 13040
    },
    {
      "epoch": 0.9368269921033741,
      "grad_norm": 0.1661064624786377,
      "learning_rate": 0.00015283294780782218,
      "loss": 0.2121,
      "step": 13050
    },
    {
      "epoch": 0.9375448671931084,
      "grad_norm": 28.066375732421875,
      "learning_rate": 0.00015277977187524929,
      "loss": 0.3887,
      "step": 13060
    },
    {
      "epoch": 0.9382627422828428,
      "grad_norm": 5.216662883758545,
      "learning_rate": 0.00015272659594267637,
      "loss": 0.2074,
      "step": 13070
    },
    {
      "epoch": 0.9389806173725772,
      "grad_norm": 0.21464675664901733,
      "learning_rate": 0.00015267342001010342,
      "loss": 0.1292,
      "step": 13080
    },
    {
      "epoch": 0.9396984924623115,
      "grad_norm": 10.692529678344727,
      "learning_rate": 0.00015262024407753052,
      "loss": 0.0965,
      "step": 13090
    },
    {
      "epoch": 0.9404163675520459,
      "grad_norm": 3.622119665145874,
      "learning_rate": 0.0001525670681449576,
      "loss": 0.2858,
      "step": 13100
    },
    {
      "epoch": 0.9411342426417804,
      "grad_norm": 9.0260009765625,
      "learning_rate": 0.00015251389221238468,
      "loss": 0.3433,
      "step": 13110
    },
    {
      "epoch": 0.9418521177315147,
      "grad_norm": 10.08508014678955,
      "learning_rate": 0.00015246071627981176,
      "loss": 0.4739,
      "step": 13120
    },
    {
      "epoch": 0.9425699928212491,
      "grad_norm": 27.981531143188477,
      "learning_rate": 0.00015240754034723884,
      "loss": 0.1744,
      "step": 13130
    },
    {
      "epoch": 0.9432878679109835,
      "grad_norm": 0.03637298569083214,
      "learning_rate": 0.00015235436441466592,
      "loss": 0.1827,
      "step": 13140
    },
    {
      "epoch": 0.9440057430007178,
      "grad_norm": 0.036912642419338226,
      "learning_rate": 0.000152301188482093,
      "loss": 0.1359,
      "step": 13150
    },
    {
      "epoch": 0.9447236180904522,
      "grad_norm": 13.243953704833984,
      "learning_rate": 0.0001522480125495201,
      "loss": 0.5445,
      "step": 13160
    },
    {
      "epoch": 0.9454414931801867,
      "grad_norm": 0.7170562744140625,
      "learning_rate": 0.0001521948366169472,
      "loss": 0.1688,
      "step": 13170
    },
    {
      "epoch": 0.9461593682699211,
      "grad_norm": 8.873669624328613,
      "learning_rate": 0.00015214166068437427,
      "loss": 0.3503,
      "step": 13180
    },
    {
      "epoch": 0.9468772433596554,
      "grad_norm": 0.1338668316602707,
      "learning_rate": 0.00015208848475180135,
      "loss": 0.3024,
      "step": 13190
    },
    {
      "epoch": 0.9475951184493898,
      "grad_norm": 2.469115734100342,
      "learning_rate": 0.00015203530881922843,
      "loss": 0.2323,
      "step": 13200
    },
    {
      "epoch": 0.9483129935391242,
      "grad_norm": 0.6403381824493408,
      "learning_rate": 0.0001519821328866555,
      "loss": 0.3151,
      "step": 13210
    },
    {
      "epoch": 0.9490308686288585,
      "grad_norm": 0.24782107770442963,
      "learning_rate": 0.00015192895695408259,
      "loss": 0.1439,
      "step": 13220
    },
    {
      "epoch": 0.949748743718593,
      "grad_norm": 2.3050003051757812,
      "learning_rate": 0.00015187578102150966,
      "loss": 0.0741,
      "step": 13230
    },
    {
      "epoch": 0.9504666188083274,
      "grad_norm": 13.073541641235352,
      "learning_rate": 0.00015182260508893674,
      "loss": 0.338,
      "step": 13240
    },
    {
      "epoch": 0.9511844938980617,
      "grad_norm": 19.75440216064453,
      "learning_rate": 0.00015176942915636382,
      "loss": 0.3475,
      "step": 13250
    },
    {
      "epoch": 0.9519023689877961,
      "grad_norm": 9.956701278686523,
      "learning_rate": 0.00015171625322379093,
      "loss": 0.3875,
      "step": 13260
    },
    {
      "epoch": 0.9526202440775305,
      "grad_norm": 18.7369384765625,
      "learning_rate": 0.000151663077291218,
      "loss": 0.3491,
      "step": 13270
    },
    {
      "epoch": 0.9533381191672649,
      "grad_norm": 18.07200050354004,
      "learning_rate": 0.0001516099013586451,
      "loss": 0.3256,
      "step": 13280
    },
    {
      "epoch": 0.9540559942569993,
      "grad_norm": 9.636195182800293,
      "learning_rate": 0.00015155672542607217,
      "loss": 0.3949,
      "step": 13290
    },
    {
      "epoch": 0.9547738693467337,
      "grad_norm": 9.509434700012207,
      "learning_rate": 0.00015150354949349925,
      "loss": 0.2316,
      "step": 13300
    },
    {
      "epoch": 0.955491744436468,
      "grad_norm": 11.158812522888184,
      "learning_rate": 0.00015145037356092633,
      "loss": 0.1716,
      "step": 13310
    },
    {
      "epoch": 0.9562096195262024,
      "grad_norm": 0.9315239191055298,
      "learning_rate": 0.0001513971976283534,
      "loss": 0.1394,
      "step": 13320
    },
    {
      "epoch": 0.9569274946159368,
      "grad_norm": 14.900269508361816,
      "learning_rate": 0.00015134402169578051,
      "loss": 0.2862,
      "step": 13330
    },
    {
      "epoch": 0.9576453697056713,
      "grad_norm": 0.09935925155878067,
      "learning_rate": 0.00015129084576320757,
      "loss": 0.1739,
      "step": 13340
    },
    {
      "epoch": 0.9583632447954056,
      "grad_norm": 2.293579339981079,
      "learning_rate": 0.00015123766983063465,
      "loss": 0.1571,
      "step": 13350
    },
    {
      "epoch": 0.95908111988514,
      "grad_norm": 9.825700759887695,
      "learning_rate": 0.00015118449389806175,
      "loss": 0.5714,
      "step": 13360
    },
    {
      "epoch": 0.9597989949748744,
      "grad_norm": 26.26235008239746,
      "learning_rate": 0.00015113131796548883,
      "loss": 0.3397,
      "step": 13370
    },
    {
      "epoch": 0.9605168700646087,
      "grad_norm": 2.09824275970459,
      "learning_rate": 0.0001510781420329159,
      "loss": 0.4172,
      "step": 13380
    },
    {
      "epoch": 0.9612347451543432,
      "grad_norm": 0.29338911175727844,
      "learning_rate": 0.000151024966100343,
      "loss": 0.2663,
      "step": 13390
    },
    {
      "epoch": 0.9619526202440776,
      "grad_norm": 14.681293487548828,
      "learning_rate": 0.00015097179016777007,
      "loss": 0.0958,
      "step": 13400
    },
    {
      "epoch": 0.9626704953338119,
      "grad_norm": 1.014146327972412,
      "learning_rate": 0.00015091861423519715,
      "loss": 0.1604,
      "step": 13410
    },
    {
      "epoch": 0.9633883704235463,
      "grad_norm": 15.669000625610352,
      "learning_rate": 0.00015086543830262423,
      "loss": 0.1256,
      "step": 13420
    },
    {
      "epoch": 0.9641062455132807,
      "grad_norm": 13.046699523925781,
      "learning_rate": 0.00015081226237005134,
      "loss": 0.0801,
      "step": 13430
    },
    {
      "epoch": 0.964824120603015,
      "grad_norm": 0.2385810762643814,
      "learning_rate": 0.00015075908643747842,
      "loss": 0.3289,
      "step": 13440
    },
    {
      "epoch": 0.9655419956927495,
      "grad_norm": 12.898436546325684,
      "learning_rate": 0.00015070591050490547,
      "loss": 0.2898,
      "step": 13450
    },
    {
      "epoch": 0.9662598707824839,
      "grad_norm": 3.2042651176452637,
      "learning_rate": 0.00015065273457233257,
      "loss": 0.2105,
      "step": 13460
    },
    {
      "epoch": 0.9669777458722182,
      "grad_norm": 0.06560003012418747,
      "learning_rate": 0.00015059955863975965,
      "loss": 0.2724,
      "step": 13470
    },
    {
      "epoch": 0.9676956209619526,
      "grad_norm": 2.880194902420044,
      "learning_rate": 0.00015054638270718673,
      "loss": 0.3737,
      "step": 13480
    },
    {
      "epoch": 0.968413496051687,
      "grad_norm": 0.2954391539096832,
      "learning_rate": 0.0001504932067746138,
      "loss": 0.2464,
      "step": 13490
    },
    {
      "epoch": 0.9691313711414213,
      "grad_norm": 10.652420997619629,
      "learning_rate": 0.0001504400308420409,
      "loss": 0.2421,
      "step": 13500
    },
    {
      "epoch": 0.9698492462311558,
      "grad_norm": 2.6683199405670166,
      "learning_rate": 0.00015038685490946797,
      "loss": 0.2024,
      "step": 13510
    },
    {
      "epoch": 0.9705671213208902,
      "grad_norm": 24.8371524810791,
      "learning_rate": 0.00015033367897689505,
      "loss": 0.3289,
      "step": 13520
    },
    {
      "epoch": 0.9712849964106246,
      "grad_norm": 14.60846996307373,
      "learning_rate": 0.00015028050304432216,
      "loss": 0.3001,
      "step": 13530
    },
    {
      "epoch": 0.9720028715003589,
      "grad_norm": 3.500464677810669,
      "learning_rate": 0.00015022732711174924,
      "loss": 0.6851,
      "step": 13540
    },
    {
      "epoch": 0.9727207465900933,
      "grad_norm": 5.790135383605957,
      "learning_rate": 0.0001501741511791763,
      "loss": 0.2592,
      "step": 13550
    },
    {
      "epoch": 0.9734386216798278,
      "grad_norm": 7.436849117279053,
      "learning_rate": 0.0001501209752466034,
      "loss": 0.1465,
      "step": 13560
    },
    {
      "epoch": 0.9741564967695621,
      "grad_norm": 11.616500854492188,
      "learning_rate": 0.00015006779931403048,
      "loss": 0.1637,
      "step": 13570
    },
    {
      "epoch": 0.9748743718592965,
      "grad_norm": 0.11369828134775162,
      "learning_rate": 0.00015001462338145756,
      "loss": 0.0442,
      "step": 13580
    },
    {
      "epoch": 0.9755922469490309,
      "grad_norm": 3.370940685272217,
      "learning_rate": 0.00014996144744888466,
      "loss": 0.2073,
      "step": 13590
    },
    {
      "epoch": 0.9763101220387652,
      "grad_norm": 8.556706428527832,
      "learning_rate": 0.00014990827151631171,
      "loss": 0.12,
      "step": 13600
    },
    {
      "epoch": 0.9770279971284996,
      "grad_norm": 0.3264889717102051,
      "learning_rate": 0.0001498550955837388,
      "loss": 0.2487,
      "step": 13610
    },
    {
      "epoch": 0.9777458722182341,
      "grad_norm": 2.4446659088134766,
      "learning_rate": 0.00014980191965116587,
      "loss": 0.2184,
      "step": 13620
    },
    {
      "epoch": 0.9784637473079684,
      "grad_norm": 4.6823835372924805,
      "learning_rate": 0.00014974874371859298,
      "loss": 0.2293,
      "step": 13630
    },
    {
      "epoch": 0.9791816223977028,
      "grad_norm": 10.568124771118164,
      "learning_rate": 0.00014969556778602006,
      "loss": 0.2502,
      "step": 13640
    },
    {
      "epoch": 0.9798994974874372,
      "grad_norm": 4.744994163513184,
      "learning_rate": 0.00014964239185344714,
      "loss": 0.1942,
      "step": 13650
    },
    {
      "epoch": 0.9806173725771715,
      "grad_norm": 0.11141906678676605,
      "learning_rate": 0.00014958921592087422,
      "loss": 0.1854,
      "step": 13660
    },
    {
      "epoch": 0.9813352476669059,
      "grad_norm": 0.008504096418619156,
      "learning_rate": 0.0001495360399883013,
      "loss": 0.2225,
      "step": 13670
    },
    {
      "epoch": 0.9820531227566404,
      "grad_norm": 0.23196864128112793,
      "learning_rate": 0.00014948286405572838,
      "loss": 0.1607,
      "step": 13680
    },
    {
      "epoch": 0.9827709978463748,
      "grad_norm": 8.974105834960938,
      "learning_rate": 0.00014942968812315548,
      "loss": 0.1914,
      "step": 13690
    },
    {
      "epoch": 0.9834888729361091,
      "grad_norm": 0.014874087646603584,
      "learning_rate": 0.00014937651219058256,
      "loss": 0.2723,
      "step": 13700
    },
    {
      "epoch": 0.9842067480258435,
      "grad_norm": 9.376877784729004,
      "learning_rate": 0.00014932333625800962,
      "loss": 0.243,
      "step": 13710
    },
    {
      "epoch": 0.9849246231155779,
      "grad_norm": 2.4265124797821045,
      "learning_rate": 0.00014927016032543672,
      "loss": 0.1848,
      "step": 13720
    },
    {
      "epoch": 0.9856424982053122,
      "grad_norm": 1.0372475385665894,
      "learning_rate": 0.0001492169843928638,
      "loss": 0.3579,
      "step": 13730
    },
    {
      "epoch": 0.9863603732950467,
      "grad_norm": 0.27790752053260803,
      "learning_rate": 0.00014916380846029088,
      "loss": 0.1596,
      "step": 13740
    },
    {
      "epoch": 0.9870782483847811,
      "grad_norm": 6.133638381958008,
      "learning_rate": 0.00014911063252771796,
      "loss": 0.2342,
      "step": 13750
    },
    {
      "epoch": 0.9877961234745154,
      "grad_norm": 12.65764045715332,
      "learning_rate": 0.00014905745659514504,
      "loss": 0.1188,
      "step": 13760
    },
    {
      "epoch": 0.9885139985642498,
      "grad_norm": 11.169763565063477,
      "learning_rate": 0.00014900428066257212,
      "loss": 0.2156,
      "step": 13770
    },
    {
      "epoch": 0.9892318736539842,
      "grad_norm": 1.823293685913086,
      "learning_rate": 0.0001489511047299992,
      "loss": 0.2947,
      "step": 13780
    },
    {
      "epoch": 0.9899497487437185,
      "grad_norm": 7.151967525482178,
      "learning_rate": 0.0001488979287974263,
      "loss": 0.3663,
      "step": 13790
    },
    {
      "epoch": 0.990667623833453,
      "grad_norm": 15.785416603088379,
      "learning_rate": 0.00014884475286485339,
      "loss": 0.2759,
      "step": 13800
    },
    {
      "epoch": 0.9913854989231874,
      "grad_norm": 0.9865481853485107,
      "learning_rate": 0.00014879157693228047,
      "loss": 0.1975,
      "step": 13810
    },
    {
      "epoch": 0.9921033740129217,
      "grad_norm": 15.943313598632812,
      "learning_rate": 0.00014873840099970754,
      "loss": 0.1603,
      "step": 13820
    },
    {
      "epoch": 0.9928212491026561,
      "grad_norm": 1.106642484664917,
      "learning_rate": 0.00014868522506713462,
      "loss": 0.054,
      "step": 13830
    },
    {
      "epoch": 0.9935391241923905,
      "grad_norm": 0.09535820782184601,
      "learning_rate": 0.0001486320491345617,
      "loss": 0.2935,
      "step": 13840
    },
    {
      "epoch": 0.994256999282125,
      "grad_norm": 0.17130906879901886,
      "learning_rate": 0.00014857887320198878,
      "loss": 0.1392,
      "step": 13850
    },
    {
      "epoch": 0.9949748743718593,
      "grad_norm": 5.365914344787598,
      "learning_rate": 0.00014852569726941586,
      "loss": 0.0858,
      "step": 13860
    },
    {
      "epoch": 0.9956927494615937,
      "grad_norm": 11.845368385314941,
      "learning_rate": 0.00014847252133684294,
      "loss": 0.2124,
      "step": 13870
    },
    {
      "epoch": 0.9964106245513281,
      "grad_norm": 0.6609846353530884,
      "learning_rate": 0.00014841934540427002,
      "loss": 0.1966,
      "step": 13880
    },
    {
      "epoch": 0.9971284996410624,
      "grad_norm": 0.5441268086433411,
      "learning_rate": 0.00014836616947169713,
      "loss": 0.5906,
      "step": 13890
    },
    {
      "epoch": 0.9978463747307968,
      "grad_norm": 0.024298978969454765,
      "learning_rate": 0.0001483129935391242,
      "loss": 0.1454,
      "step": 13900
    },
    {
      "epoch": 0.9985642498205313,
      "grad_norm": 0.2875455617904663,
      "learning_rate": 0.0001482598176065513,
      "loss": 0.1299,
      "step": 13910
    },
    {
      "epoch": 0.9992821249102656,
      "grad_norm": 1.5657727718353271,
      "learning_rate": 0.00014820664167397837,
      "loss": 0.2246,
      "step": 13920
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.08295547217130661,
      "learning_rate": 0.00014815346574140545,
      "loss": 0.4096,
      "step": 13930
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9417898571967,
      "eval_f1_class_0_Oxidored": 0.9498901098901099,
      "eval_f1_class_1_Transfer": 0.9404856013551666,
      "eval_f1_class_2_Hydrolas": 0.9246370623398804,
      "eval_f1_class_3_Lyases": 0.9132749441868453,
      "eval_f1_class_4_Isomeras": 0.9224067519738633,
      "eval_f1_class_5_Ligases": 0.9786644499861458,
      "eval_f1_class_6_Transloc": 0.9604944907282988,
      "eval_f1_macro": 0.9414076300657586,
      "eval_f1_micro": 0.9417898571967,
      "eval_loss": 0.23410142958164215,
      "eval_precision_macro": 0.9464988717512034,
      "eval_precision_micro": 0.9417898571967,
      "eval_recall_macro": 0.9381444223541068,
      "eval_recall_micro": 0.9417898571967,
      "eval_runtime": 207.036,
      "eval_samples_per_second": 115.337,
      "eval_steps_per_second": 14.418,
      "step": 13930
    }
  ],
  "logging_steps": 10,
  "max_steps": 41790,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.271535821934596e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
